{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stream Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "    CREATE TABLE orders (\n",
    "        order_uid  BIGINT,\n",
    "        product_id BIGINT,\n",
    "        price      DECIMAL(32, 2),\n",
    "        order_time TIMESTAMP(3)\n",
    "    ) WITH (\n",
    "        'connector' = 'datagen'\n",
    "    );\n",
    "\n",
    "    SELECT * FROM orders;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Insert Into"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "CREATE TABLE server_logs ( \n",
    "    client_ip STRING,\n",
    "    client_identity STRING, \n",
    "    userid STRING, \n",
    "    user_agent STRING,\n",
    "    log_time TIMESTAMP(3),\n",
    "    request_line STRING, \n",
    "    status_code STRING, \n",
    "    size INT\n",
    ") WITH (\n",
    "  'connector' = 'faker', \n",
    "  'fields.client_ip.expression' = '#{Internet.publicIpV4Address}',\n",
    "  'fields.client_identity.expression' =  '-',\n",
    "  'fields.userid.expression' =  '-',\n",
    "  'fields.user_agent.expression' = '#{Internet.userAgentAny}',\n",
    "  'fields.log_time.expression' =  '#{date.past ''15'',''5'',''SECONDS''}',\n",
    "  'fields.request_line.expression' = '#{regexify ''(GET|POST|PUT|PATCH){1}''} #{regexify ''(/search\\.html|/login\\.html|/prod\\.html|cart\\.html|/order\\.html){1}''} #{regexify ''(HTTP/1\\.1|HTTP/2|/HTTP/1\\.0){1}''}',\n",
    "  'fields.status_code.expression' = '#{regexify ''(200|201|204|400|401|403|301){1}''}',\n",
    "  'fields.size.expression' = '#{number.numberBetween ''100'',''10000000''}'\n",
    ");\n",
    "\n",
    "CREATE TABLE client_errors (\n",
    "  log_time TIMESTAMP(3),\n",
    "  request_line STRING,\n",
    "  status_code STRING,\n",
    "  size INT\n",
    ")\n",
    "WITH (\n",
    "  'connector' = 'blackhole'\n",
    ");\n",
    "\n",
    "INSERT INTO client_errors\n",
    "SELECT \n",
    "  log_time,\n",
    "  request_line,\n",
    "  status_code,\n",
    "  size\n",
    "FROM server_logs\n",
    "WHERE \n",
    "  regexp_like(status_code, '4[0-9][0-9]');\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Temporary Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Temp tables live within the current session, while non-temporary tables are stored in a catalog\n",
    "- Notice how we use a blackhole connector to avoid publishing metadata in a catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "CREATE TEMPORARY TABLE server_logs ( \n",
    "    client_ip STRING,\n",
    "    client_identity STRING, \n",
    "    userid STRING, \n",
    "    user_agent STRING,\n",
    "    log_time TIMESTAMP(3),\n",
    "    request_line STRING, \n",
    "    status_code STRING, \n",
    "    size INT\n",
    ") WITH (\n",
    "  'connector' = 'faker', \n",
    "  'fields.client_ip.expression' = '#{Internet.publicIpV4Address}',\n",
    "  'fields.client_identity.expression' =  '-',\n",
    "  'fields.userid.expression' =  '-',\n",
    "  'fields.user_agent.expression' = '#{Internet.userAgentAny}',\n",
    "  'fields.log_time.expression' =  '#{date.past ''15'',''5'',''SECONDS''}',\n",
    "  'fields.request_line.expression' = '#{regexify ''(GET|POST|PUT|PATCH){1}''} #{regexify ''(/search\\.html|/login\\.html|/prod\\.html|cart\\.html|/order\\.html){1}''} #{regexify ''(HTTP/1\\.1|HTTP/2|/HTTP/1\\.0){1}''}',\n",
    "  'fields.status_code.expression' = '#{regexify ''(200|201|204|400|401|403|301){1}''}',\n",
    "  'fields.size.expression' = '#{number.numberBetween ''100'',''10000000''}'\n",
    ");\n",
    "\n",
    "CREATE TEMPORARY TABLE client_errors (\n",
    "  log_time TIMESTAMP(3),\n",
    "  request_line STRING,\n",
    "  status_code STRING,\n",
    "  size INT\n",
    ")\n",
    "WITH (\n",
    "  'connector' = 'blackhole'\n",
    ");\n",
    "\n",
    "INSERT INTO client_errors\n",
    "SELECT \n",
    "  log_time,\n",
    "  request_line,\n",
    "  status_code,\n",
    "  size\n",
    "FROM server_logs\n",
    "WHERE \n",
    "  regexp_list(status_code, '4[0-9][0-9]');\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. SQL Basics (where / group by / order by)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is an additional complexity when dealing with stuff like `order by` in streaming tables; because FlinkSQL can potentially deal with unbounded tables (i.e. table keeps accumulating over time)\n",
    "\n",
    "- As such, when new data comes in, you may need to reorder the table with every refresh\n",
    "\n",
    "- To deal with this, when working on unbounded tables, you need to ensure that there is a primary sorting key defined. This is `log_time` in the example below\n",
    "\n",
    "- `WATERMARK` is used to create a watermark for each row, for event time processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "CREATE TEMPORARY TABLE server_logs ( \n",
    "    client_ip STRING,\n",
    "    client_identity STRING, \n",
    "    userid STRING, \n",
    "    user_agent STRING,\n",
    "    log_time TIMESTAMP(3),\n",
    "    request_line STRING, \n",
    "    status_code STRING, \n",
    "    size INT, \n",
    "    WATERMARK FOR log_time AS log_time - INTERVAL '15' SECONDS\n",
    ") WITH (\n",
    "  'connector' = 'faker', \n",
    "  'fields.client_ip.expression' = '#{Internet.publicIpV4Address}',\n",
    "  'fields.client_identity.expression' =  '-',\n",
    "  'fields.userid.expression' =  '-',\n",
    "  'fields.user_agent.expression' = '#{Internet.userAgentAny}',\n",
    "  'fields.log_time.expression' =  '#{date.past ''15'',''5'',''SECONDS''}',\n",
    "  'fields.request_line.expression' = '#{regexify ''(GET|POST|PUT|PATCH){1}''} #{regexify ''(/search\\.html|/login\\.html|/prod\\.html|cart\\.html|/order\\.html){1}''} #{regexify ''(HTTP/1\\.1|HTTP/2|/HTTP/1\\.0){1}''}',\n",
    "  'fields.status_code.expression' = '#{regexify ''(200|201|204|400|401|403|301){1}''}',\n",
    "  'fields.size.expression' = '#{number.numberBetween ''100'',''10000000''}'\n",
    ");\n",
    "\n",
    "SELECT \n",
    "  TUMBLE_ROWTIME(log_time, INTERVAL '1' MINUTE) AS window_time,\n",
    "  REGEXP_EXTRACT(user_agent,'[^\\/]+') AS browser,\n",
    "  COUNT(*) AS cnt_browser\n",
    "FROM server_logs\n",
    "GROUP BY \n",
    "  REGEXP_EXTRACT(user_agent,'[^\\/]+'),\n",
    "  TUMBLE(log_time, INTERVAL '1' MINUTE)\n",
    "ORDER BY\n",
    "  window_time,\n",
    "  cnt_browser DESC;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A view is not a temporary table, in that no data is actually stored. It is simply a pre-defined query, and every time it gets referenced, the query gets run once\n",
    "\n",
    "- This is simply meant to encourage re-use of code in generating SQL queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "CREATE TABLE server_logs ( \n",
    "    client_ip STRING,\n",
    "    client_identity STRING, \n",
    "    userid STRING, \n",
    "    user_agent STRING,\n",
    "    log_time TIMESTAMP(3),\n",
    "    request_line STRING, \n",
    "    status_code STRING, \n",
    "    size INT\n",
    ") WITH (\n",
    "  'connector' = 'faker', \n",
    "  'fields.client_ip.expression' = '#{Internet.publicIpV4Address}',\n",
    "  'fields.client_identity.expression' =  '-',\n",
    "  'fields.userid.expression' =  '-',\n",
    "  'fields.user_agent.expression' = '#{Internet.userAgentAny}',\n",
    "  'fields.log_time.expression' =  '#{date.past ''15'',''5'',''SECONDS''}',\n",
    "  'fields.request_line.expression' = '#{regexify ''(GET|POST|PUT|PATCH){1}''} #{regexify ''(/search\\.html|/login\\.html|/prod\\.html|cart\\.html|/order\\.html){1}''} #{regexify ''(HTTP/1\\.1|HTTP/2|/HTTP/1\\.0){1}''}',\n",
    "  'fields.status_code.expression' = '#{regexify ''(200|201|204|400|401|403|301){1}''}',\n",
    "  'fields.size.expression' = '#{number.numberBetween ''100'',''10000000''}'\n",
    ");\n",
    "\n",
    "CREATE VIEW successful_requests AS \n",
    "SELECT * \n",
    "FROM server_logs\n",
    "WHERE regexp_like(status_code '[2,3][0-9][0-9]');\n",
    "\n",
    "SELECT * FROM successful_requests;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Statement Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are cases where you need to output to multiple sinks (e.g. Kafka + Offline Store)\n",
    "- In such cases, the job may share some very expensive intermediate operations\n",
    "- So instead of trying to compute the same thing twice, you can use statement sets to reuse intermediate query objects!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "CREATE TEMPORARY TABLE server_logs ( \n",
    "    client_ip       STRING,\n",
    "    client_identity STRING, \n",
    "    userid          STRING, \n",
    "    user_agent      STRING,\n",
    "    log_time        TIMESTAMP(3),\n",
    "    request_line    STRING, \n",
    "    status_code     STRING, \n",
    "    size            INT,\n",
    "    WATERMARK FOR log_time AS log_time - INTERVAL '30' SECONDS\n",
    ") WITH (\n",
    "  'connector' = 'faker', \n",
    "  'fields.client_ip.expression' = '#{Internet.publicIpV4Address}',\n",
    "  'fields.client_identity.expression' =  '-',\n",
    "  'fields.userid.expression' =  '-',\n",
    "  'fields.user_agent.expression' = '#{Internet.userAgentAny}',\n",
    "  'fields.log_time.expression' =  '#{date.past ''15'',''5'',''SECONDS''}',\n",
    "  'fields.request_line.expression' = '#{regexify ''(GET|POST|PUT|PATCH){1}''} #{regexify ''(/search\\.html|/login\\.html|/prod\\.html|cart\\.html|/order\\.html){1}''} #{regexify ''(HTTP/1\\.1|HTTP/2|/HTTP/1\\.0){1}''}',\n",
    "  'fields.status_code.expression' = '#{regexify ''(200|201|204|400|401|403|301){1}''}',\n",
    "  'fields.size.expression' = '#{number.numberBetween ''100'',''10000000''}'\n",
    ");\n",
    "\n",
    "CREATE TEMPORARY TABLE realtime_aggregations (\n",
    "  `browser`     STRING,\n",
    "  `status_code` STRING,\n",
    "  `end_time`    TIMESTAMP(3),\n",
    "  `requests`    BIGINT NOT NULL\n",
    ") WITH (\n",
    "  'connector' = 'kafka',\n",
    "  'topic' = 'browser-status-codes', \n",
    "  'properties.bootstrap.servers' = 'localhost:9092',\n",
    "  'properties.group.id' = 'browser-countds',\n",
    "  'format' = 'avro' \n",
    ");\n",
    "\n",
    "\n",
    "CREATE TEMPORARY TABLE offline_datawarehouse (\n",
    "    `browser`     STRING,\n",
    "    `status_code` STRING,\n",
    "    `dt`          STRING,\n",
    "    `hour`        STRING,\n",
    "    `requests`    BIGINT NOT NULL\n",
    ") PARTITIONED BY (`dt`, `hour`) WITH (\n",
    "  'connector' = 'filesystem',\n",
    "  'path' = 's3://my-bucket/browser-into',\n",
    "  'sink.partition-commit.trigger' = 'partition-time', \n",
    "  'format' = 'parquet' \n",
    ");\n",
    "\n",
    "-- This is a shared view that will be used by both \n",
    "-- insert into statements\n",
    "CREATE TEMPORARY VIEW browsers AS  \n",
    "SELECT \n",
    "  REGEXP_EXTRACT(user_agent,'[^\\/]+') AS browser,\n",
    "  status_code,\n",
    "  log_time\n",
    "FROM server_logs;\n",
    "\n",
    "BEGIN STATEMENT SET;\n",
    "INSERT INTO realtime_aggregations\n",
    "SELECT\n",
    "    browser,\n",
    "    status_code,\n",
    "    TUMBLE_ROWTIME(log_time, INTERVAL '5' MINUTE) AS end_time,\n",
    "    COUNT(*) requests\n",
    "FROM browsers\n",
    "GROUP BY \n",
    "    browser,\n",
    "    status_code,\n",
    "    TUMBLE(log_time, INTERVAL '5' MINUTE);\n",
    "INSERT INTO offline_datawarehouse\n",
    "SELECT\n",
    "    browser,\n",
    "    status_code,\n",
    "    DATE_FORMAT(TUMBLE_ROWTIME(log_time, INTERVAL '1' HOUR), 'yyyy-MM-dd') AS `dt`,\n",
    "    DATE_FORMAT(TUMBLE_ROWTIME(log_time, INTERVAL '1' HOUR), 'HH') AS `hour`,\n",
    "    COUNT(*) requests\n",
    "FROM browsers\n",
    "GROUP BY \n",
    "    browser,\n",
    "    status_code,\n",
    "    TUMBLE(log_time, INTERVAL '1' HOUR);\n",
    "END;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Convert Timestamps with Timezones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "CREATE TABLE iot_status ( \n",
    "    device_ip       STRING,\n",
    "    device_timezone STRING,\n",
    "    iot_timestamp   TIMESTAMP(3),\n",
    "    status_code     STRING\n",
    ") WITH (\n",
    "  'connector' = 'faker', \n",
    "  'fields.device_ip.expression' = '#{Internet.publicIpV4Address}',\n",
    "  'fields.device_timezone.expression' =  '#{regexify ''(America\\/Los_Angeles|Europe\\/Rome|Europe\\/London|Australia\\/Sydney){1}''}',\n",
    "  'fields.iot_timestamp.expression' =  '#{date.past ''15'',''5'',''SECONDS''}',\n",
    "  'fields.status_code.expression' = '#{regexify ''(OK|KO|WARNING){1}''}',\n",
    "  'rows-per-second' = '3'\n",
    ");\n",
    "\n",
    "SELECT \n",
    "  device_ip, \n",
    "  device_timezone,\n",
    "  iot_timestamp,\n",
    "  convert_tz(cast(iot_timestamp as string), device_timezone, 'UTC') iot_timestamp_utc,\n",
    "  status_code\n",
    "FROM iot_status;\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
