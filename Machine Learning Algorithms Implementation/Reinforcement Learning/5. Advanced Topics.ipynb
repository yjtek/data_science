{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf605200",
   "metadata": {},
   "source": [
    "# Advanced Topics (Optional)\n",
    "\n",
    "These topics extend the core RL methods to handle **large-scale, continuous, or complex environments**.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Function Approximation and Deep RL\n",
    "\n",
    "- **Problem**: Tabular methods (storing value for every state/action) are infeasible for large or continuous state spaces.\n",
    "- **Solution**: Approximate value functions or policies using parameterized functions (linear, neural networks, etc.).\n",
    "  - Examples:\n",
    "    - **Deep Q-Network (DQN)**: neural network approximates Q(s,a).  \n",
    "    - **Deep Policy Gradient / Actor–Critic**: neural networks parameterize policy π_θ(a|s) and value function V_φ(s).\n",
    "- **Benefit**: generalization across similar states; handle high-dimensional inputs like images.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Off-Policy Learning\n",
    "\n",
    "- **On-policy**: learn only from data generated by the current policy.  \n",
    "- **Off-policy**: learn from data generated by any policy (important for replay buffers, experience reuse).\n",
    "\n",
    "Examples:\n",
    "- **Q-learning**: off-policy TD control.  \n",
    "- **DDPG (Deep Deterministic Policy Gradient)**: continuous action, off-policy actor–critic.  \n",
    "- **Importance**: enables **replay and sample efficiency**, critical for deep RL.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Exploration Strategies\n",
    "\n",
    "Exploration is harder in **large or continuous environments**.\n",
    "\n",
    "Common strategies:\n",
    "- **ε-greedy / Softmax**: simple probabilistic exploration.  \n",
    "- **Upper Confidence Bound (UCB)**: explore actions with high uncertainty.  \n",
    "- **Intrinsic motivation / curiosity**: reward agent for visiting novel states.  \n",
    "- **Noisy networks**: add parameter noise to encourage diverse behavior.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Stability Issues\n",
    "\n",
    "Deep RL introduces **instability** because function approximation + bootstrapping + off-policy updates can diverge.\n",
    "\n",
    "Common solutions:\n",
    "- **Target networks**: use a slowly updated copy of the network for bootstrapping targets (DQN).  \n",
    "- **Replay buffers**: store past transitions and sample randomly to reduce correlation.  \n",
    "- **Gradient clipping / normalization**: prevent exploding updates.  \n",
    "- **Advantage normalization**: reduces variance in policy gradients.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Key Takeaways\n",
    "\n",
    "- **Function approximation** is necessary for large state/action spaces.  \n",
    "- **Off-policy learning** improves sample efficiency but requires care.  \n",
    "- **Exploration strategies** prevent the agent from converging to suboptimal behavior.  \n",
    "- **Stability techniques** (target networks, replay buffers) are crucial for deep RL.  \n",
    "- These topics bridge **classical RL** and **modern deep RL applications**.\n",
    "\n",
    "---\n",
    "\n",
    "*Next: Practical Implementation — coding examples, environments, debugging, and evaluation.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a62787a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
