{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2473b4fd",
   "metadata": {},
   "source": [
    "# RL Foundations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b9c936",
   "metadata": {},
   "source": [
    "- This notebook will serve as an introductory guide to RL, focusing on motivation and intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf34ef9c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## What is RL?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb70f83",
   "metadata": {},
   "source": [
    "- RL is somewhat similar to supervised learning\n",
    "\n",
    "- Recall that in supervised learning, we pair an outcome with data, and try to get the model weights adjusted such that, given an input data, we will replicate the outcome\n",
    "\n",
    "- For an RL learner, we have the same idea; some inputs are provided, and some reward outcomes are logged.\n",
    "    - The difference is that in RL, in between seeing the inputs, and the rewards, the agent needs to take an action $A$\n",
    "    - With different set of inputs (**state**), we can take different actions, which can lead to different rewards\n",
    "    - So the qn for the learner is; given a state, which action should we take that will maximise our reward?\n",
    "\n",
    "- Since there is an action required by the agent, this choice of action can only be learnt by the agent through trial and error\n",
    "    \n",
    "- While there are many sophisticated approaches to guide how an agent to make decisions, it is always possible to turn an RL problem into a supervised learning problem. For example, if a we have a dataset with: \n",
    "    - State: the input context seen by the agent\n",
    "    - Action: The action taken by the agent\n",
    "    - Reward: The reward received by the agent for taking action $A$ at state $S$\n",
    "\n",
    "- Then a simple approach to RL might be to say; train a model that takes in state and action, and predicts reward $R$\n",
    "    - In this case, the best action is simply the one that produces the $\\argmax(R)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e450ab8a",
   "metadata": {},
   "source": [
    "## Formalising the Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5418da8f",
   "metadata": {},
   "source": [
    "- For every step $t$\n",
    "    1. The agent observe the current state $s_t$\n",
    "    2. The agent chooses and action $a_t$. **How** the agent chooses the action is based on its **policy**\n",
    "    3. Based on $s_t, a_t$, the environment will make a transition to some state $s_{t+1}$ with some reward $r_{t+1}$\n",
    "    4. Based on the knowledge of $s_t, a_t, r_{t+1}, s_{t+1}$, the agent tries to improve its decision making policy!\n",
    "\n",
    "- Don't be confused by jargon. Can think of a **policy** as a deterministic map of how the agent makes decisions.\n",
    "    - Simple example; let's say $s_t$ tells the agent that (i) the forecast is that is will rain soon, (ii) there are water droplets on the window\n",
    "    - Then the policy can be how the agent chooses within the set of `[bring umbrella, don't bring umbrella]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d0fe81",
   "metadata": {},
   "source": [
    "## Explore/Exploit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba992d7d",
   "metadata": {},
   "source": [
    "- Let's zoom in on an agent's **policy**\n",
    "\n",
    "- Because the agent is making decisions under uncertainty, every decision is a tradeoff between exploration and exploitation\n",
    "    - Should I make use of the information I know now to choose the best action?\n",
    "    - Or should I explore other actions to get more information?\n",
    "    - For example, suppose you try a new food place. You've tried one dish, that you quite liked. Do you stick to that dish (exploitation), or do you try something new off the menu (exploration)?\n",
    "\n",
    "- We'll go through each of these in a later notebook, but here are some common ideas about how we want to structure our policy:\n",
    "    1. $\\epsilon$-Greedy: With some probability $\\epsilon$, choose a random action. Otherwise, choose the action that maximises your reward\n",
    "    2. Softmax/Boltzmann: Pick actions based on estimated value. That is, take a softmax of all rewards, and sample probabilistically. The softmax here is a bit different, because you reweight the softmax-ed probabilities by some constant $\\tau$, which we call the temperature\n",
    "    \n",
    "    \\begin{aligned}\n",
    "        P(a_t | s_t) &= \\frac{e^{r_a^{t+1} / \\tau}}{\\sum_{a'=1}^{N} e^{r_{a'}^{t+1} / \\tau}}\n",
    "    \\end{aligned}\n",
    "\n",
    "    3. UCB Sampling: The idea of UCB sampling is that you greedily sample action using the next available reward $r_{t+1}$, PLUS an additional factor that increases if the proportion of action $i$ attempted out of all actions attempted exceeds $y$%\n",
    "\n",
    "    \\begin{aligned}\n",
    "        UCB(a_t) &= r_{a_t}^{t+1} + c \\cdot \\sqrt{\\frac{\\ln(N)}{N_{a_t}}} \\\\\n",
    "\n",
    "        \\text{where} \\\\\n",
    "        &N \\text{: Total number of trials} \\\\\n",
    "        &N_a \\text{: Count of action a taken} \\\\\n",
    "        &c \\text{: Constant controlling the aggressiveness of our exploration} \\\\\n",
    "    \\end{aligned}\n",
    "\n",
    "    4. Thompson Sampling: \n",
    "        - Assume that the reward for each action $a_t$ follows some distribution $D$ governed by some parameters $\\alpha_{a_t}, \\beta_{a_t}, ...$. \n",
    "        - Using these parameters, \"draw\" a reward for each action by taking a sample from the distribution $D(\\alpha_{a_t}, \\beta_{a_t}, ...)$\n",
    "        - Using this reward, decide which action to take greedily\n",
    "        - Then using the outcome of the action, update your beliefs about the distribution's hyperparameters\n",
    "\n",
    "        - This is still not very concrete; we will go through this in more detail lateer; let's just treat this as intuition building first"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e714015",
   "metadata": {},
   "source": [
    "## Episodic Tasks vs Continuing Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbfadde",
   "metadata": {},
   "source": [
    "- Another quite useful distinction is the idea of episodic vs continuing tasks\n",
    "    - Episodic tasks end at some point (e.g. a chess game always ends)\n",
    "    - Continuing tasks extend to infinity\n",
    "\n",
    "- The implication here is that for episodic tasks, we only aggregate rewards up to a certain terminal timesteps, while continuing tasks will require some sort of discounted return\n",
    "\n",
    "\\begin{aligned}\n",
    "    G_t &= \\sum_{k=t}^{T} R_{k+1} & \\text{ for episodic} \\\\ \\\\\n",
    "\n",
    "    G_t &= \\sum_{k=0}^{\\inf} \\gamma^k R_{t+k+1} & \\text{ for continuous} \\\\ \\\\\n",
    "\\end{aligned}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
