{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c81132f9",
   "metadata": {},
   "source": [
    "# Approaches to Solve the Bellman Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f40f9c6",
   "metadata": {},
   "source": [
    "- We have established the basic setup for Reinforcement Learning by introducing the equilibrium equations of the State-Value and Action Value functions of the Bellman Equation\n",
    "\n",
    "- Now we look into **how** the Bellman Equations can be solved in practise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d06256",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "    \n",
    "    &\\text{State-Value Function:} \\\\\n",
    "    &\\quad v_{\\pi}(s) = \\sum_a \\pi(a|s) \\sum_{s'} P(s' | s, a) [R(s, a, s') + \\gamma v_{\\pi}(s')] \\\\ \\\\\n",
    "\n",
    "    &\\text{Action-Value Function:} \\\\\n",
    "    &\\quad q_{\\pi}(s,a) = \\sum_{s'} P(s' | s, a) [R(s, a, s') + \\gamma \\sum_{a'} \\pi(a'|s') q_{\\pi}(s', a')]\n",
    "\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180daa30",
   "metadata": {},
   "source": [
    "## Dynamic Programming (DP) - Model Based"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc4773e",
   "metadata": {},
   "source": [
    "- The simplest case of solving the Bellman Equation is when you have **both** the transition probabilities $P(s'|s,a)$ and the rewards $R(s,a,s')$\n",
    "\n",
    "- That is, you know the full Markov Decision Process (MDP)\n",
    "\n",
    "- This is, of course, exceedingly rare. But it is instructive to see how to solve for $\\pi^*$ when you have almost all available information\n",
    "\n",
    "- We will go through 2 ways of implementing the DP solver\n",
    "    - Policy Interation\n",
    "    - Value Iteration\n",
    "\n",
    "- To do this, let's assume the following set up:\n",
    "    - States $S = {0,1,2}$\n",
    "    - Actions $S = {0,1}$\n",
    "    - Transitions and rewards are known\n",
    "    - Discount Factor $\\gamma = 0.9$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d544b54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "N_STATES = 20\n",
    "N_ACTIONS = 20\n",
    "GAMMA = 0.9\n",
    "\n",
    "# Shape corresponding to State, Action, Next State\n",
    "TRANSITION_PROBABILITIES = np.random.rand(N_STATES, N_ACTIONS, N_STATES)\n",
    "# Normalise so transition probabilities for a given state sums to 1 across all actions\n",
    "TRANSITION_PROBABILITIES /= np.sum(TRANSITION_PROBABILITIES, axis=2, keepdims=True)\n",
    "\n",
    "REWARDS = np.random.rand(N_STATES, N_ACTIONS, N_STATES) * 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54792600",
   "metadata": {},
   "source": [
    "### Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df82c340",
   "metadata": {},
   "source": [
    "- In policy iteration, we create a loop that does (i) policy evaluation, and (ii) policy improvement based on the evaluated outcomes of the policy, and the loop runs until some stopping criteria is reached\n",
    "\n",
    "- **Policy Evaluation:** \n",
    "    - Since we know the transition probabilities and rewards with full certainty, we can update the State-Value function by computing the Bellman Equation\n",
    "    \n",
    "    - Start with a seed array representing the State-Value function $V$, and a policy $\\pi$ that deterministically tells us what action to take in a given state $S$\n",
    "    \n",
    "    - Compute 1 pass of the Bellman State Value function update for all states. This is just a transition-probability-weighted average of the state values of the next states $S'$\n",
    "\n",
    "    - Check that the largest change in the state value estimates for all $N$ states exceeds some threshold $\\theta$. If even the largest change falls below the threshold, we assume convergence, and we return the state values $V$\n",
    "\n",
    "    - Otherwise, keep looping\n",
    "\n",
    "- **Policy Improvement**\n",
    "    - We've previously updated our estimate of the State-Value function $V$\n",
    "    - Now, we need to update our policy based on the new estimate of the State-Value function. That is, given a state $S$, I want to know the value of taking an action $A$. \n",
    "    - That is; the **Q-Value**\n",
    "\n",
    "    - Looping over every state $S$, we init an array to hold the new Q values\n",
    "\n",
    "    - Given $S$, loop over every action available, and update the Action-Value function $Q$. This is simply the transition-probability-weighted average of rewards from the transition $S -> S'$, plus the discounted state-value of $S'$\n",
    "\n",
    "    - Finally, the new policy for state $S$ is simply to take the argmax of all the Q values\n",
    "\n",
    "- **Policy Iteration**\n",
    "    - Bringing these 2 together, we simply create an infinite loop of `Evaluation --> Improvement --> Evalution ...`\n",
    "\n",
    "    - This will keep going until some convergence criteria is met. An easy on is to say that; if for some $N$ consecutive loops we do not change our policy, we have reached convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bbc794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(\n",
    "    transition_probabilities: np.ndarray,\n",
    "    rewards: np.ndarray,\n",
    "    curr_state_value_function: np.ndarray,\n",
    "    policy: np.ndarray,\n",
    "    gamma: float = 0.9,\n",
    "    theta: float = 1e-6\n",
    ") -> np.ndarray:\n",
    "    updated_state_value_function = curr_state_value_function.copy()\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for curr_state in range(N_STATES):\n",
    "            v_updated = sum([\n",
    "                transition_probabilities[curr_state, policy[curr_state], s_prime] * \n",
    "                (rewards[curr_state, policy[curr_state], s_prime] + gamma * updated_state_value_function[s_prime])\n",
    "                for s_prime in range(N_STATES)\n",
    "            ])\n",
    "            delta= max(delta, abs(v_updated - updated_state_value_function[curr_state]))\n",
    "            updated_state_value_function[curr_state] = v_updated\n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    return updated_state_value_function\n",
    "\n",
    "def policy_improvement(\n",
    "    transition_probabilities: np.ndarray,\n",
    "    rewards: np.ndarray,\n",
    "    curr_state_value_function: np.ndarray,\n",
    "    curr_action_value_function: np.ndarray,\n",
    "    curr_policy: np.ndarray,\n",
    "    gamma: float = 0.9\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    updated_policy = curr_policy.copy()\n",
    "    updated_action_value_function = curr_action_value_function.copy()\n",
    "    for s in range(N_STATES):\n",
    "        action_value_function_for_state_s = updated_action_value_function[s].copy()\n",
    "        for a in range(N_ACTIONS):\n",
    "            new_action_value = sum([\n",
    "                transition_probabilities[s, a, s_prime] * \n",
    "                (rewards[s, a, s_prime] + gamma * curr_state_value_function[s_prime])\n",
    "                for s_prime in range(N_STATES)\n",
    "            ])\n",
    "            action_value_function_for_state_s[a] = new_action_value\n",
    "        \n",
    "        updated_policy[s] = np.argmax(action_value_function_for_state_s)\n",
    "        updated_action_value_function[s] = action_value_function_for_state_s\n",
    "    return updated_policy, updated_action_value_function\n",
    "\n",
    "def policy_iteration(\n",
    "    transition_probabilities: np.ndarray,\n",
    "    rewards: np.ndarray,\n",
    "    gamma: float = 0.9,\n",
    "    theta: float = 1e-6\n",
    "):\n",
    "    curr_state_value_function: np.ndarray = np.zeros(N_STATES)\n",
    "    curr_action_value_function: np.ndarray = np.zeros((N_STATES, N_ACTIONS))\n",
    "    curr_policy: np.ndarray = np.zeros(N_STATES).astype(int)\n",
    "    \n",
    "    count_no_change = 0\n",
    "    count_iters = 0\n",
    "    while True:\n",
    "        print(count_iters)\n",
    "        updated_state_value_function = policy_evaluation(\n",
    "            transition_probabilities,\n",
    "            rewards,\n",
    "            curr_state_value_function,\n",
    "            curr_policy,\n",
    "            gamma,\n",
    "            theta\n",
    "        )\n",
    "        updated_policy, updated_action_value_function = policy_improvement(\n",
    "            transition_probabilities,\n",
    "            rewards,\n",
    "            updated_state_value_function,\n",
    "            curr_action_value_function,\n",
    "            curr_policy,\n",
    "            gamma\n",
    "        )\n",
    "        if np.array_equal(curr_policy, updated_policy):\n",
    "            count_no_change += 1\n",
    "            if count_no_change >= 5:\n",
    "                break\n",
    "        else:\n",
    "            count_no_change = 0\n",
    "\n",
    "        curr_policy = updated_policy.copy()\n",
    "        curr_action_value_function = updated_action_value_function.copy()\n",
    "        curr_state_value_function = updated_state_value_function.copy()\n",
    "\n",
    "        count_iters += 1\n",
    "    \n",
    "    return curr_policy, curr_action_value_function, curr_state_value_function\n",
    "\n",
    "pi_star, q_star, v_star = policy_iteration(TRANSITION_PROBABILITIES, REWARDS)\n",
    "\n",
    "print(f\"\"\"\n",
    "    Optimal Policy: {pi_star}\n",
    "    Action-Value: {q_star}\n",
    "    State-Value: {v_star}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e01a0a",
   "metadata": {},
   "source": [
    "- To ascertain if your policy iteration has converged correctly, check that the action values computed from the current transition probability, rewards, and optimal state values matches the values in your optimal q value array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a0d67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in range(N_STATES):\n",
    "    for a in range(N_ACTIONS):\n",
    "        q = sum([\n",
    "            TRANSITION_PROBABILITIES[s, a, s_prime] *\n",
    "            (REWARDS[s, a, s_prime] + GAMMA * v_star[s_prime])\n",
    "            for s_prime in range(N_STATES)\n",
    "        ])\n",
    "        assert np.isclose(q, q_star[s, a])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e4080c",
   "metadata": {},
   "source": [
    "### Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25862008",
   "metadata": {},
   "source": [
    "- In value iteration, the idea is almost identical. We are doing the same update of state value, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c2e063",
   "metadata": {},
   "source": [
    "# Solution Methods\n",
    "---\n",
    "\n",
    "## 2. Monte Carlo (MC) – Sampling-Based, Episodic\n",
    "\n",
    "- **Learns from experience** rather than knowing the model.\n",
    "- **Key idea**: estimate value functions by averaging returns from multiple episodes.\n",
    "- Only works well for **episodic tasks** (episodes must terminate).  \n",
    "- **Pros**: no need for environment model; simple conceptually.  \n",
    "- **Cons**: high variance; inefficient for long episodes; must wait until episode ends.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Temporal-Difference (TD) – Bootstrapped Learning\n",
    "\n",
    "- Combines DP and MC:\n",
    "  - Like DP: updates use existing estimates (bootstrapping).  \n",
    "  - Like MC: learns from sampled experience, not full model.\n",
    "- **TD(0) update rule (state-value)**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eac3224",
   "metadata": {},
   "source": [
    "V(s_t) ← V(s_t) + α [ r_{t+1} + γ V(s_{t+1}) − V(s_t) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6082351",
   "metadata": {},
   "source": [
    "\n",
    "- **Pros**: can learn online, before episode ends; lower variance than MC.  \n",
    "- **Cons**: introduces bias due to bootstrapping; sensitive to step-size α.\n",
    "\n",
    "- **TD control methods**: SARSA (on-policy), Q-learning (off-policy).\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Policy Gradient – Direct Optimization\n",
    "\n",
    "- Instead of computing value functions, **directly parameterize the policy** π_θ(a|s) and optimize expected return:\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64292ca1",
   "metadata": {},
   "source": [
    "J(θ) = E_πθ [ G_t ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870cca08",
   "metadata": {},
   "source": [
    "\n",
    "- **Update rule (gradient ascent)**:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b72dae",
   "metadata": {},
   "source": [
    "θ ← θ + α ∇_θ J(θ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e58e51",
   "metadata": {},
   "source": [
    "\n",
    "- **Pros**: handles continuous action spaces naturally; can represent stochastic policies.  \n",
    "- **Cons**: high variance in gradients; slower convergence.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Actor–Critic Overview\n",
    "\n",
    "- Combines **value-based** and **policy-based** methods:\n",
    "  - **Actor**: updates the policy (like policy gradient).  \n",
    "  - **Critic**: estimates value function (like TD) to reduce variance of updates.\n",
    "- Intuition: the critic **guides** the actor by telling it whether actions are good or bad.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Key Takeaways\n",
    "\n",
    "| Method | Model Needed | Online Learning | Bias vs Variance | Action Space |\n",
    "|--------|--------------|----------------|----------------|--------------|\n",
    "| Dynamic Programming | Yes | No | None | Discrete |\n",
    "| Monte Carlo | No | Episodic | Low variance, high bias? | Discrete/Continuous |\n",
    "| Temporal-Difference | No | Yes | Some bias, lower variance | Discrete/Continuous |\n",
    "| Policy Gradient | No | Yes | High variance | Continuous-friendly |\n",
    "| Actor–Critic | No | Yes | Balanced bias/variance | Continuous-friendly |\n",
    "\n",
    "- Choice depends on **model availability**, **task type**, and **action space**.\n",
    "- These methods form the backbone for **practical RL algorithms** like DQN, PPO, A3C, etc.\n",
    "\n",
    "---\n",
    "\n",
    "*Next: Practical Implementation — coding examples, environments, and debugging tips.*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
