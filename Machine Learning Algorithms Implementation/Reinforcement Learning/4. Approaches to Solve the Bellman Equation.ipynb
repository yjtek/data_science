{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c81132f9",
   "metadata": {},
   "source": [
    "# Approaches to Solve the Bellman Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f40f9c6",
   "metadata": {},
   "source": [
    "- We have established the basic setup for Reinforcement Learning by introducing the equilibrium equations of the State-Value and Action Value functions of the Bellman Equation\n",
    "\n",
    "- Now we look into **how** the Bellman Equations can be solved in practise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d06256",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "    \n",
    "    &\\text{State-Value Function:} \\\\\n",
    "    &\\quad v_{\\pi}(s) = \\sum_a \\pi(a|s) \\sum_{s'} P(s' | s, a) [R(s, a, s') + \\gamma v_{\\pi}(s')] \\\\ \\\\\n",
    "\n",
    "    &\\text{Action-Value Function:} \\\\\n",
    "    &\\quad q_{\\pi}(s,a) = \\sum_{s'} P(s' | s, a) [R(s, a, s') + \\gamma \\sum_{a'} \\pi(a'|s') q_{\\pi}(s', a')]\n",
    "\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e2aeec",
   "metadata": {},
   "source": [
    "## Overview of Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c86808f",
   "metadata": {},
   "source": [
    "- In RL, there are 2 types of tasks; **prediction**, and **control**\n",
    "    - **Prediction (Policy Evaluation)**: Given a policy $\\pi$, estimate the state value function $V_{\\pi}(s)$ and/or the action value function $Q_{\\pi}(s,a)$\n",
    "    - **Control (Policy Optimisation)**: Find the optimal policy $\\pi^*$ \n",
    "\n",
    "- Generally, given a choice to estimate $Q_{\\pi}(s,a)$ or $V_{\\pi}(s)$, it is preferable to estimate $Q_{\\pi}(s,a)$, because state values can be derived by averaging across the corresponding action values. Nontheless, there are techniques where we estimate state-value alongside action-value; say, in DP where we already know the transition probabilities and rewards.\n",
    "\n",
    "- In this noteboook, we'll go through a few methods used in RL, and we'll see what they solve, and how they finally use this to derive $\\pi^*$\n",
    "\n",
    "- Methods\n",
    "    - **Dynamic Programming**: Since we know both transition probabilities $P$ and rewards $R$, we can optimise for $\\pi^*$ directly until some convergence criteria is met, because with a fixed $P$ and $R$, there is a \"right answer\" for $\\pi^*$ with no uncertainty\n",
    "\n",
    "    - **Monte Carlo**: When we don't have the model (no $P$ or $R$), we learn from experience by running complete episodes. \n",
    "        - We solve for the **action-value function** $Q(s,a)$ by averaging the returns observed when taking action $a$ in state $s$. \n",
    "        - The policy $\\pi^*$ is then derived by acting greedily (or $\\epsilon$-greedy) with respect to $Q$: $$\\pi^*(s) = \\arg\\max_a Q(s,a)$$\n",
    "        - Unlike DP which learns state values $V$ and needs the model to extract actions, MC learns $Q$ directly which is immediately actionable\n",
    "    \n",
    "    - **Temporal Difference**: Also model-free, but unlike MC which waits for full episodes, TD methods update estimates incrementally after each step using bootstrapping. \n",
    "        - For control tasks, TD methods like Q-learning and SARSA learn $Q(s,a)$ directly, updating via $$Q(s,a) \\leftarrow Q(s,a) + \\alpha[r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)]$$. \n",
    "        - The policy is again extracted greedily from $Q_{\\pi}(s,a)$. \n",
    "        - TD can also learn state values $V$ for prediction tasks, but $V$ alone is insufficient for control without the model \n",
    "\n",
    "    - **Policy Gradient – Direct Optimization**: When the goal is to optimize the policy directly without relying on value functions, **policy gradient methods** parameterize the policy as $\\pi_\\theta(a|s)$ with parameters $\\theta$. \n",
    "        - The objective is to maximize the expected return $J(\\theta) = \\mathbb{E}[G | \\pi_\\theta]$. \n",
    "        - Using the **policy gradient theorem**, we can compute $\\nabla_\\theta J(\\theta)$ and update the policy via gradient ascent:  \n",
    "            $$\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta J(\\theta)$$\n",
    "        - Advantages: directly targets $\\pi^*$, naturally handles stochastic policies, and works for high-dimensional or continuous action spaces. \n",
    "        - Drawbacks: typically higher variance in updates and slower convergence than value-based methods.  \n",
    "\n",
    "    - **Actor–Critic Overview**: Combines **value-based** and **policy-based** approaches. The **actor** represents the policy $\\pi_\\theta(a|s)$ and is updated in the direction suggested by the **critic**, which estimates a value function $V(s)$ or $Q(s,a)$. \n",
    "        - The critic provides a low-variance estimate of the **advantage**:  $$A(s,a) = Q(s,a) - V(s)$$\n",
    "        - The actor is updated using this advantage to push probabilities toward better-than-expected actions, while the critic is trained with TD methods. \n",
    "        - This setup stabilizes training compared to pure policy gradients and allows learning in continuous or large action spaces efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180daa30",
   "metadata": {},
   "source": [
    "## Dynamic Programming (DP) - Model Based"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc4773e",
   "metadata": {},
   "source": [
    "- The simplest case of solving the Bellman Equation is when you have **both** the transition probabilities $P(s'|s,a)$ and the rewards $R(s,a,s')$\n",
    "\n",
    "- That is, you know the full Markov Decision Process (MDP). Hence the term \"model based\", because we are using the MDP model\n",
    "\n",
    "- This is, of course, exceedingly rare. But it is instructive to see how to solve for $\\pi^*$ when you have almost all available information\n",
    "\n",
    "- We will go through 2 ways of implementing the DP solver\n",
    "    - Policy Interation\n",
    "    - Value Iteration\n",
    "\n",
    "- To do this, let's assume the following set up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d544b54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "N_STATES = 20\n",
    "N_ACTIONS = 20\n",
    "GAMMA = 0.9\n",
    "\n",
    "# Shape corresponding to State, Action, Next State\n",
    "TRANSITION_PROBABILITIES = np.random.rand(N_STATES, N_ACTIONS, N_STATES)\n",
    "# Normalise so transition probabilities for a given state sums to 1 across all actions\n",
    "TRANSITION_PROBABILITIES /= np.sum(TRANSITION_PROBABILITIES, axis=2, keepdims=True)\n",
    "\n",
    "REWARDS = np.random.rand(N_STATES, N_ACTIONS, N_STATES) * 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54792600",
   "metadata": {},
   "source": [
    "### Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df82c340",
   "metadata": {},
   "source": [
    "- In policy iteration, we create a loop that does (i) policy evaluation, and (ii) policy improvement based on the evaluated outcomes of the policy, and the loop runs until some stopping criteria is reached\n",
    "\n",
    "- **Policy Evaluation:** \n",
    "    - Since we know the transition probabilities and rewards with full certainty, we can update the State-Value function by computing the Bellman Equation\n",
    "    \n",
    "    - Start with a seed array representing the State-Value function $V$, and a policy $\\pi$ that deterministically tells us what action to take in a given state $S$\n",
    "    \n",
    "    - Compute 1 pass of the Bellman State Value function update for all states. This is just a transition-probability-weighted average of the state values of the next states $S'$\n",
    "\n",
    "    - Check that the largest change in the state value estimates for all $N$ states exceeds some threshold $\\theta$. If even the largest change falls below the threshold, we assume convergence, and we return the state values $V$\n",
    "\n",
    "    - Otherwise, keep looping\n",
    "\n",
    "- **Policy Improvement**\n",
    "    - We've previously updated our estimate of the State-Value function $V$\n",
    "    - Now, we need to update our policy based on the new estimate of the State-Value function. That is, given a state $S$, I want to know the value of taking an action $A$. \n",
    "    - That is; the **Q-Value**\n",
    "\n",
    "    - Looping over every state $S$, we init an array to hold the new Q values\n",
    "\n",
    "    - Given $S$, loop over every action available, and update the Action-Value function $Q$. This is simply the transition-probability-weighted average of rewards from the transition $S -> S'$, plus the discounted state-value of $S'$\n",
    "\n",
    "    - Finally, the new policy for state $S$ is simply to take the argmax of all the Q values\n",
    "\n",
    "- **Policy Iteration**\n",
    "    - Bringing these 2 together, we simply create an infinite loop of `Evaluation --> Improvement --> Evalution ...`\n",
    "\n",
    "    - This will keep going until some convergence criteria is met. An easy on is to say that; if for some $N$ consecutive loops we do not change our policy, we have reached convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bbc794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "\n",
      "    Optimal Policy: [12 16 18 19 17  4 16  6 19 17 17  2 10 14 18 19 17  5  7  9]\n",
      "    Action-Value: [[30.94841478 31.32490423 32.19788999 31.9444741  31.79106206 31.84517046\n",
      "  31.290961   31.28625888 31.67130119 31.75888851 31.64210072 31.40075868\n",
      "  32.21000994 31.70431886 32.00087219 30.9218291  31.23823039 31.00114137\n",
      "  31.35150682 31.38025686]\n",
      " [31.09281456 31.19694478 31.74030897 31.97479618 31.94372851 31.00817106\n",
      "  31.06199313 31.43872014 31.59123965 31.17103483 31.83600553 31.7249459\n",
      "  31.14808322 31.37639636 31.84617734 30.88066009 32.05620933 31.25515259\n",
      "  31.48655157 31.54256258]\n",
      " [31.61237654 31.29259542 31.17586312 32.2678108  32.01706964 31.44388102\n",
      "  32.08529495 31.07045071 31.49056397 31.71061504 31.02807655 30.90594336\n",
      "  31.68019248 31.24952417 31.73166221 31.61723427 32.19383784 31.65359456\n",
      "  32.53945021 31.81073227]\n",
      " [32.05617899 31.82333462 31.68052992 31.05107583 32.03523231 31.60465759\n",
      "  31.81101442 32.20996754 31.45421427 31.10691636 31.48853139 31.84642156\n",
      "  31.11589792 31.31467339 32.0437155  32.0146979  31.45848263 31.74417274\n",
      "  32.09585302 32.3384664 ]\n",
      " [31.73863841 31.79437923 31.26959741 31.61433795 31.91880981 31.76264575\n",
      "  31.7015182  31.72896369 31.0475385  30.89541821 31.37098987 31.02817838\n",
      "  31.29544678 30.74502741 31.84256972 30.93572906 31.56248691 32.1741029\n",
      "  31.13068849 31.13810303]\n",
      " [31.30725565 31.74994061 32.07803162 31.76494607 32.48933637 31.92830815\n",
      "  31.48080026 32.25398118 32.35836861 31.60661809 31.67381875 31.84419746\n",
      "  31.59732025 31.24761026 30.89958002 31.63079221 31.81944675 31.8026537\n",
      "  31.29263384 31.47892525]\n",
      " [31.19914681 31.87098404 31.30279615 31.42725544 31.25796811 31.03716222\n",
      "  32.0786561  31.4378775  31.40512384 31.90228803 31.16887549 30.93126363\n",
      "  31.59232694 31.62862619 31.70811731 31.38529016 32.14463724 31.94279416\n",
      "  31.34277728 31.61328553]\n",
      " [30.99076531 31.98611845 31.50108865 31.24169256 31.40343523 31.80713212\n",
      "  32.24949883 31.80243454 31.19678341 31.43056289 31.15547386 31.97739334\n",
      "  31.69492248 31.14343988 31.44334112 31.69447457 31.20891624 31.70945872\n",
      "  31.7470955  31.39449481]\n",
      " [31.73974763 31.75936927 31.32384599 31.74107264 31.45514321 31.61443255\n",
      "  31.88283065 31.70003353 32.11492899 31.7595051  31.49787525 31.15752467\n",
      "  31.14647163 31.70585755 32.05244863 31.84606403 30.86549668 31.79064853\n",
      "  31.9322769  32.26843626]\n",
      " [31.86705003 32.18505948 31.61268483 31.05337451 31.78511483 31.45226405\n",
      "  31.17941384 32.12167851 31.30888033 32.01370938 31.69130246 30.66759393\n",
      "  31.19801981 31.86730941 31.71625711 31.38326528 31.11292379 32.42456183\n",
      "  31.25440488 31.42729077]\n",
      " [32.11268193 31.60577148 31.64621138 31.39743754 31.45765847 31.75252482\n",
      "  30.72417944 31.59956637 31.54681688 31.04292908 31.65556355 31.25213511\n",
      "  31.89816479 31.4966708  31.66103526 31.93792417 31.38619446 32.26555341\n",
      "  31.93315416 31.44142264]\n",
      " [31.21982368 31.45231734 32.31171931 31.84529471 32.20327994 31.12818211\n",
      "  31.68921563 31.22272172 30.66963594 31.4901787  31.48251072 30.97694317\n",
      "  31.97856286 31.74344459 31.55302903 31.99364432 31.30385192 32.16004078\n",
      "  31.8369227  31.94669736]\n",
      " [31.66687024 31.69543416 32.28528641 31.36477508 31.25502629 31.74038498\n",
      "  31.35236755 31.53067024 31.15892711 31.41422446 32.43796168 31.21149068\n",
      "  31.6923935  31.62583026 31.75940112 31.80169641 31.82230354 31.4328158\n",
      "  31.71080598 31.23174027]\n",
      " [31.28144885 32.06578808 31.84978565 31.98038203 31.36729104 30.92415163\n",
      "  31.85770731 31.47656368 31.74004652 31.22816112 31.46496453 32.0056815\n",
      "  31.63487328 31.8248629  32.09600004 31.59755102 31.41510532 31.96895501\n",
      "  31.82259359 31.8985461 ]\n",
      " [31.60219843 31.78844297 31.527741   31.6883589  32.13565232 31.64916927\n",
      "  31.56485247 32.07715346 32.21421801 31.33630067 31.4154296  31.72565975\n",
      "  32.12818703 31.80040456 31.54380552 32.15260988 31.9003869  32.21860983\n",
      "  32.64112773 31.84786638]\n",
      " [31.35009409 31.39085232 31.62147474 32.01977854 31.82582623 31.28768439\n",
      "  30.7865028  31.23015452 31.90238478 32.07313468 32.08282382 31.74585121\n",
      "  31.59994491 31.88294945 31.94730748 31.92049594 31.42331362 31.25665681\n",
      "  31.3207553  32.31968844]\n",
      " [31.82866916 31.28199027 31.97409584 31.56543434 31.63683595 31.64963511\n",
      "  32.43552999 32.00953337 31.21820727 31.63351445 31.21586037 32.14989531\n",
      "  31.91152083 31.84916614 31.54802035 31.79564734 31.46139793 32.60392454\n",
      "  31.78021976 31.77715831]\n",
      " [31.26553422 31.32845465 31.54795412 31.79594926 31.49521122 32.19392032\n",
      "  31.2404395  32.07791023 32.00867776 31.69155132 31.64494267 32.17402121\n",
      "  31.83059891 31.00661157 31.50820611 31.83676937 32.06024281 31.1534086\n",
      "  31.60925034 31.49076811]\n",
      " [32.17916304 32.00297501 31.26591192 32.00260364 31.40775653 31.05737754\n",
      "  31.64348826 32.53185635 31.62018266 31.91698287 31.20478233 31.23910548\n",
      "  31.95315765 31.01957898 32.08283366 31.31324996 31.62320826 32.00823627\n",
      "  32.34240564 31.56525483]\n",
      " [31.79562966 31.45378813 31.62153274 31.56904403 31.48662388 31.64228255\n",
      "  32.09806764 31.52527212 31.75200823 32.59174591 31.44387921 31.26473349\n",
      "  32.36785617 31.50118577 31.16649571 31.02920019 31.9273415  31.48583741\n",
      "  31.50964845 31.87387478]]\n",
      "    State-Value: [32.21000948 32.05620889 32.53944985 32.33846597 32.17410255 32.48933601\n",
      " 32.14463692 32.24949853 32.26843598 32.42456155 32.26555315 32.31171909\n",
      " 32.4379615  32.09599986 32.64112757 32.31968832 32.60392446 32.19392027\n",
      " 32.5318563  32.59174587]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def policy_evaluation(\n",
    "    transition_probabilities: np.ndarray,\n",
    "    rewards: np.ndarray,\n",
    "    curr_state_value_function: np.ndarray,\n",
    "    policy: np.ndarray,\n",
    "    gamma: float = 0.9,\n",
    "    theta: float = 1e-6\n",
    ") -> np.ndarray:\n",
    "    updated_state_value_function = curr_state_value_function.copy()\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for curr_state in range(N_STATES):\n",
    "            v_updated = sum([\n",
    "                transition_probabilities[curr_state, policy[curr_state], s_prime] * \n",
    "                (rewards[curr_state, policy[curr_state], s_prime] + gamma * updated_state_value_function[s_prime])\n",
    "                for s_prime in range(N_STATES)\n",
    "            ])\n",
    "            delta = max(delta, abs(v_updated - updated_state_value_function[curr_state]))\n",
    "            updated_state_value_function[curr_state] = v_updated\n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    return updated_state_value_function\n",
    "\n",
    "def policy_improvement(\n",
    "    transition_probabilities: np.ndarray,\n",
    "    rewards: np.ndarray,\n",
    "    curr_state_value_function: np.ndarray,\n",
    "    curr_action_value_function: np.ndarray,\n",
    "    curr_policy: np.ndarray,\n",
    "    gamma: float = 0.9\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    updated_policy = curr_policy.copy()\n",
    "    updated_action_value_function = curr_action_value_function.copy()\n",
    "    for s in range(N_STATES):\n",
    "        action_value_function_for_state_s = updated_action_value_function[s].copy()\n",
    "        for a in range(N_ACTIONS):\n",
    "            new_action_value = sum([\n",
    "                transition_probabilities[s, a, s_prime] * \n",
    "                (rewards[s, a, s_prime] + gamma * curr_state_value_function[s_prime])\n",
    "                for s_prime in range(N_STATES)\n",
    "            ])\n",
    "            action_value_function_for_state_s[a] = new_action_value\n",
    "        \n",
    "        updated_policy[s] = np.argmax(action_value_function_for_state_s)\n",
    "        updated_action_value_function[s] = action_value_function_for_state_s\n",
    "    return updated_policy, updated_action_value_function\n",
    "\n",
    "def policy_iteration(\n",
    "    transition_probabilities: np.ndarray,\n",
    "    rewards: np.ndarray,\n",
    "    gamma: float = 0.9,\n",
    "    theta: float = 1e-6\n",
    "):\n",
    "    curr_state_value_function: np.ndarray = np.zeros(N_STATES)\n",
    "    curr_action_value_function: np.ndarray = np.zeros((N_STATES, N_ACTIONS))\n",
    "    curr_policy: np.ndarray = np.zeros(N_STATES).astype(int)\n",
    "    \n",
    "    count_no_change = 0\n",
    "    count_iters = 0\n",
    "    while True:\n",
    "        print(count_iters)\n",
    "        updated_state_value_function = policy_evaluation(\n",
    "            transition_probabilities,\n",
    "            rewards,\n",
    "            curr_state_value_function,\n",
    "            curr_policy,\n",
    "            gamma,\n",
    "            theta\n",
    "        )\n",
    "        updated_policy, updated_action_value_function = policy_improvement(\n",
    "            transition_probabilities,\n",
    "            rewards,\n",
    "            updated_state_value_function,\n",
    "            curr_action_value_function,\n",
    "            curr_policy,\n",
    "            gamma\n",
    "        )\n",
    "        if np.array_equal(curr_policy, updated_policy):\n",
    "            count_no_change += 1\n",
    "            if count_no_change >= 5:\n",
    "                break\n",
    "        else:\n",
    "            count_no_change = 0\n",
    "\n",
    "        curr_policy = updated_policy.copy()\n",
    "        curr_action_value_function = updated_action_value_function.copy()\n",
    "        curr_state_value_function = updated_state_value_function.copy()\n",
    "\n",
    "        count_iters += 1\n",
    "    \n",
    "    return curr_policy, curr_action_value_function, curr_state_value_function\n",
    "\n",
    "pi_star, q_star, v_star = policy_iteration(TRANSITION_PROBABILITIES, REWARDS)\n",
    "\n",
    "print(f\"\"\"\n",
    "    Optimal Policy: {pi_star}\n",
    "    Action-Value: {q_star}\n",
    "    State-Value: {v_star}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e01a0a",
   "metadata": {},
   "source": [
    "- To ascertain if your policy iteration has converged correctly, check that the action values computed from the current transition probability, rewards, and optimal state values matches the values in your optimal q value array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a0d67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in range(N_STATES):\n",
    "    for a in range(N_ACTIONS):\n",
    "        q = sum([\n",
    "            TRANSITION_PROBABILITIES[s, a, s_prime] *\n",
    "            (REWARDS[s, a, s_prime] + GAMMA * v_star[s_prime])\n",
    "            for s_prime in range(N_STATES)\n",
    "        ])\n",
    "        assert np.isclose(q, q_star[s, a])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e4080c",
   "metadata": {},
   "source": [
    "### Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25862008",
   "metadata": {},
   "source": [
    "- In policy iteration, notice we have 2 distinct steps\n",
    "    - There is policy evaluation, which exhaustively uses given policy $\\pi$ to update the state value function $v$ until convergence\n",
    "    - Following which there is policy improvement, which uses the updated state value function $v$ to pick the best action $a$ for every state $s$\n",
    "\n",
    "- The key difference between policy iteration and value iteraton is in the order we perform these steps. The methods are exactly the same, For value iteration:\n",
    "    - We don't maintain an explicit policy. Instead, we focus on updating the state-value function $v$\n",
    "    - The action taken at each time step is simply the maximum of the action values computed using the current $v$\n",
    "\n",
    "- Note that there is no performance difference expected on average, and both approaches should converge to the same solution\n",
    "\n",
    "- TLDR; \n",
    "    - Policy Iteration does Loop(evaluate policy --> improve policy)\n",
    "    - Value Iteration does Loop(update state value function) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0069f088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(\n",
    "    transition_probabilities: np.ndarray,\n",
    "    rewards: np.ndarray,\n",
    "    gamma: float = 0.9,\n",
    "    theta: float = 1e-6\n",
    ") -> np.ndarray:\n",
    "    curr_state_value_function = np.zeros(N_STATES)\n",
    "    updated_state_value_function = np.zeros(N_STATES)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(N_STATES):\n",
    "            new_action_values = [\n",
    "                sum([\n",
    "                    transition_probabilities[s, a, s_prime] * \n",
    "                    (rewards[s, a, s_prime] + gamma * curr_state_value_function[s_prime])\n",
    "                    for s_prime in range(N_STATES)\n",
    "                ])\n",
    "                for a in range(N_ACTIONS)\n",
    "            ]\n",
    "            updated_state_value_function[s] = max(new_action_values)\n",
    "            delta = max(delta, abs(curr_state_value_function[s] - updated_state_value_function[s]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "            \n",
    "        curr_state_value_function = updated_state_value_function.copy()\n",
    "    \n",
    "    policy = np.zeros(N_STATES).astype(int)\n",
    "    for s in range(N_STATES):\n",
    "        action_values = [\n",
    "            sum(transition_probabilities[s, a, s_prime] * \n",
    "            (rewards[s, a, s_prime] + gamma * updated_state_value_function[s_prime])\n",
    "            for s_prime in range(N_STATES))\n",
    "            for a in range(N_ACTIONS)\n",
    "        ]\n",
    "        policy[s] = np.argmax(action_values)\n",
    "    \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a88c1f93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([12, 16, 18, 19, 17,  4, 16,  6, 19, 17, 17,  2, 10, 14, 18, 19, 17,\n",
       "         5,  7,  9]),\n",
       " array([32.21000379, 32.0562032 , 32.53944408, 32.33846026, 32.17409677,\n",
       "        32.48933021, 32.14463111, 32.2494927 , 32.26843013, 32.42455566,\n",
       "        32.26554727, 32.31171317, 32.43795555, 32.0959939 , 32.64112159,\n",
       "        32.31968229, 32.60391839, 32.1939142 , 32.53185021, 32.59173977]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_iteration(TRANSITION_PROBABILITIES, REWARDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a05a8b",
   "metadata": {},
   "source": [
    "## Monte Carlo - Sampling-Based"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77280b3d",
   "metadata": {},
   "source": [
    "- We've covered the DP approach, which looks at the fairly restrictive case of solving the Bellman Equations when the full MDP is known; i.e. you know both the transition probability, and the reward. This is typically unrealistic. Very few process in reality will be kind enough to provide you both pieces of information perfectly. And even if you estimate both using data, it will be estimated under some uncertainty\n",
    "\n",
    "- Therefore, we need more general methods to solve for the optimal policy when the transition probabilities and rewards are not known\n",
    "\n",
    "- Here, we consider the Monte-Carlo approach. \n",
    "\n",
    "- Note that the traditional Monte-Carlo approach assumes an **episodic task**. That is, the iteration will run for some time, and reach a terminal point.\n",
    "    - You can manually impose a terminal point for Monte-Carlo, but note that this isn't a true Monte Carlo approach, but a **truncated** Monte Ccarlo\n",
    "\n",
    "- Idea:\n",
    "    - The idea of the monte carlo is to simply let entire episodes play out multiple times, to find the long term average state-value function and hence the optimal policy\n",
    "    - So you have a `step()` method that determines 2 things; (i) what is the next state you are in, and (ii) is this next state a terminal one?\n",
    "    - Having done this, you now have 2 series recorded (i) a series of states, ending with a terminal state, and (ii) a series of rewards, indicating the reward you get for landing on that state\n",
    "        - Note that the series of rewards is NOT the sum of your future rewards (i.e. not the true Q-value). It is the specific reward of reaching some state $S$\n",
    "    - Now, for every point we land on in our array, we want to compute the discounted future rewards observed from that point, so we can derive a proper state value\n",
    "    - Init an array to hold the state-value function $V$\n",
    "    - We'll compute this in reverse order of the rewards array;\n",
    "        - Init $G$ as the running total of the discounted rewards. This starts at 0\n",
    "        - We know that the last state is terminal. Suppose this is the rightmost terminal state. \n",
    "            - We know that reaching this state gives a reward of 1. \n",
    "            - We also know that, this being the terminal state, there is no future reward. \n",
    "            - So the cumulative future reward $G = 1$\n",
    "            - Since we know the rightmost state $N$ has value $G=1$, update its state-value function $V[N]$ by taking $V[N] + \\alpha[G - V[N]]$\n",
    "        - Moving to the second last state. \n",
    "            - We know that this state has reward 0. Therefore, the value of this state comes only from the discounted value of transitioning to the last state and receiving the terminal reward 1. \n",
    "            - Thus, update $G = reward[N-1] + \\gamma \\cdot G$\n",
    "            - Then, update the state value function $V[N-1] + \\alpha[G - V[N-1]]$\n",
    "        - Iterate until you reach the end of the episode \n",
    "    - Continue to the next episode for a fixed number of episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616d4bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "# Environment setup; we create a number line with 10 states. The right-most state has a reward of 1\n",
    "# Therefore, train the RL loop to take rightward actions [+1] more than leftward actions [-1]\n",
    "# Terminate if you reach the left or rightmost states\n",
    "N_STATES = 5\n",
    "N_ACTIONS = 5\n",
    "GAMMA = 0.9\n",
    "ALPHA = 0.1\n",
    "EPSILON = 0.5\n",
    "EPISODES = 1000\n",
    "TD_N = 3\n",
    "\n",
    "TRANSITION_PROBABILITIES = np.random.rand(N_STATES, N_ACTIONS, N_STATES)\n",
    "TRANSITION_PROBABILITIES /= np.sum(TRANSITION_PROBABILITIES, axis=2, keepdims=True)\n",
    "REWARDS = np.random.rand(N_STATES, N_ACTIONS, N_STATES) * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16a95dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "episode_step = namedtuple('episode_step', ['curr_state', 'action', 'new_state', 'reward'])\n",
    "\n",
    "def take_action(curr_state: int, action_values: np.ndarray) -> tuple[int]:\n",
    "    action_values_for_curr_state = action_values[curr_state, :]\n",
    "    \n",
    "    if np.random.rand() >= EPSILON:\n",
    "        highest_action_value_action = np.argmax(action_values_for_curr_state)\n",
    "        return highest_action_value_action\n",
    "    else:\n",
    "        random_action = np.random.choice(N_ACTIONS)\n",
    "        return random_action\n",
    "\n",
    "def monte_carlo_epsilon_greedy():\n",
    "    action_values = np.zeros((N_STATES, N_ACTIONS))\n",
    "    state_values = np.zeros(N_STATES)\n",
    "\n",
    "    rewards_intermediate_store = {\n",
    "        (s,a): [] for s in range(N_STATES) for a in range(N_ACTIONS)\n",
    "    }\n",
    "    for _ in range(EPISODES):\n",
    "        curr_state = np.random.choice(list(range(1,N_STATES-1)))\n",
    "        episode_history = []\n",
    "\n",
    "        terminal_state_reached = curr_state in [0, N_STATES-1]\n",
    "        while not terminal_state_reached:\n",
    "            action = take_action(curr_state, action_values)\n",
    "            new_state = np.random.choice(\n",
    "                N_STATES, p=TRANSITION_PROBABILITIES[curr_state, action]\n",
    "            )\n",
    "            terminal_state_reached = new_state in [0, N_STATES-1]\n",
    "            reward = REWARDS[curr_state, action, new_state]\n",
    "            episode_history.append(\n",
    "                episode_step(curr_state=curr_state, action=action, new_state=new_state, reward=reward)\n",
    "            )\n",
    "            curr_state = new_state\n",
    "        \n",
    "        return_val = 0\n",
    "        for step in reversed(episode_history):\n",
    "            return_val = step.reward + GAMMA * return_val\n",
    "            rewards_intermediate_store[(step.curr_state, step.action)].append(return_val)\n",
    "        \n",
    "        for curr_state, action in rewards_intermediate_store.keys():\n",
    "            action_values[curr_state, action] = np.mean(rewards_intermediate_store.get((curr_state, action))) if rewards_intermediate_store.get((curr_state, action)) != [] else 0\n",
    "        \n",
    "        state_values = np.max(action_values, axis=1)\n",
    "    \n",
    "    return state_values, action_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "be80aaa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.        , 11.06340833,  9.65237593, 11.93171671,  0.        ]),\n",
       " array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 7.31191006, 11.06340833,  9.40697801,  7.43068361,  5.76089819],\n",
       "        [ 8.37778159,  9.59896629,  9.65237593,  8.6572607 ,  7.6457071 ],\n",
       "        [ 9.9728158 ,  8.30667043,  7.33269837,  9.40128791, 11.93171671],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ]]))"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monte_carlo_epsilon_greedy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5734220e",
   "metadata": {},
   "source": [
    "## Temporal-Difference (TD) - Bootstrapped Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c741ed01",
   "metadata": {},
   "source": [
    "- The motivation for this is similar to the Monte Carlo approach; this is used when we can't solve the Bellman Equation because transition probabilities and/or rewards are not known\n",
    "\n",
    "- Unlike Monte Carlo, which relies on the assumption that the task is episodic (i.e. terminates at some point), TD lets you update state value functions and policy incrementally. Therefore, TD is applied to tasks with no natural termination state\n",
    "\n",
    "- We'll cover 2 forms of TD;\n",
    "    - **TD(N)**: N-Step TD Learning\n",
    "    - **TD($\\lambda$)**: $\\lambda$ Return TD Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcd4d91",
   "metadata": {},
   "source": [
    "### N-Step TD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002f9dae",
   "metadata": {},
   "source": [
    "- In the N-Step TD, we want to update either the state value function $V(S_t)$ or action-value function $Q(S_t, a)$ based on some observed reward plus the estimated value of the next state.\n",
    "    - The $N$ simply tells you how many observed rewards we use in updating our current state value $V(S_t)$\n",
    "\n",
    "- TD doesn't dictate whether you should learn $Q$ or $V$; it simply gives an approach to learn these by updating an estimate using the difference between successive estimates; or the **TD Error**. This is defined as:\n",
    "\n",
    "\\begin{aligned}\n",
    "    \\delta_t &= R_t + \\gamma V(S_{t+1}) - V(S_t)\n",
    "\\end{aligned}\n",
    "\n",
    "- For example, in `TD(0)`, the updated state-value is the current state-value plus some learning rate multipled by the observed deviation from your current state-value. The observed deviation is the return you experience $R_t + \\gamma V(S_{t+1})$ minus the current estimated expected return $V(S_t)$\n",
    "\\begin{aligned}\n",
    "    V(S_t) &= V(S_t) + \\alpha (\\hat{G_t^{(1)}} - V(S_t)) \\\\\n",
    "    &= V(S_t) + \\alpha (R_t + \\gamma V(S_{t+1}) - V(S_t))\n",
    "\\end{aligned}\n",
    "    - Here, we update $V(S_t)$ at every step. So $G_t^{(1)}$ is not known, but your best guess at the time step.\n",
    "    - This is why some tutorials mention that `TD(N)` approach requires bootstrapping; it comes from the fact that $\\hat{G_t^{(1)}} = R_t + \\gamma V(S_{t+1})$ is an estimate based on $V(S_{t+1})$\n",
    "    - This is known as the **1-step bootstrap return**\n",
    "    - This differs from something like Monte Carlo, for example, where episodes are played out till the end, and we do not need to bootstrap because we use the actual rewards $R_t, R_{t+1}, ...$ to compute our state-value function instead of some intermediate state-value estimate $V(S_{t+1})$\n",
    "\n",
    "- Extending this idea, for `TD(N)`, we accumulate $N$ steps of the future rewards $R_t, R_{t+1}, ... R_{t+n}$ plus the estimated state value $V(S_{t+n})$ to compute the **TD error** \n",
    "\\begin{aligned}\n",
    "    V(S_t) &= V(S_t) + \\alpha (\\hat{G_t^{(N)}} - V(S_t)) \\\\\n",
    "    &= V(S_t) + \\alpha (\\sum_{i=0}^{n-1} \\gamma^{i} R_{t+i} +  \\gamma^n V(S_{t+n}) - V(S_t))\n",
    "\\end{aligned}\n",
    "    - Similar to the example above, $\\hat{G_t^{(N)}}$ is known as the **N-step bootstrap return**\n",
    "    - Fun fact; if N is the episode length, then this is simply Monte Carlo!\n",
    "\n",
    "- The intuition here is: if $V(S_t)$ has converged, then there should be no TD error observed, the current step return should equal the current state values. So $V(S_t)$ should be unchanged\n",
    "\n",
    "- Note that the $TD(N)$ idea can be applied to both **continuing** and **episodic** tasks. For clarity, we will implement both here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fc915a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "# Environment setup; we create a number line with 10 states. The right-most state has a reward of 1\n",
    "# Therefore, train the RL loop to take rightward actions [+1] more than leftward actions [-1]\n",
    "N_STATES = 10\n",
    "N_ACTIONS = 5\n",
    "GAMMA = 0.9\n",
    "ALPHA = 0.1\n",
    "EPSILON = 0.5\n",
    "EPISODES = 1000\n",
    "TD_N = 5\n",
    "\n",
    "N_STEPS = 10\n",
    "\n",
    "TRANSITION_PROBABILITIES = np.random.rand(N_STATES, N_ACTIONS, N_STATES)\n",
    "TRANSITION_PROBABILITIES /= np.sum(TRANSITION_PROBABILITIES, axis=2, keepdims=True)\n",
    "REWARDS = np.random.rand(N_STATES, N_ACTIONS, N_STATES) * 5\n",
    "\n",
    "TERMINAL_STATES = [0, N_STATES-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8926169b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "\n",
    "episode_step = namedtuple('episode_step', ['state', 'reward', 'next_state'])\n",
    "\n",
    "def update_timestep_is_valid(timestep_to_update: int) -> bool:\n",
    "    return timestep_to_update >= 0\n",
    "\n",
    "def next_state_is_terminal(next_state: int) -> bool:\n",
    "    return next_state in [0, N_STATES-1]\n",
    "\n",
    "def bootstrap_state_is_nonterminal(update_td_return_for_this_timestep, reward_steps_available, termination_time):\n",
    "    bootstrap_state_index = update_td_return_for_this_timestep + reward_steps_available\n",
    "    return bootstrap_state_index < termination_time\n",
    "\n",
    "def n_step_td_episodic(TD_N: int):\n",
    "    ## Init array to store state-values\n",
    "    V = np.zeros(N_STATES)\n",
    "\n",
    "    ## Going across all `EPISODES`\n",
    "    for _ in range(EPISODES):\n",
    "\n",
    "        ## Init current state such that it is not a terminal state\n",
    "        curr_state = np.random.randint(1, N_STATES - 1)\n",
    "        \n",
    "        ## For `TD_N`, we need to store N steps of state updates in the trajectory\n",
    "        trajectory = []\n",
    "        \n",
    "        ## For every episode, we define a variable to hold termination time. \n",
    "        ## This will be populated in the loop\n",
    "        termination_time = np.inf\n",
    "        \n",
    "        ## Set current episode time as 0\n",
    "        curr_time = 0  \n",
    "\n",
    "        ## Playing through the whole episode...\n",
    "        while True:\n",
    "\n",
    "            ## If curr_time is less than termination time, we make ONE transition\n",
    "            if curr_time < termination_time:\n",
    "\n",
    "                ## Pick an action\n",
    "                curr_action = np.random.randint(N_ACTIONS)\n",
    "\n",
    "                ## Make a transition\n",
    "                next_state = np.random.choice(N_STATES, p=TRANSITION_PROBABILITIES[curr_state, curr_action])\n",
    "\n",
    "                ## Get the reward of the transition\n",
    "                reward = REWARDS[curr_state, curr_action, next_state]\n",
    "\n",
    "                ## Store the transition in the trajectory history\n",
    "                trajectory.append(episode_step(curr_state, reward, next_state))\n",
    "\n",
    "                ## If we reach a terminal state\n",
    "                if next_state_is_terminal(next_state):\n",
    "                    ## Set termination time condition to the next time step. The condition `curr_time < termination_time` in the episode loop will therefore fail in the next step\n",
    "                    termination_time = curr_time + 1 \n",
    "                \n",
    "                ## Set current state to the next_state\n",
    "                curr_state = next_state\n",
    "\n",
    "            # Identify which index we should update. Remember that in TD(N), we need to accumulate N steps in our buffer before updating reward. Therefore, the index we update must always be TD_N - 1 steps behind. For example, to update the first state value index seen in our episode, if TD_N = 3, we need to have made 3 transitions. This happens at curr_time = 2, where we have (0 --> 1, 1 --> 2, 2 --> 3). At this point, we want to update index at 2 - 3 + 1 = 0 \n",
    "            update_td_return_for_this_timestep = curr_time - TD_N + 1\n",
    "\n",
    "            ## Assert that the update logic is valid (i.e. your update index >= 0)\n",
    "            if update_timestep_is_valid(update_td_return_for_this_timestep):\n",
    "                \n",
    "                ## Use `G` as the accumulator for the rewards\n",
    "                G = 0.0\n",
    "                \n",
    "                \n",
    "                ## Depending on the length of the episode, we may not be able to update using the full TD_N update. For example, let's say you want a TD(3) update. Through your simulation, you end up with a trajectory with 5 `episode_steps`, which correspond to the transitions (0 --> 1, 1 --> 2, 2 --> 3, 3 --> 4, 4 --> 5)\n",
    "\n",
    "                ## For TD(3), we need to have 3 rewards available before updating. This happens in the third timestep. So (0 --> 1, 1 --> 2, 2 --> 3). Therefore at time index 2, we can compute the TD return for the state at time index 0, which also happens to be state 0.\n",
    "\n",
    "                ## So this update goes on. At 4th timestep, we update time index 1. At 5th timestep, we update time index 2. Now, we reach the time to update time index 3. For this, we need (3 --> 4, 4 --> 5, 5 --> 6). But wait, since 5 state 5 at time index 5 is terminal, we don't have a 5 --> 6!! What do we do?\n",
    "\n",
    "                ## Instead of discarding usable reward information, TD(N) will make use of as many rewards as possible. Since we cannot do TD(3) without 5 --> 6, we do a TD(2) update instead!\n",
    "\n",
    "                ## Thus, here we keep track of the number of rewards we have available. When we reach termination, we are at time step 4, and termination_time = 5. But we want to keep updating our state value `V` even if a we don't meet the criteria for TD(3). So for fixed termination time 5, suppose we reach curr_time = 5. Then `update_td_return_for_this_timestep` becomes 5-3+1 = 3. To update index 3, we then say the number of rewards available is min(3, 5-3) = 2\n",
    "                reward_steps_available = min(\n",
    "                    TD_N, \n",
    "                    termination_time - update_td_return_for_this_timestep\n",
    "                )\n",
    "\n",
    "                ## For every reward available\n",
    "                for i in range(int(reward_steps_available)):\n",
    "                    ## Add the reward multiplied by the relevant gamma discount\n",
    "                    G += (GAMMA ** i) * trajectory[update_td_return_for_this_timestep + i].reward\n",
    "\n",
    "                ## if the time index to update + reward_steps_available is less than termination_time, it means that the next state value is available, because it is non terminal. So we can add the bootstrapped state-value \n",
    "\n",
    "                ## BUT if we get update_td_return_for_this_timestep + reward_steps_available == termination_time, then by definition, the last timestep must be terminal. Then there is no need to add the bootstrapped state value of the next state, because there is no \"next state\" after the terminal state\n",
    "                if bootstrap_state_is_nonterminal(reward_steps_available=reward_steps_available, update_td_return_for_this_timestep=update_td_return_for_this_timestep, termination_time=termination_time):\n",
    "\n",
    "                    ## After adding N rewards, add the bootstrapped state value of the next state. This only applies if we have not reached terminal state. Otherwise the value of the next state is 0 anyway, because there is no post-terminal transition\n",
    "                    \n",
    "                    G += (GAMMA ** reward_steps_available) * V[\n",
    "                        trajectory[\n",
    "                            update_td_return_for_this_timestep + reward_steps_available - 1\n",
    "                        ].next_state\n",
    "                    ]\n",
    "\n",
    "                # Update state value\n",
    "                state_to_update = trajectory[update_td_return_for_this_timestep].state\n",
    "                V[state_to_update] += ALPHA * (G - V[state_to_update])\n",
    "\n",
    "            curr_time += 1\n",
    "\n",
    "            # Break out of the while loop if our update value is termination_time - 1 (because there are no further rewards after this step)\n",
    "            if update_td_return_for_this_timestep == termination_time - 1:\n",
    "                break\n",
    "\n",
    "    return V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e1fd8605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  9.65660816, 10.57941725,  9.19019031,  7.76120898,\n",
       "        8.84684624,  9.45925917,  8.59510287,  9.85313675,  0.        ])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_step_td_episodic(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "27a0a3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque, namedtuple\n",
    "import numpy as np\n",
    "\n",
    "episode_step = namedtuple('episode_step', ['state', 'reward', 'next_state'])\n",
    "\n",
    "def n_step_td_continuing(steps: int = N_STEPS):\n",
    "    \"\"\"\n",
    "    N-step TD for continuing tasks.\n",
    "    \"\"\"\n",
    "\n",
    "    V = np.zeros(N_STATES, dtype=float)\n",
    "\n",
    "    # Store \n",
    "    td_n_buffer = deque(maxlen=TD_N)\n",
    "\n",
    "    # Start at a random (non-terminal) state\n",
    "    curr_state = np.random.randint(1, N_STATES - 1)\n",
    "\n",
    "    for _ in range(steps):\n",
    "        # Random action (ε-greedy or random for continuing case)\n",
    "        action = np.random.randint(N_ACTIONS)\n",
    "\n",
    "        # Sample next state using transition probabilities\n",
    "        next_state = np.random.choice(N_STATES, p=TRANSITION_PROBABILITIES[curr_state, action])\n",
    "\n",
    "        # Sample reward for (s, a, s_next)\n",
    "        reward = REWARDS[curr_state, action, next_state]\n",
    "\n",
    "        # Append to buffers\n",
    "        td_n_buffer.append(episode_step(state=curr_state, reward=reward, next_state=next_state))\n",
    "\n",
    "        # When we have enough samples, perform an update\n",
    "        G = 0\n",
    "        if len(td_n_buffer) == TD_N:\n",
    "            for t, step in enumerate(td_n_buffer):\n",
    "                G += (GAMMA ** t) * step.reward\n",
    "            G += (GAMMA ** TD_N) * V[td_n_buffer[-1].next_state]  # bootstrap from S_{t+n}\n",
    "\n",
    "            state_to_update = td_n_buffer[0].state\n",
    "            V[state_to_update] += ALPHA * (G - V[state_to_update])\n",
    "\n",
    "            # Slide window manually\n",
    "            td_n_buffer.popleft()\n",
    "\n",
    "        curr_state = next_state\n",
    "\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2cd97667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([25.2924899 , 25.06303537, 24.91916825, 24.80299663, 25.0760952 ,\n",
       "       25.72068895, 25.83358932, 25.0155259 , 24.44503252, 24.915488  ])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_step_td_continuing(20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978fa5b8",
   "metadata": {},
   "source": [
    "### $\\lambda$ Return TD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680cfcf0",
   "metadata": {},
   "source": [
    "- Having discussed $TD(N)$, we now move on to $TD(\\lambda)$. This is very similar to the $TD(N)$ case, we want to use successive TD errors to update either the state value $V(s)$ or action value $Q(s,a)$\n",
    "\n",
    "- Recall in $TD(N)$ that we define some $N$ steps of reward accumulation before bootstrapping an update\n",
    "\n",
    "\\begin{aligned}\n",
    "    G^{(n)}_t &= R_{t+1} + \\gamma R_{t+2} + ... + \\gamma^{n-1} R_{t+n} + \\gamma^n V(S_{t+n})\n",
    "\\end{aligned}\n",
    "\n",
    "- The question arises; what value of $N$ do we choose? \n",
    "    - If $N$ is small, we update fast, and therefore the variance is low, because we each step is small. But because we don't accumulate the update over multiple observations of rewards, the estimate is more biased\n",
    "    - If $N$ is large, we update slowly averaging over more reward steps. Thus, the bias is lower. But since each update takes longer to accumulate, the variance is also larger\n",
    "\n",
    "- To resolve this problem of choosing $N$, we introduce $TD(\\lambda)$\n",
    "\n",
    "- $TD(\\lambda)$ uses **all** possible future steps without bootstrapping, but **exponentially weights** them using $\\lambda \\in [0, 1)$. The idea here is that you don't need to choose which steps are important; since you are using all of them, they are all important. You just determine how best to weight them using $\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d18e08d",
   "metadata": {},
   "source": [
    "- Formally, the $TD(\\lambda)$ setup is:\n",
    "\n",
    "\\begin{aligned}\n",
    "    G^{(\\lambda)}_t &= (1 - \\lambda) \\sum_{n=1}^{\\infty} \\lambda^{n-1} G_t^{(n)}\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200a6d42",
   "metadata": {},
   "source": [
    "- Why geometric weighting? Because geometric weights are guaranteed to sum to 1. Looking at the weights:\n",
    "\n",
    "\\begin{aligned}\n",
    "    w_n &= (1 - \\lambda) \\sum_{n=1}^{\\infty} \\lambda^{n-1} \\\\\n",
    "    &= (1 - \\lambda) \\sum_{n=0}^{\\infty} \\lambda^{n} \\\\\n",
    "    &= (1 - \\lambda) \\cdot \\frac{1}{1 - \\lambda} & \\text{by geometric series} \\\\\n",
    "    &= 1\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30612fb3",
   "metadata": {},
   "source": [
    "- Since the geometric weights only sum to 1 in the limit, $TD(\\lambda)$ should conceptually only be valid for continuous learning tasks, not episodic task. In theory, you probably could apply it to episodic tasks, and lose the \"weighted average\" interpretation of the weights. But for the purposes of accuracy, we'll focus on the continuous learning task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec4de4b",
   "metadata": {},
   "source": [
    "#### General form of $TD(\\lambda)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0333d8",
   "metadata": {},
   "source": [
    "- Let $G_t(N)$ be the $TD(N)$ return\n",
    "\\begin{aligned}\n",
    "    G_t(1) &= R_{t+1} + \\gamma V(S_{t+1}) \\\\\n",
    "    G_t(2) &= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 V(S_{t+3}) \\\\\\\n",
    "    G_t(3) &= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\gamma^3 V(S_{t+4})\n",
    "    ...\n",
    "\\end{aligned}\n",
    "\n",
    "- Recall that in $TD(\\lambda)$, we are simply taking the exponentially weighted sums of the $TD(N)$ returns. Therefore:\n",
    "\n",
    "\\begin{aligned}\n",
    "    G_t(\\lambda) &= (1 - \\lambda) \\cdot [\\lambda^0 G_t(1) + \\lambda^1 G_t(2) + \\lambda^2 G_t(3) + ...] \\\\\n",
    "    &= (1 - \\lambda) \\cdot [ R_{t+1} + \\gamma V(S_{t+1}) + \\lambda (R_{t+1} + \\gamma R_{t+2} + \\gamma^2 V(S_{t+2})) + \\lambda^2 (R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\gamma^3 V(S_{t+3})) + ...] \\\\\n",
    "    &= (1 - \\lambda) \\cdot [ (R_{t+1} + \\lambda R_{t+1} + \\lambda^2 R_{t+1} + ...) + (\\lambda \\gamma R_{t+2} + \\lambda^2 \\gamma R_{t+2} + ...) + (\\lambda^2 \\gamma^2 R_{t+3} + \\lambda^3 \\gamma^2 R_{t+3} + ...) + ... + (\\gamma V_{s+1} + \\lambda \\gamma^2 V_{s+2} + \\lambda^2 \\gamma^3 V_{s+3} + ...)] \\\\\n",
    "\\end{aligned}\n",
    "\n",
    "- Let's first focus on the weights $W(R_{t+k})$ of the reward terms:\n",
    "\n",
    "\\begin{aligned}\n",
    "    W(R_{t+1}) &= (1 - \\lambda) [1 + \\lambda + \\lambda^2 + ...] \\\\\n",
    "    &= (1 - \\lambda) \\frac{1}{1 - \\lambda} \\\\\n",
    "    &= 1 \\\\ \\\\\n",
    "\n",
    "    W(R_{t+2}) &= (1 - \\lambda) [\\lambda \\gamma + \\lambda^2 \\gamma + \\lambda^3 \\gamma ...] \\\\\n",
    "    &= (1 - \\lambda) \\frac{\\lambda \\gamma}{1 - \\lambda} \\\\\n",
    "    &= \\lambda \\gamma \\\\ \\\\\n",
    "\n",
    "    W(R_{t+3}) &= (1 - \\lambda) [\\lambda^2 \\gamma^2 + \\lambda^3 \\gamma^2 + \\lambda^4 \\gamma^2 ...] \\\\\n",
    "    &= (1 - \\lambda) \\frac{\\lambda^2 \\gamma^2}{1 - \\lambda} \\\\\n",
    "    &= \\lambda^2 \\gamma^2 \\\\ \\\\\n",
    "\\end{aligned}\n",
    "\n",
    "- From the pattern above, for a given $R_{t+k}$, we see that as $\\lambda \\rightarrow 1$, we will get $(\\lambda \\gamma)^{k-1} = \\gamma^{k-1}$\n",
    "\n",
    "\\begin{aligned}\n",
    "    W(R_{t+k}) &= (1 - \\lambda) [\\lambda^{k-1} \\gamma^{k-1} + \\lambda^{k} \\gamma^{k-1} + \\lambda^{k+1} \\gamma^{k-1} + ...] \\\\\n",
    "    &= (1 - \\lambda) \\gamma^{k-1} \\sum_{i=k-1}^{\\infty} \\lambda^i \\\\\n",
    "    &= (1 - \\lambda) \\gamma^{k-1} \\frac{\\lambda^{k-1}}{1 - \\lambda} \\\\\n",
    "    &= (\\gamma \\lambda)^{k-1} \n",
    "\\end{aligned}\n",
    "\n",
    "- Next, we focus on the weight on the bootstrap term for each return $G^{(n)}_t$, which takes the general form $W_V(S_{t+k})$. Recall that we previously accounted for all reward weights, so we collect the $1 - \\lambda$ term into the weight of the bootstrap term:\n",
    "\n",
    "\\begin{aligned}\n",
    "    W_V(S_{t+k}) &= (1 - \\lambda) \\lambda^{k-1} \\gamma^{k} \\\\\n",
    "\\end{aligned}\n",
    "\n",
    "- As such, we arrive at the key $TD(\\lambda)$ formulation:\n",
    "\n",
    "\\begin{aligned}\n",
    "    G_t(\\lambda) &= (1 - \\lambda) \\cdot [\\lambda^0 G_t(1) + \\lambda^1 G_t(2) + \\lambda^2 G_t(3) + ...] \\\\\n",
    "    &= (1 - \\lambda) \\cdot [ R_{t+1} + \\gamma V(S_{t+1}) + \\lambda (R_{t+1} + \\gamma R_{t+2} + \\gamma^2 V(S_{t+2})) + \\lambda^2 (R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\gamma^3 V(S_{t+3})) + ...] \\\\\n",
    "    &= (1 - \\lambda) \\cdot [ (R_{t+1} + \\lambda R_{t+1} + \\lambda^2 R_{t+1} + ...) + (\\lambda \\gamma R_{t+2} + \\lambda^2 \\gamma R_{t+2} + ...) + (\\lambda^2 \\gamma^2 R_{t+3} + \\lambda^3 \\gamma^2 R_{t+3} + ...) + ... + (\\gamma V_{s+1} + \\lambda \\gamma^2 V_{s+2} + \\lambda^2 \\gamma^3 V_{s+3} + ...)] \\\\\n",
    "    &= \\sum_{k=1}^{\\infty} (\\gamma \\lambda)^{k-1}  R_{t+k} + (1 - \\lambda) \\sum_{n=1}^{\\infty} \\lambda^{n-1} \\gamma^n V(S_{t+n})\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba2071f",
   "metadata": {},
   "source": [
    "#### Boundary values of $\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97454641",
   "metadata": {},
   "source": [
    "- It's useful to see what happens to the formulation at the boundary values of $\\lambda$\n",
    "\n",
    "- If $\\lambda = 0$, all powers of $\\lambda^{n-1} = 0$, except for the case $0^0 = 1$. So $TD(\\lambda)$ reduces to a $TD(N=0)$ return\n",
    "\\begin{aligned}\n",
    "    G^{(\\lambda)}_t &= (1 - \\lambda) \\sum_{n=1}^{\\infty} \\lambda^{n-1} G_t^{(n)} \\\\\n",
    "    &= \\sum_{n=1}^{\\infty} \\lambda^{n-1} G_t^{(n)} \\\\\n",
    "    &= G_t^{(1)} \\\\\n",
    "    &= R_{t+1} + \\gamma V(S_{t+1})\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e180c4",
   "metadata": {},
   "source": [
    "- If $\\lambda \\rightarrow 1$, in the limit, $TD(\\lambda=1)$ simply becomes Monte Carlo, where we take the full episodic reward\n",
    "\n",
    "\\begin{aligned}\n",
    "G_t(\\lambda) &= \\sum_{k=1}^{\\infty} (\\gamma \\lambda)^{k-1}  R_{t+k} + (1 - \\lambda) \\sum_{n=1}^{\\infty} \\lambda^{n-1} \\gamma^n V(S_{t+n}) \\\\\n",
    "&= \\sum_{k=1}^{\\infty} (\\gamma)^{k-1}  R_{t+k} \\\\\n",
    "&= R_{t+1} + \\gamma R_{t+2} + ...\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d95d9b",
   "metadata": {},
   "source": [
    "#### Backward vs Forward Views"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee331ad4",
   "metadata": {},
   "source": [
    "- Having gone through the theory of $TD(\\lambda)$, there is an immediate gotcha when implementing - it's not possible to actually accumulate infinite reward terms to compute $G$\n",
    "\n",
    "\\begin{aligned}\n",
    "    G_t(\\lambda) &= (1 - \\lambda) \\sum_{n=1}^{\\infty} \\lambda^{n-1} G_t(n)\n",
    "\\end{aligned}\n",
    "\n",
    "- Let's consider another view of this. Recall that in TD learning, we are trying to incrementally update some value function by the TD error:\n",
    "\n",
    "\\begin{aligned}\n",
    "    V(S_t) \\leftarrow V(S_t) + \\alpha [G_t(\\lambda) - V(S_t)]\n",
    "\\end{aligned}\n",
    "\n",
    "- Let's call $G_t(\\lambda) - V(S_t)$ the $\\lambda$ return error\n",
    "\n",
    "- **CLAIM:** Since we cannot compute $G_t(\\lambda)$ by taking the infinite sum of $TD(N)$ terms (the **forward view**), we can replace the $\\lambda$ return error with the weighted sum to infinity of one step TD errors (**backward view**):\n",
    "\n",
    "\\begin{aligned}\n",
    "    G_t(\\lambda) - V(S_t) &= \\sum_{k=0}^{\\infty} (\\gamma \\lambda)^{k} \\delta_{t+k} \\\\ \\\\\n",
    "\n",
    "    \\delta_{t+k} &= R_{t+k} + \\gamma V(S_{t+k+1}) - V(S_{t+k})\n",
    "\\end{aligned}\n",
    "\n",
    "- We previously said that we cannot accumulate infinite rewards before computing $G_t(\\lambda)$. But isn't this just another sum to infinity?\n",
    "    - Yes, it is. So this also cuts off at some arbitrary point. \n",
    "    - But unlike summing the TD(N) terms, we don't need a buffer to accumulate rewards in this approach! \n",
    "    - Since updates are happening for every $\\delta_{t+k}$, and $\\delta_{t+k}$ can be computed at every step, there is no delay in the updating of the state value at time $t$, so long as we can accept bootstrapped values of the next state $V(S_{t+k+1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0e84aa",
   "metadata": {},
   "source": [
    "##### Proof that backward = forward view\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7153537",
   "metadata": {},
   "source": [
    "- Let's start with definitions:\n",
    "\n",
    "\\begin{aligned}\n",
    "    \\delta_t &= R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) \\\\\n",
    "    G_t(n) &= \\sum_{j=1}^{n} \\gamma^{j-1} R_{t+j} + \\gamma^n V(S_{t+n}) \\\\\n",
    "    G_t(\\lambda) &= (1 - \\lambda) \\sum_{n=1}^{\\infty} \\lambda^{n-1} G_t(n) \\\\ \n",
    "    &= \\sum_{k=1}^{\\infty} (\\gamma \\lambda)^{k-1}  R_{t+k} + (1 - \\lambda) \\sum_{n=1}^{\\infty} \\lambda^{n-1} \\gamma^n V(S_{t+n}) & \\text{Forward View}\n",
    "\\end{aligned}\n",
    "\n",
    "- Looking at the RHS of the equation to prove, let's call the summation of discounted 1-step TD errors $S$. So we have\n",
    "\n",
    "\\begin{aligned}\n",
    "    S &= \\sum_{k=0}^{\\infty} (\\gamma \\lambda)^{k} \\delta_{t+k}\n",
    "\\end{aligned}\n",
    "\n",
    "- Now, by the definition above $\\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)$\n",
    "\n",
    "\\begin{aligned}\n",
    "    S &= \\sum_{k=0}^{\\infty} (\\gamma \\lambda)^{k} \\delta_{t+k} \\\\\n",
    "    &= \\sum_{k=0}^{\\infty} (\\gamma \\lambda)^{k} [R_{t+k+1} + \\gamma V(S_{t+k+1}) - V(S_{t+k})] \\\\\n",
    "    &= \\underbrace{\\sum_{k=0}^{\\infty} (\\gamma \\lambda)^k R_{t+k+1}}_{(A)} + \n",
    "    \\underbrace{\\sum_{k=0}^{\\infty} (\\gamma \\lambda)^k \\gamma V(S_{t+k+1})}_{(B)} - \n",
    "    \\underbrace{\\sum_{k=0}^{\\infty} (\\gamma \\lambda)^k V(S_{t+k})}_{(C)}\n",
    "\\end{aligned}\n",
    "\n",
    "- Let's rewrite $(C)$ by separating out the first term of the summation, when $k=0$\n",
    "\n",
    "\\begin{aligned}\n",
    "    (C) &= \\sum_{k=0}^{\\infty} (\\gamma \\lambda)^k V(S_{t+k}) \\\\\n",
    "    &= V(S_{t+k}) + \\sum_{k=1}^{\\infty} (\\gamma \\lambda)^k V(S_{t+k}) \\\\\n",
    "\\end{aligned}\n",
    "\n",
    "- Then, rewriting $S$\n",
    "\n",
    "\\begin{aligned}\n",
    "    S &= \\sum_{k=0}^{\\infty} (\\gamma \\lambda)^{k} \\delta_{t+k} \\\\\n",
    "    &= (A) + (B) - (C) \\\\\n",
    "    &= (A) + \\underbrace{(B) - \\sum_{k=1}^{\\infty} (\\gamma \\lambda)^k V(S_{t+k})}_{(D)} - V(S_{t+k})\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63679e16",
   "metadata": {},
   "source": [
    "- Notice that the term we extracted can be combined with $(B)$, which leads to \n",
    "\n",
    "\\begin{aligned}\n",
    "    (D) &= \\sum_{k=0}^{\\infty} (\\gamma \\lambda)^k \\gamma V(S_{t+k+1}) - \\sum_{k=1}^{\\infty} (\\gamma \\lambda)^k V(S_{t+k}) \\\\\n",
    "    &= \\sum_{k=1}^{\\infty} (\\gamma \\lambda)^{k-1} \\gamma V(S_{t+k}) - \\sum_{k=1}^{\\infty} (\\gamma \\lambda)^k V(S_{t+k}) \\\\\n",
    "    &= \\sum_{k=1}^{\\infty} (\\gamma \\lambda)^{k-1} \\gamma V(S_{t+k}) - (\\gamma \\lambda)^k V(S_{t+k}) \\\\\n",
    "    &= \\sum_{k=1}^{\\infty} V(S_{t+k}) [(\\gamma \\lambda)^{k-1} \\gamma - (\\gamma \\lambda)^k] \\\\\n",
    "    &= \\sum_{k=1}^{\\infty} V(S_{t+k}) (\\gamma \\lambda)^{k-1} [\\gamma - \\gamma \\lambda] \\\\\n",
    "    &= \\sum_{k=1}^{\\infty} V(S_{t+k}) (\\gamma \\lambda)^{k-1} \\gamma (1 - \\lambda) \\\\\n",
    "    &= (1 - \\lambda) \\sum_{k=1}^{\\infty} \\gamma^k \\lambda^{k-1} V(S_{t+k}) \\\\\n",
    "\\end{aligned}\n",
    "\n",
    "- Looking at the forward view, this is exactly the bootstrap term!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97989f0e",
   "metadata": {},
   "source": [
    "- Simiarly, let's rewrite $(A)$\n",
    "\n",
    "\\begin{aligned}\n",
    "    (A) &= \\sum_{k=0}^{\\infty} (\\gamma \\lambda)^k R_{t+k+1} \\\\\n",
    "    &= \\sum_{k=1}^{\\infty} (\\gamma \\lambda)^{k-1} R_{t+k} \\\\\n",
    "\\end{aligned}\n",
    "\n",
    "- Going back to the forward view again, this is exactly the discounted reward term!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0801746e",
   "metadata": {},
   "source": [
    "- So going back to $S$\n",
    "\n",
    "\\begin{aligned}\n",
    "    S &= \\sum_{k=0}^{\\infty} (\\gamma \\lambda)^{k} \\delta_{t+k} \\\\\n",
    "    &= \\underbrace{\\sum_{k=0}^{\\infty} (\\gamma \\lambda)^k R_{t+k+1}}_{(A)} + \n",
    "    \\underbrace{\\sum_{k=0}^{\\infty} (\\gamma \\lambda)^k \\gamma V(S_{t+k+1})}_{(B)} - \n",
    "    \\underbrace{\\sum_{k=0}^{\\infty} (\\gamma \\lambda)^k V(S_{t+k})}_{(C)} \\\\\n",
    "    &= \\sum_{k=1}^{\\infty} (\\gamma \\lambda)^{k-1} R_{t+k} + (1 - \\lambda) \\sum_{k=1}^{\\infty} \\gamma^k \\lambda^{k-1} V(S_{t+k}) - V(S_{t+k}) \\\\ \\\\\n",
    "\n",
    "    &\\therefore V(S_{t+k}) + S = G_{t+k}(\\lambda) \\\\\n",
    "    G_{t+k}(\\lambda) &= V(S_{t+k}) + \\sum_{k=0}^{\\infty} (\\gamma \\lambda)^{k} \\delta_{t+k}\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f74730",
   "metadata": {},
   "source": [
    "- This tells us that, though the forward view asks us to accumulate $\\infty$ rewards before updating the current state $t$, it is actually equivalent to taking the current state value, and adding discounted 1-step TD errors for infinite steps!\n",
    "\n",
    "- The main difference here is that, of course, we no longer need to \"buffer\". The state values can be updated incrementally!\n",
    "\n",
    "- This is why, to update state value, we can simply add the summation of 1 step TD errors to the existing state value\n",
    "\n",
    "\\begin{aligned}\n",
    "    V(S_t) \\leftarrow V(S_t) + \\alpha [G_t(\\lambda) - V(S_t)] \\\\\n",
    "    V(S_t) \\leftarrow V(S_t) + \\alpha [\\sum_{k=0}^{\\infty} (\\gamma \\lambda)^{k} \\delta_{t+k}] \\\\\n",
    "\\end{aligned}\n",
    "\n",
    "- And we don't need to wait for $\\infty$ steps before updating, because at every step, I simply need to attribute the 1-step TD error proportionally to every state $S_t$! \n",
    "    - That is, suppose I have states 1,2,3 and I move from 1 --> 2 --> 3\n",
    "    - Then \n",
    "        - the TD error I get from step 2 will be added to V(S_2) with weight $(\\gamma \\lambda)^0$\n",
    "        - the TD error I get from step 2 will be added to V(S_1) with weight $(\\gamma \\lambda)^1$\n",
    "\n",
    "- The idea here is that at every step, I update all prior visited states. Just scaling the error by the number of timesteps ago I visited the state\n",
    "\n",
    "- To maintain the weights, we simply need to maintain the product of $(\\gamma \\lambda)^x$ for every state, and add 1 for the latest state visited. Then we just multiply the entire array by a factor of $(\\gamma \\lambda)$ in each step\n",
    "    - This is known as the **eligibility trace**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d55e0f",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "5b261a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "N_STATES = 10\n",
    "N_ACTIONS = 5\n",
    "GAMMA = 0.9\n",
    "ALPHA = 0.1\n",
    "EPISODES = 1000\n",
    "TD_LAMBDA = 0.5\n",
    "\n",
    "N_STEPS = int(1e5)\n",
    "\n",
    "TRANSITION_PROBABILITIES = np.random.rand(N_STATES, N_ACTIONS, N_STATES)\n",
    "TRANSITION_PROBABILITIES /= np.sum(TRANSITION_PROBABILITIES, axis=2, keepdims=True)\n",
    "REWARDS = np.random.rand(N_STATES, N_ACTIONS, N_STATES) * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "926ec9d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24.77539107, 24.69010724, 24.40725054, 25.26559653, 24.88448932,\n",
       "       24.78805943, 24.97204458, 25.46356333, 24.68696901, 24.95584939])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import deque, namedtuple\n",
    "\n",
    "episode_step = namedtuple('episode_step', ['curr_state', 'action', 'reward', 'next_state'])\n",
    "\n",
    "def td_lambda_continuing():\n",
    "    V = np.zeros(N_STATES)\n",
    "    curr_state = np.random.randint(N_STATES)\n",
    "    ELIGIBILITY_TRACE = np.zeros(N_STATES)\n",
    "    for _ in range(N_STEPS):\n",
    "        action = np.random.randint(N_ACTIONS)\n",
    "        next_state = np.random.choice(N_STATES, p=TRANSITION_PROBABILITIES[curr_state, action])\n",
    "        curr_reward = REWARDS[curr_state, action, next_state]\n",
    "\n",
    "        one_step_td_error = curr_reward + (GAMMA * V[next_state]) - V[curr_state]\n",
    "\n",
    "        ELIGIBILITY_TRACE[curr_state] += 1\n",
    "        V[curr_state] += ALPHA * one_step_td_error\n",
    "        \n",
    "        ELIGIBILITY_TRACE *= TD_LAMBDA * GAMMA\n",
    "        curr_state = next_state\n",
    "    \n",
    "    return V\n",
    "\n",
    "td_lambda_continuing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043b1efe",
   "metadata": {},
   "source": [
    "## Policy Gradient – Direct Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d41a93",
   "metadata": {},
   "source": [
    "### Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee982a5",
   "metadata": {},
   "source": [
    "- In **Policy Gradient** approach, we attempt to parameterize the policy $\\pi$ using some set of parameters $\\theta$. \n",
    "\n",
    "- Then, to improve our policy, we iteratively modify $\\theta$ by using the first derivative of the expected reward $J_{\\pi_{\\theta}}$ w.r.t a change in $\\theta$. That is:\n",
    "\\begin{aligned}\n",
    "    \\frac{\\partial J_{\\pi_{\\theta}}}{\\partial \\theta}\n",
    "\\end{aligned}\n",
    "\n",
    "- This is known as **gradient ascent**. It's an ascent, because we want to maximise the reward $J$, instead of the usual gradient descent in supervised learning when we are minimising some loss $L$\n",
    "\n",
    "- For each iteration, we update $\\theta$ by taking\n",
    "\n",
    "\\begin{aligned}\n",
    "    \\theta \\leftarrow \\theta + \\alpha \\cdot \\bigtriangledown_{\\theta} \\sum_{t=0}^{T-1} \\log \\pi_{\\theta}(a_t | s_t) \\cdot G_t\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6082351",
   "metadata": {},
   "source": [
    "### Theory: How does the gradient ascent update come about?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b38e86",
   "metadata": {},
   "source": [
    "- In the policy gradient approach, we want to maximise the expected return $J$ under some policy $\\pi_{\\theta}$, where the policy is entirely dependent on parameter $\\theta$\n",
    "\n",
    "\\begin{aligned}\n",
    "    J(\\theta) &= E_{\\tau \\sim \\pi_{\\theta}}[R(\\tau)] \\\\\n",
    "    &\\text{where} \\\\\n",
    "    &\\quad \\tau = [(s_0, a_0), (s_1, a_1), ...] \\text{ is the trajectory of states and actions } (s_t, a_t) \\\\\n",
    "    &\\quad R(\\tau) = \\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t) \\text{ is the discounted total reward} \\\\\n",
    "    &\\quad \\pi_{\\theta}(a|s) \\text{ is the theta-parameterised probability of taking action } a \\text{ at state } s\\\\\n",
    "\\end{aligned}\n",
    "\n",
    "- So if we want to take different actions, we need to modify $\\theta$. And to decide how to modify $\\theta$, we use the gradient $\\frac{\\partial J(\\theta)}{\\partial \\theta}$. This will be written as $\\bigtriangledown_{\\theta} J(\\theta)$\n",
    "\n",
    "- Intuitively, since we want to maximise $J$, if the gradient is positive, we want to move in the direction of the gradient!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc59da0",
   "metadata": {},
   "source": [
    "#### Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1dbce3",
   "metadata": {},
   "source": [
    "- Let's rewrite the return $J(\\theta)$ as the summation over all trajectories $\\tau$, instead of leaving it as the expectation\n",
    "\n",
    "\\begin{aligned}\n",
    "    J(\\theta) &= \\sum_{\\tau} P(\\tau; \\theta) R(\\tau)\n",
    "\\end{aligned}\n",
    "\n",
    "- We know that $P(\\tau; \\theta)$ can be rewritten as the product of the Markov Process (i.e. the actions and transition probabilities)\n",
    "\n",
    "\\begin{aligned}\n",
    "    P(\\tau; \\theta) &= \\rho(s_0) \\prod_{t=0}^{T-1} \\pi_{\\theta}(a_t | s_t) P(s_{t+1} | s_t, a_t) \\\\\n",
    "    &\\text{where} \\\\\n",
    "    &\\quad \\rho(s_0) = \\text{ Probability of starting at state } s_0 \\\\\n",
    "    &\\quad \\pi_{\\theta}(a_t | s_t) = \\text{ Probability of taking action } a \\text{ at state } s \n",
    "    \\text{ under policy } \\pi_{\\theta} \\\\\n",
    "    &\\quad P(s_{t+1} | s_t, a_t) = \\text{ Transition probability } \\\\\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95db43cf",
   "metadata": {},
   "source": [
    "- Therefore, $\\bigtriangledown_{\\theta} J(\\theta)$ can be written as\n",
    "\n",
    "\\begin{aligned}\n",
    "    \\bigtriangledown_{\\theta} J(\\theta) &= \\bigtriangledown_{\\theta} \\sum_{\\tau} P(\\tau; \\theta) R(\\tau) \\\\\n",
    "    &= \\sum_{\\tau} \\bigtriangledown_{\\theta} P(\\tau, \\theta) R(\\tau)\n",
    "\\end{aligned}\n",
    "\n",
    "- By the **log-derivative trick**, we know that:\n",
    "\n",
    "\\begin{aligned}\n",
    "    \\bigtriangledown_{\\theta} \\log f(\\theta) &= \\frac{\\bigtriangledown_{\\theta} f(\\theta)}{f(\\theta)} \\\\\n",
    "    \\therefore f(\\theta) &= f(\\theta) \\bigtriangledown_{\\theta} \\log f(\\theta)\n",
    "\\end{aligned}\n",
    "\n",
    "- Therefore, it must be true that\n",
    "\n",
    "\\begin{aligned}\n",
    "    \\bigtriangledown_{\\theta} J(\\theta) &= \\bigtriangledown_{\\theta} \\sum_{\\tau} P(\\tau; \\theta) R(\\tau) \\\\\n",
    "    &= \\sum_{\\tau} \\bigtriangledown_{\\theta} P(\\tau, \\theta) R(\\tau) \\\\\n",
    "    &= \\sum_{\\tau} P(\\tau, \\theta) \\bigtriangledown_{\\theta} \\log P(\\tau, \\theta) R(\\tau) \\\\\n",
    "    &= \\mathbb{E}_{\\tau \\sim P(\\tau, \\theta)}[R(\\tau) \\bigtriangledown_{\\theta} \\log P(\\tau, \\theta)]\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c8c2fd",
   "metadata": {},
   "source": [
    "- This suggests that the gradient of the reward $\\bigtriangledown_{\\theta}J(\\theta)$ is simply the expected product of reward from following trajectory $R(\\tau)$, multiplied by the derivative of log probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf13187",
   "metadata": {},
   "source": [
    "- Ok, but how do we compute $\\bigtriangledown_{\\theta} \\log P(\\tau, \\theta)$?\n",
    "\n",
    "\\begin{aligned}\n",
    "    P(\\tau; \\theta) &= \\rho(s_0) \\prod_{t=0}^{T-1} \\pi_{\\theta}(a_t | s_t) P(s_{t+1} | s_t, a_t) \\\\\n",
    "    \\log P(\\tau; \\theta) &= \\log [\\rho(s_0) \\prod_{t=0}^{T-1} \\pi_{\\theta}(a_t | s_t) P(s_{t+1} | s_t, a_t)] \\\\\n",
    "    \\log P(\\tau; \\theta) &= \\log \\rho(s_0) + \\sum_{t=0}^{T-1} \\log \\pi_{\\theta}(a_t | s_t) + \\sum_{t=0}^{T-1} \\log P(s_{t+1} | s_t, a_t) \\\\\n",
    "\\end{aligned}\n",
    "\n",
    "- But notice that of all the terms in the last expression, only $\\sum_{t=0}^{T-1} \\log \\pi_{\\theta}(a_t | s_t)$ contains $\\theta$! Therefore;\n",
    "\n",
    "\\begin{aligned}\n",
    "    \\bigtriangledown_{\\theta} \\log P(\\tau; \\theta) &= \\bigtriangledown_{\\theta} [\\log \\rho(s_0) + \\sum_{t=0}^{T-1} \\log \\pi_{\\theta}(a_t | s_t) + \\sum_{t=0}^{T-1} \\log P(s_{t+1} | s_t, a_t)] \\\\\n",
    "    &= \\bigtriangledown_{\\theta} \\sum_{t=0}^{T-1} \\log \\pi_{\\theta}(a_t | s_t) \\\\\n",
    "\\end{aligned}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c69519c",
   "metadata": {},
   "source": [
    "- Substituting back into our original gradient expression\n",
    "\n",
    "\\begin{aligned}\n",
    "    \\bigtriangledown_{\\theta} J(\\theta) &= \\mathbb{E}_{\\tau \\sim P(\\tau, \\theta)}[R(\\tau) \\bigtriangledown_{\\theta} \\log P(\\tau, \\theta)] \\\\\n",
    "    &= \\mathbb{E}_{\\tau \\sim P(\\tau, \\theta)}[R(\\tau) \\bigtriangledown_{\\theta} \\sum_{t=0}^{T-1} \\log \\pi_{\\theta}(a_t | s_t)]\n",
    "\\end{aligned}\n",
    "\n",
    "- We usually subsitute the reward $R(\\tau)$ with a manageable number of discounted reward steps, giving us th final expression:\n",
    "\n",
    "\\begin{aligned}\n",
    "    \\bigtriangledown_{\\theta} J(\\theta) &= \\mathbb{E}_{\\tau \\sim P(\\tau, \\theta)}[R(\\tau) \\bigtriangledown_{\\theta} \\sum_{t=0}^{T-1} \\log \\pi_{\\theta}(a_t | s_t)] \\\\\n",
    "    &= \\mathbb{E}_{\\tau \\sim P(\\tau, \\theta)}[\\sum_{i=0}^{k} \\gamma^i r_{t+i} \\bigtriangledown_{\\theta} \\sum_{t=0}^{T-1} \\log \\pi_{\\theta}(a_t | s_t)] \\\\\n",
    "    &= \\mathbb{E}_{\\tau \\sim P(\\tau, \\theta)} [G_t \\bigtriangledown_{\\theta} \\sum_{t=0}^{T-1} \\log \\pi_{\\theta}(a_t | s_t)]\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35add5d5",
   "metadata": {},
   "source": [
    "- This is why, for each iteration, we update $\\theta$ by taking\n",
    "\n",
    "\\begin{aligned}\n",
    "    \\theta \\leftarrow \\theta + \\alpha \\cdot \\bigtriangledown_{\\theta} \\sum_{t=0}^{T-1} \\log \\pi_{\\theta}(a_t | s_t) \\cdot G_t\n",
    "\\end{aligned}\n",
    "\n",
    "- Intuitively, the gradient moves us in the direction of improving $J(\\theta)$, with $\\alpha$ as the learning rate controlling step size\n",
    "\n",
    "- Note that here, assuming an episodic task, $G_t$ is simply the discounted reward of all steps in the episode remaining after time $t$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770fc482",
   "metadata": {},
   "source": [
    "### Theory: Choosing a differentiable policy $\\pi_{\\theta}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc14fb68",
   "metadata": {},
   "source": [
    "- We established the update step above as \n",
    "\n",
    "\\begin{aligned}\n",
    "    \\theta \\leftarrow \\theta + \\alpha \\cdot \\bigtriangledown_{\\theta} \\sum_{t=0}^{T-1} \\log \\pi_{\\theta}(a_t | s_t) \\cdot G_t\n",
    "\\end{aligned}\n",
    "\n",
    "- So the core idea here is: we need some $\\pi_{\\theta}$ to be **differentiable w.r.t its parameters**, because you need to compute $\\bigtriangledown_{\\theta} \\log \\pi_{\\theta}(a | s)$\n",
    "\n",
    "- That is, the probability of sampling action a $P(a)$ must be a function of $\\theta$, so we can compute $\\frac{\\partial P(a)}{\\partial \\theta}$\n",
    "\n",
    "- Here, we will go through a few examples of what forms $P(a)$ can take, so that we can differentiate the policies for both continuous and discrete action spaces "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a16fe1",
   "metadata": {},
   "source": [
    "#### Softmax Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbc5b95",
   "metadata": {},
   "source": [
    "#### Gaussian Policy with Direct Parameter Update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1465af3",
   "metadata": {},
   "source": [
    "#### Gaussian Policy with Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daf0341",
   "metadata": {},
   "source": [
    "#### Squashed Gaussian / Tanh-Gaussian Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5b5ebf",
   "metadata": {},
   "source": [
    "- For bounded continuous actions, you can sample a Gaussian and squash with a tanh:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7d3b6e",
   "metadata": {},
   "source": [
    "#### Distributions other than Gaussian/Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613151a8",
   "metadata": {},
   "source": [
    "- Continuous\n",
    "    - Beta: $\\alpha_{\\theta}(s), \\beta_{\\theta}(s)$\n",
    "        - Naturally bounded (0,1); can scale to arbitrary ranges\n",
    "\n",
    "    - Laplace / Logistic: Mean, scale\n",
    "        - heavy-tailed action distributions\n",
    "\n",
    "    - Gaussian Mixture Model: Multiple gaussians + their mixture weights\n",
    "        - Multimodal actions; useful when one Gaussian cannot model the distribution\n",
    "\n",
    "- Discrete\n",
    "    - Gumbel-Softmax: logits + Gumbel noise + temperature\n",
    "        - Used when you need the sampling process itself to be differentiable (i.e. not just argmax of the action-values)\n",
    "        - Rare in RL\n",
    "\n",
    "- Remember the core idea: any distribution that is **differentiable w.r.t its parameters** works, because you need to compute $\\bigtriangledown_{\\theta} \\log \\pi_{\\theta}(a | s)$\n",
    "    - That is, $\\log P(a)$ must be a function of $\\theta$, so we can compute $\\frac{\\partial \\log P(a)}{\\partial \\theta}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e58e51",
   "metadata": {},
   "source": [
    "## 5. Actor–Critic Overview\n",
    "\n",
    "- Combines **value-based** and **policy-based** methods:\n",
    "  - **Actor**: updates the policy (like policy gradient).  \n",
    "  - **Critic**: estimates value function (like TD) to reduce variance of updates.\n",
    "- Intuition: the critic **guides** the actor by telling it whether actions are good or bad.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Key Takeaways\n",
    "\n",
    "| Method | Model Needed | Online Learning | Bias vs Variance | Action Space |\n",
    "|--------|--------------|----------------|----------------|--------------|\n",
    "| Dynamic Programming | Yes | No | None | Discrete |\n",
    "| Monte Carlo | No | Episodic | Low variance, high bias? | Discrete/Continuous |\n",
    "| Temporal-Difference | No | Yes | Some bias, lower variance | Discrete/Continuous |\n",
    "| Policy Gradient | No | Yes | High variance | Continuous-friendly |\n",
    "| Actor–Critic | No | Yes | Balanced bias/variance | Continuous-friendly |\n",
    "\n",
    "- Choice depends on **model availability**, **task type**, and **action space**.\n",
    "- These methods form the backbone for **practical RL algorithms** like DQN, PPO, A3C, etc.\n",
    "\n",
    "---\n",
    "\n",
    "*Next: Practical Implementation — coding examples, environments, and debugging tips.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59c1e3e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
