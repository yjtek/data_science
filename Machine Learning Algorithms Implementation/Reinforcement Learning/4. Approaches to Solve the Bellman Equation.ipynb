{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c81132f9",
   "metadata": {},
   "source": [
    "# Approaches to Solve the Bellman Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f40f9c6",
   "metadata": {},
   "source": [
    "- We have established the basic setup for Reinforcement Learning by introducing the equilibrium equations of the State-Value and Action Value functions of the Bellman Equation\n",
    "\n",
    "- Now we look into **how** the Bellman Equations can be solved in practise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d06256",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "    \n",
    "    &\\text{State-Value Function:} \\\\\n",
    "    &\\quad v_{\\pi}(s) = \\sum_a \\pi(a|s) \\sum_{s'} P(s' | s, a) [R(s, a, s') + \\gamma v_{\\pi}(s')] \\\\ \\\\\n",
    "\n",
    "    &\\text{Action-Value Function:} \\\\\n",
    "    &\\quad q_{\\pi}(s,a) = \\sum_{s'} P(s' | s, a) [R(s, a, s') + \\gamma \\sum_{a'} \\pi(a'|s') q_{\\pi}(s', a')]\n",
    "\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180daa30",
   "metadata": {},
   "source": [
    "## Dynamic Programming (DP) - Model Based"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc4773e",
   "metadata": {},
   "source": [
    "- The simplest case of solving the Bellman Equation is when you have **both** the transition probabilities $P(s'|s,a)$ and the rewards $R(s,a,s')$\n",
    "\n",
    "- That is, you know the full Markov Decision Process (MDP). Hence the term \"model based\", because we are using the MDP model\n",
    "\n",
    "- This is, of course, exceedingly rare. But it is instructive to see how to solve for $\\pi^*$ when you have almost all available information\n",
    "\n",
    "- We will go through 2 ways of implementing the DP solver\n",
    "    - Policy Interation\n",
    "    - Value Iteration\n",
    "\n",
    "- To do this, let's assume the following set up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d544b54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "N_STATES = 20\n",
    "N_ACTIONS = 20\n",
    "GAMMA = 0.9\n",
    "\n",
    "# Shape corresponding to State, Action, Next State\n",
    "TRANSITION_PROBABILITIES = np.random.rand(N_STATES, N_ACTIONS, N_STATES)\n",
    "# Normalise so transition probabilities for a given state sums to 1 across all actions\n",
    "TRANSITION_PROBABILITIES /= np.sum(TRANSITION_PROBABILITIES, axis=2, keepdims=True)\n",
    "\n",
    "REWARDS = np.random.rand(N_STATES, N_ACTIONS, N_STATES) * 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54792600",
   "metadata": {},
   "source": [
    "### Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df82c340",
   "metadata": {},
   "source": [
    "- In policy iteration, we create a loop that does (i) policy evaluation, and (ii) policy improvement based on the evaluated outcomes of the policy, and the loop runs until some stopping criteria is reached\n",
    "\n",
    "- **Policy Evaluation:** \n",
    "    - Since we know the transition probabilities and rewards with full certainty, we can update the State-Value function by computing the Bellman Equation\n",
    "    \n",
    "    - Start with a seed array representing the State-Value function $V$, and a policy $\\pi$ that deterministically tells us what action to take in a given state $S$\n",
    "    \n",
    "    - Compute 1 pass of the Bellman State Value function update for all states. This is just a transition-probability-weighted average of the state values of the next states $S'$\n",
    "\n",
    "    - Check that the largest change in the state value estimates for all $N$ states exceeds some threshold $\\theta$. If even the largest change falls below the threshold, we assume convergence, and we return the state values $V$\n",
    "\n",
    "    - Otherwise, keep looping\n",
    "\n",
    "- **Policy Improvement**\n",
    "    - We've previously updated our estimate of the State-Value function $V$\n",
    "    - Now, we need to update our policy based on the new estimate of the State-Value function. That is, given a state $S$, I want to know the value of taking an action $A$. \n",
    "    - That is; the **Q-Value**\n",
    "\n",
    "    - Looping over every state $S$, we init an array to hold the new Q values\n",
    "\n",
    "    - Given $S$, loop over every action available, and update the Action-Value function $Q$. This is simply the transition-probability-weighted average of rewards from the transition $S -> S'$, plus the discounted state-value of $S'$\n",
    "\n",
    "    - Finally, the new policy for state $S$ is simply to take the argmax of all the Q values\n",
    "\n",
    "- **Policy Iteration**\n",
    "    - Bringing these 2 together, we simply create an infinite loop of `Evaluation --> Improvement --> Evalution ...`\n",
    "\n",
    "    - This will keep going until some convergence criteria is met. An easy on is to say that; if for some $N$ consecutive loops we do not change our policy, we have reached convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "47bbc794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "\n",
      "    Optimal Policy: [12 16 18 19 17  4 16  6 19 17 17  2 10 14 18 19 17  5  7  9]\n",
      "    Action-Value: [[30.94841478 31.32490423 32.19788999 31.9444741  31.79106206 31.84517046\n",
      "  31.290961   31.28625888 31.67130119 31.75888851 31.64210072 31.40075868\n",
      "  32.21000994 31.70431886 32.00087219 30.9218291  31.23823039 31.00114137\n",
      "  31.35150682 31.38025686]\n",
      " [31.09281456 31.19694478 31.74030897 31.97479618 31.94372851 31.00817106\n",
      "  31.06199313 31.43872014 31.59123965 31.17103483 31.83600553 31.7249459\n",
      "  31.14808322 31.37639636 31.84617734 30.88066009 32.05620933 31.25515259\n",
      "  31.48655157 31.54256258]\n",
      " [31.61237654 31.29259542 31.17586312 32.2678108  32.01706964 31.44388102\n",
      "  32.08529495 31.07045071 31.49056397 31.71061504 31.02807655 30.90594336\n",
      "  31.68019248 31.24952417 31.73166221 31.61723427 32.19383784 31.65359456\n",
      "  32.53945021 31.81073227]\n",
      " [32.05617899 31.82333462 31.68052992 31.05107583 32.03523231 31.60465759\n",
      "  31.81101442 32.20996754 31.45421427 31.10691636 31.48853139 31.84642156\n",
      "  31.11589792 31.31467339 32.0437155  32.0146979  31.45848263 31.74417274\n",
      "  32.09585302 32.3384664 ]\n",
      " [31.73863841 31.79437923 31.26959741 31.61433795 31.91880981 31.76264575\n",
      "  31.7015182  31.72896369 31.0475385  30.89541821 31.37098987 31.02817838\n",
      "  31.29544678 30.74502741 31.84256972 30.93572906 31.56248691 32.1741029\n",
      "  31.13068849 31.13810303]\n",
      " [31.30725565 31.74994061 32.07803162 31.76494607 32.48933637 31.92830815\n",
      "  31.48080026 32.25398118 32.35836861 31.60661809 31.67381875 31.84419746\n",
      "  31.59732025 31.24761026 30.89958002 31.63079221 31.81944675 31.8026537\n",
      "  31.29263384 31.47892525]\n",
      " [31.19914681 31.87098404 31.30279615 31.42725544 31.25796811 31.03716222\n",
      "  32.0786561  31.4378775  31.40512384 31.90228803 31.16887549 30.93126363\n",
      "  31.59232694 31.62862619 31.70811731 31.38529016 32.14463724 31.94279416\n",
      "  31.34277728 31.61328553]\n",
      " [30.99076531 31.98611845 31.50108865 31.24169256 31.40343523 31.80713212\n",
      "  32.24949883 31.80243454 31.19678341 31.43056289 31.15547386 31.97739334\n",
      "  31.69492248 31.14343988 31.44334112 31.69447457 31.20891624 31.70945872\n",
      "  31.7470955  31.39449481]\n",
      " [31.73974763 31.75936927 31.32384599 31.74107264 31.45514321 31.61443255\n",
      "  31.88283065 31.70003353 32.11492899 31.7595051  31.49787525 31.15752467\n",
      "  31.14647163 31.70585755 32.05244863 31.84606403 30.86549668 31.79064853\n",
      "  31.9322769  32.26843626]\n",
      " [31.86705003 32.18505948 31.61268483 31.05337451 31.78511483 31.45226405\n",
      "  31.17941384 32.12167851 31.30888033 32.01370938 31.69130246 30.66759393\n",
      "  31.19801981 31.86730941 31.71625711 31.38326528 31.11292379 32.42456183\n",
      "  31.25440488 31.42729077]\n",
      " [32.11268193 31.60577148 31.64621138 31.39743754 31.45765847 31.75252482\n",
      "  30.72417944 31.59956637 31.54681688 31.04292908 31.65556355 31.25213511\n",
      "  31.89816479 31.4966708  31.66103526 31.93792417 31.38619446 32.26555341\n",
      "  31.93315416 31.44142264]\n",
      " [31.21982368 31.45231734 32.31171931 31.84529471 32.20327994 31.12818211\n",
      "  31.68921563 31.22272172 30.66963594 31.4901787  31.48251072 30.97694317\n",
      "  31.97856286 31.74344459 31.55302903 31.99364432 31.30385192 32.16004078\n",
      "  31.8369227  31.94669736]\n",
      " [31.66687024 31.69543416 32.28528641 31.36477508 31.25502629 31.74038498\n",
      "  31.35236755 31.53067024 31.15892711 31.41422446 32.43796168 31.21149068\n",
      "  31.6923935  31.62583026 31.75940112 31.80169641 31.82230354 31.4328158\n",
      "  31.71080598 31.23174027]\n",
      " [31.28144885 32.06578808 31.84978565 31.98038203 31.36729104 30.92415163\n",
      "  31.85770731 31.47656368 31.74004652 31.22816112 31.46496453 32.0056815\n",
      "  31.63487328 31.8248629  32.09600004 31.59755102 31.41510532 31.96895501\n",
      "  31.82259359 31.8985461 ]\n",
      " [31.60219843 31.78844297 31.527741   31.6883589  32.13565232 31.64916927\n",
      "  31.56485247 32.07715346 32.21421801 31.33630067 31.4154296  31.72565975\n",
      "  32.12818703 31.80040456 31.54380552 32.15260988 31.9003869  32.21860983\n",
      "  32.64112773 31.84786638]\n",
      " [31.35009409 31.39085232 31.62147474 32.01977854 31.82582623 31.28768439\n",
      "  30.7865028  31.23015452 31.90238478 32.07313468 32.08282382 31.74585121\n",
      "  31.59994491 31.88294945 31.94730748 31.92049594 31.42331362 31.25665681\n",
      "  31.3207553  32.31968844]\n",
      " [31.82866916 31.28199027 31.97409584 31.56543434 31.63683595 31.64963511\n",
      "  32.43552999 32.00953337 31.21820727 31.63351445 31.21586037 32.14989531\n",
      "  31.91152083 31.84916614 31.54802035 31.79564734 31.46139793 32.60392454\n",
      "  31.78021976 31.77715831]\n",
      " [31.26553422 31.32845465 31.54795412 31.79594926 31.49521122 32.19392032\n",
      "  31.2404395  32.07791023 32.00867776 31.69155132 31.64494267 32.17402121\n",
      "  31.83059891 31.00661157 31.50820611 31.83676937 32.06024281 31.1534086\n",
      "  31.60925034 31.49076811]\n",
      " [32.17916304 32.00297501 31.26591192 32.00260364 31.40775653 31.05737754\n",
      "  31.64348826 32.53185635 31.62018266 31.91698287 31.20478233 31.23910548\n",
      "  31.95315765 31.01957898 32.08283366 31.31324996 31.62320826 32.00823627\n",
      "  32.34240564 31.56525483]\n",
      " [31.79562966 31.45378813 31.62153274 31.56904403 31.48662388 31.64228255\n",
      "  32.09806764 31.52527212 31.75200823 32.59174591 31.44387921 31.26473349\n",
      "  32.36785617 31.50118577 31.16649571 31.02920019 31.9273415  31.48583741\n",
      "  31.50964845 31.87387478]]\n",
      "    State-Value: [32.21000948 32.05620889 32.53944985 32.33846597 32.17410255 32.48933601\n",
      " 32.14463692 32.24949853 32.26843598 32.42456155 32.26555315 32.31171909\n",
      " 32.4379615  32.09599986 32.64112757 32.31968832 32.60392446 32.19392027\n",
      " 32.5318563  32.59174587]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def policy_evaluation(\n",
    "    transition_probabilities: np.ndarray,\n",
    "    rewards: np.ndarray,\n",
    "    curr_state_value_function: np.ndarray,\n",
    "    policy: np.ndarray,\n",
    "    gamma: float = 0.9,\n",
    "    theta: float = 1e-6\n",
    ") -> np.ndarray:\n",
    "    updated_state_value_function = curr_state_value_function.copy()\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for curr_state in range(N_STATES):\n",
    "            v_updated = sum([\n",
    "                transition_probabilities[curr_state, policy[curr_state], s_prime] * \n",
    "                (rewards[curr_state, policy[curr_state], s_prime] + gamma * updated_state_value_function[s_prime])\n",
    "                for s_prime in range(N_STATES)\n",
    "            ])\n",
    "            delta= max(delta, abs(v_updated - updated_state_value_function[curr_state]))\n",
    "            updated_state_value_function[curr_state] = v_updated\n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    return updated_state_value_function\n",
    "\n",
    "def policy_improvement(\n",
    "    transition_probabilities: np.ndarray,\n",
    "    rewards: np.ndarray,\n",
    "    curr_state_value_function: np.ndarray,\n",
    "    curr_action_value_function: np.ndarray,\n",
    "    curr_policy: np.ndarray,\n",
    "    gamma: float = 0.9\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    updated_policy = curr_policy.copy()\n",
    "    updated_action_value_function = curr_action_value_function.copy()\n",
    "    for s in range(N_STATES):\n",
    "        action_value_function_for_state_s = updated_action_value_function[s].copy()\n",
    "        for a in range(N_ACTIONS):\n",
    "            new_action_value = sum([\n",
    "                transition_probabilities[s, a, s_prime] * \n",
    "                (rewards[s, a, s_prime] + gamma * curr_state_value_function[s_prime])\n",
    "                for s_prime in range(N_STATES)\n",
    "            ])\n",
    "            action_value_function_for_state_s[a] = new_action_value\n",
    "        \n",
    "        updated_policy[s] = np.argmax(action_value_function_for_state_s)\n",
    "        updated_action_value_function[s] = action_value_function_for_state_s\n",
    "    return updated_policy, updated_action_value_function\n",
    "\n",
    "def policy_iteration(\n",
    "    transition_probabilities: np.ndarray,\n",
    "    rewards: np.ndarray,\n",
    "    gamma: float = 0.9,\n",
    "    theta: float = 1e-6\n",
    "):\n",
    "    curr_state_value_function: np.ndarray = np.zeros(N_STATES)\n",
    "    curr_action_value_function: np.ndarray = np.zeros((N_STATES, N_ACTIONS))\n",
    "    curr_policy: np.ndarray = np.zeros(N_STATES).astype(int)\n",
    "    \n",
    "    count_no_change = 0\n",
    "    count_iters = 0\n",
    "    while True:\n",
    "        print(count_iters)\n",
    "        updated_state_value_function = policy_evaluation(\n",
    "            transition_probabilities,\n",
    "            rewards,\n",
    "            curr_state_value_function,\n",
    "            curr_policy,\n",
    "            gamma,\n",
    "            theta\n",
    "        )\n",
    "        updated_policy, updated_action_value_function = policy_improvement(\n",
    "            transition_probabilities,\n",
    "            rewards,\n",
    "            updated_state_value_function,\n",
    "            curr_action_value_function,\n",
    "            curr_policy,\n",
    "            gamma\n",
    "        )\n",
    "        if np.array_equal(curr_policy, updated_policy):\n",
    "            count_no_change += 1\n",
    "            if count_no_change >= 5:\n",
    "                break\n",
    "        else:\n",
    "            count_no_change = 0\n",
    "\n",
    "        curr_policy = updated_policy.copy()\n",
    "        curr_action_value_function = updated_action_value_function.copy()\n",
    "        curr_state_value_function = updated_state_value_function.copy()\n",
    "\n",
    "        count_iters += 1\n",
    "    \n",
    "    return curr_policy, curr_action_value_function, curr_state_value_function\n",
    "\n",
    "pi_star, q_star, v_star = policy_iteration(TRANSITION_PROBABILITIES, REWARDS)\n",
    "\n",
    "print(f\"\"\"\n",
    "    Optimal Policy: {pi_star}\n",
    "    Action-Value: {q_star}\n",
    "    State-Value: {v_star}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e01a0a",
   "metadata": {},
   "source": [
    "- To ascertain if your policy iteration has converged correctly, check that the action values computed from the current transition probability, rewards, and optimal state values matches the values in your optimal q value array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a0d67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in range(N_STATES):\n",
    "    for a in range(N_ACTIONS):\n",
    "        q = sum([\n",
    "            TRANSITION_PROBABILITIES[s, a, s_prime] *\n",
    "            (REWARDS[s, a, s_prime] + GAMMA * v_star[s_prime])\n",
    "            for s_prime in range(N_STATES)\n",
    "        ])\n",
    "        assert np.isclose(q, q_star[s, a])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e4080c",
   "metadata": {},
   "source": [
    "### Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25862008",
   "metadata": {},
   "source": [
    "- In policy iteration, notice we have 2 distinct steps\n",
    "    - There is policy evaluation, which exhaustively uses given policy $\\pi$ to update the state value function $v$ until convergence\n",
    "    - Following which there is policy improvement, which uses the updated state value function $v$ to pick the best action $a$ for every state $s$\n",
    "\n",
    "- The key difference between policy iteration and value iteraton is in the order we perform these steps. The methods are exactly the same, For value iteration:\n",
    "    - We don't maintain an explicit policy. Instead, we focus on updating the state-value function $v$\n",
    "    - The action taken at each time step is simply the maximum of the action values computed using the current $v$\n",
    "\n",
    "- Note that there is no performance difference expected on average, and both approaches should converge to the same solution\n",
    "\n",
    "- TLDR; \n",
    "    - Policy Iteration does Loop(evaluate policy --> improve policy)\n",
    "    - Value Iteration does Loop(update state value function) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0069f088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(\n",
    "    transition_probabilities: np.ndarray,\n",
    "    rewards: np.ndarray,\n",
    "    gamma: float = 0.9,\n",
    "    theta: float = 1e-6\n",
    ") -> np.ndarray:\n",
    "    curr_state_value_function = np.zeros(N_STATES)\n",
    "    updated_state_value_function = np.zeros(N_STATES)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(N_STATES):\n",
    "            new_action_values = [\n",
    "                sum([\n",
    "                    transition_probabilities[s, a, s_prime] * \n",
    "                    (rewards[s, a, s_prime] + gamma * curr_state_value_function[s_prime])\n",
    "                    for s_prime in range(N_STATES)\n",
    "                ])\n",
    "                for a in range(N_ACTIONS)\n",
    "            ]\n",
    "            updated_state_value_function[s] = max(new_action_values)\n",
    "            delta = max(delta, abs(curr_state_value_function[s] - updated_state_value_function[s]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "            \n",
    "        curr_state_value_function = updated_state_value_function.copy()\n",
    "    \n",
    "    policy = np.zeros(N_STATES).astype(int)\n",
    "    for s in range(N_STATES):\n",
    "        action_values = [\n",
    "            sum(transition_probabilities[s, a, s_prime] * \n",
    "            (rewards[s, a, s_prime] + gamma * updated_state_value_function[s_prime])\n",
    "            for s_prime in range(N_STATES))\n",
    "            for a in range(N_ACTIONS)\n",
    "        ]\n",
    "        policy[s] = np.argmax(action_values)\n",
    "    \n",
    "    return policy, updated_state_value_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88c1f93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([12, 16, 18, 19, 17,  4, 16,  6, 19, 17, 17,  2, 10, 14, 18, 19, 17,\n",
       "         5,  7,  9]),\n",
       " array([32.21000379, 32.0562032 , 32.53944408, 32.33846026, 32.17409677,\n",
       "        32.48933021, 32.14463111, 32.2494927 , 32.26843013, 32.42455566,\n",
       "        32.26554727, 32.31171317, 32.43795555, 32.0959939 , 32.64112159,\n",
       "        32.31968229, 32.60391839, 32.1939142 , 32.53185021, 32.59173977]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_iteration(TRANSITION_PROBABILITIES, REWARDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a05a8b",
   "metadata": {},
   "source": [
    "## Monte Carlo - Sampling-Based"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77280b3d",
   "metadata": {},
   "source": [
    "- We've covered the DP approach, which looks at the fairly restrictive case of solving the Bellman Equations when the full MDP is known; i.e. you know both the transition probability, and the reward. This is typically unrealistic. Very few process in reality will be kind enough to provide you both pieces of information perfectly. And even if you estimate both using data, it will be estimated under some uncertainty\n",
    "\n",
    "- Therefore, we need more general methods to solve for the optimal policy when the transition probabilities and rewards are not known\n",
    "\n",
    "- Here, we consider the Monte-Carlo approach. \n",
    "\n",
    "- Note that the traditional Monte-Carlo approach assumes an **episodic task**. That is, the iteration will run for some time, and reach a terminal point.\n",
    "    - You can manually impose a terminal point for Monte-Carlo, but note that this isn't a true Monte Carlo approach, but a **truncated** Monte Ccarlo\n",
    "\n",
    "- Idea:\n",
    "    - The idea of the monte carlo is to simply let entire episodes play out multiple times, to find the long term average state-value function and hence the optimal policy\n",
    "    - So you have a `step()` method that determines 2 things; (i) what is the next state you are in, and (ii) is this next state a terminal one?\n",
    "    - Having done this, you now have 2 series recorded (i) a series of states, ending with a terminal state, and (ii) a series of rewards, indicating the reward you get for landing on that state\n",
    "        - Note that the series of rewards is NOT the sum of your future rewards (i.e. not the true Q-value). It is the specific reward of reaching some state $S$\n",
    "    - Now, for every point we land on in our array, we want to compute the discounted future rewards observed from that point, so we can derive a proper state value\n",
    "    - Init an array to hold the state-value function $V$\n",
    "    - We'll compute this in reverse order of the rewards array;\n",
    "        - Init $G$ as the running total of the discounted rewards. This starts at 0\n",
    "        - We know that the last state is terminal. Suppose this is the rightmost terminal state. \n",
    "            - We know that reaching this state gives a reward of 1. \n",
    "            - We also know that, this being the terminal state, there is no future reward. \n",
    "            - So the cumulative future reward $G = 1$\n",
    "            - Since we know the rightmost state $N$ has value $G=1$, update its state-value function $V[N]$ by taking $V[N] + \\alpha[G - V[N]]$\n",
    "        - Moving to the second last state. \n",
    "            - We know that this state has reward 0. Therefore, the value of this state comes only from the discounted value of transitioning to the last state and receiving the terminal reward 1. \n",
    "            - Thus, update $G = reward[N-1] + \\gamma \\cdot G$\n",
    "            - Then, update the state value function $V[N-1] + \\alpha[G - V[N-1]]$\n",
    "        - Iterate until you reach the end of the episode \n",
    "    - Continue to the next episode for a fixed number of episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616d4bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "# Environment setup; we create a number line with 10 states. The right-most state has a reward of 1\n",
    "# Therefore, train the RL loop to take rightward actions [+1] more than leftward actions [-1]\n",
    "# Terminate if you reach the left or rightmost states\n",
    "N_STATES = 5\n",
    "N_ACTIONS = 5\n",
    "GAMMA = 0.9 ## Discounting factor for future reward\n",
    "ALPHA = 0.1 ## Learning rate\n",
    "EPSILON = 0.5 ## Epsilon Greedy parameter\n",
    "\n",
    "# Shape corresponding to State, Action, Next State\n",
    "TRANSITION_PROBABILITIES = np.random.rand(N_STATES, N_ACTIONS, N_STATES)\n",
    "# Normalise so transition probabilities for a given state sums to 1 across all actions\n",
    "TRANSITION_PROBABILITIES /= np.sum(TRANSITION_PROBABILITIES, axis=2, keepdims=True)\n",
    "\n",
    "REWARDS = np.random.rand(N_STATES, N_ACTIONS, N_STATES) * 5\n",
    "REWARDS[:, :, -1] = 5\n",
    "\n",
    "EPISODES = 1000\n",
    "\n",
    "TERMINAL_STATES = [0, N_STATES - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "c16a95dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "episode_step = namedtuple('episode_step', ['curr_state', 'action', 'new_state', 'reward'])\n",
    "\n",
    "def take_action(curr_state: int, action_values: np.ndarray) -> tuple[int]:\n",
    "    action_values_for_curr_state = action_values[curr_state, :]\n",
    "    \n",
    "    if np.random.rand() >= EPSILON:\n",
    "        highest_action_value_action = np.argmax(action_values_for_curr_state)\n",
    "        return highest_action_value_action\n",
    "    else:\n",
    "        random_action = np.random.choice(N_ACTIONS)\n",
    "        return random_action\n",
    "\n",
    "def monte_carlo_epsilon_greedy():\n",
    "    action_values = np.zeros((N_STATES, N_ACTIONS))\n",
    "    state_values = np.zeros(N_STATES)\n",
    "\n",
    "    rewards_intermediate_store = {\n",
    "        (s,a): [] for s in range(N_STATES) for a in range(N_ACTIONS)\n",
    "    }\n",
    "    for _ in range(EPISODES):\n",
    "        curr_state = np.random.choice(list(range(1,N_STATES-1)))\n",
    "        episode_history = []\n",
    "\n",
    "        terminal_state_reached = curr_state in [0, N_STATES-1]\n",
    "        while not terminal_state_reached:\n",
    "            action = take_action(curr_state, action_values)\n",
    "            new_state = np.random.choice(\n",
    "                N_STATES, p=TRANSITION_PROBABILITIES[curr_state, action]\n",
    "            )\n",
    "            terminal_state_reached = new_state in [0, N_STATES-1]\n",
    "            reward = REWARDS[curr_state, action, new_state]\n",
    "            episode_history.append(\n",
    "                episode_step(curr_state=curr_state, action=action, new_state=new_state, reward=reward)\n",
    "            )\n",
    "            curr_state = new_state\n",
    "        \n",
    "        return_val = 0\n",
    "        for step in reversed(episode_history):\n",
    "            return_val = step.reward + GAMMA * return_val\n",
    "            rewards_intermediate_store[(step.curr_state, step.action)].append(return_val)\n",
    "        \n",
    "        for curr_state, action in rewards_intermediate_store.keys():\n",
    "            action_values[curr_state, action] = np.mean(rewards_intermediate_store.get((curr_state, action))) if rewards_intermediate_store.get((curr_state, action)) != [] else 0\n",
    "        \n",
    "        state_values = np.max(action_values, axis=1)\n",
    "    \n",
    "    return state_values, action_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "be80aaa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.        , 11.06340833,  9.65237593, 11.93171671,  0.        ]),\n",
       " array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 7.31191006, 11.06340833,  9.40697801,  7.43068361,  5.76089819],\n",
       "        [ 8.37778159,  9.59896629,  9.65237593,  8.6572607 ,  7.6457071 ],\n",
       "        [ 9.9728158 ,  8.30667043,  7.33269837,  9.40128791, 11.93171671],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ]]))"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monte_carlo_epsilon_greedy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5734220e",
   "metadata": {},
   "source": [
    "## Temporal-Difference (TD) - Bootstrapped Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c741ed01",
   "metadata": {},
   "source": [
    "- The motivation for this is similar to the Monte Carlo approach; this is used when we can't solve the Bellman Equation because transition probabilities and/or rewards are not known\n",
    "\n",
    "- Unlike Monte Carlo, which relies on the assumption that the task is episodic (i.e. terminates at some point), TD lets you update state value functions and policy incrementally. Therefore, TD is applied to tasks with no natural termination state\n",
    "\n",
    "- We'll cover 2 forms of TD;\n",
    "    - **TD(N)**: N-Step TD Learning\n",
    "    - **TD($\\lambda$)**: $\\lambda$ Return TD Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcd4d91",
   "metadata": {},
   "source": [
    "### N-Step TD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002f9dae",
   "metadata": {},
   "source": [
    "- In the N-Step TD, we want to update our state value function $V(S_t)$ based on some observed reward plus the estimated value of the next state.\n",
    "    - The $N$ simply tells you how many observed rewards we use in updating our current state value $V(S_t)$\n",
    "\n",
    "- For example, in `TD(0)`, the updated state-value is the current state-value plus some learning rate multipled by the observed deviation from your current state-value. The observed deviation is the return you experience $R_t + \\gamma V(S_{t+1})$ minus the current estimated expected return $V(S_t)$\n",
    "\\begin{aligned}\n",
    "    V(S_t) &= V(S_t) + \\alpha (\\hat{G_t^{(1)}} - V(S_t)) \\\\\n",
    "    &= V(S_t) + \\alpha (R_t + \\gamma V(S_{t+1}) - V(S_t))\n",
    "\\end{aligned}\n",
    "    - Here, we update $V(S_t)$ at every step. So $G_t^{(1)}$ is not known, but your best guess at the time step\n",
    "    - This is why some tutorials mention that `TD(N)` approach requires bootstrapping; it comes from the fact that that $\\hat{G_t^{(1)}} = R_t + \\gamma V(S_{t+1})$ is an estimate based on $V(S_{t+1})$\n",
    "    - This differs from something like Monte Carlo, for example, where episodes are played out till the end, and we do not need to bootstrap because we use the actual rewards $R_t, R_{t+1}, ...$ to compute our state-value function instead of some intermediate state-value estimate $V(S_{t+1})$\n",
    "\n",
    "- Extending this idea, for `TD(N)`, we accumulate $N$ steps of the future rewards $R_t, R_{t+1}, ... R_{t+n}$ plus the estimated state value $V(S_{t+n})$ to compute the **TD error** \n",
    "\\begin{aligned}\n",
    "    V(S_t) &= V(S_t) + \\alpha (\\hat{G_t^{(N)}} - V(S_t)) \\\\\n",
    "    &= V(S_t) + \\alpha (\\sum_{i=0}^{n-1} \\gamma^{i} R_{t+i} +  \\gamma^n V(S_{t+n}) - V(S_t))\n",
    "\\end{aligned}\n",
    "    - Fun fact; if N is the episode length, then this is simply Monte Carlo!\n",
    "\n",
    "- The intuition here is: if $V(S_t)$ has converged, then there should be no TD error observed, the current step return should equal the current state values. So $V(S_t)$ should be unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc915a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "# Environment setup; we create a number line with 10 states. The right-most state has a reward of 1\n",
    "# Therefore, train the RL loop to take rightward actions [+1] more than leftward actions [-1]\n",
    "# Terminate if you reach the left or rightmost states\n",
    "N_STATES = 5\n",
    "N_ACTIONS = 5\n",
    "GAMMA = 0.9 ## Discounting factor for future reward\n",
    "ALPHA = 0.1 ## Learning rate\n",
    "EPSILON = 0.5 ## Epsilon Greedy parameter\n",
    "\n",
    "# Shape corresponding to State, Action, Next State\n",
    "TRANSITION_PROBABILITIES = np.random.rand(N_STATES, N_ACTIONS, N_STATES)\n",
    "# Normalise so transition probabilities for a given state sums to 1 across all actions\n",
    "TRANSITION_PROBABILITIES /= np.sum(TRANSITION_PROBABILITIES, axis=2, keepdims=True)\n",
    "\n",
    "REWARDS = np.random.rand(N_STATES, N_ACTIONS, N_STATES) * 5\n",
    "\n",
    "EPISODES = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8926169b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "\n",
    "episode_step = namedtuple('episode_step', ['state', 'reward', 'next_state'])\n",
    "\n",
    "def n_step_td(TD_N: int):\n",
    "    state_values = np.zeros(N_STATES)\n",
    "\n",
    "    for _ in range(EPISODES):\n",
    "        # start new episode\n",
    "        state = np.random.choice(N_STATES)\n",
    "        episode = []\n",
    "\n",
    "        # generate a trajectory of at least TD_N steps\n",
    "        for _ in range(TD_N):\n",
    "            # pick next_state from transition probabilities averaged over actions\n",
    "            # (for state-value prediction, we can assume random policy)\n",
    "            action = np.random.randint(N_ACTIONS)\n",
    "            next_state = np.random.choice(\n",
    "                N_STATES, p=TRANSITION_PROBABILITIES[state, action]\n",
    "            )\n",
    "            reward = REWARDS[state, action, next_state]\n",
    "            episode.append(episode_step(state, reward, next_state))\n",
    "            state = next_state\n",
    "\n",
    "        # --- compute N-step TD update ---\n",
    "        # bootstrap from final state's current value estimate\n",
    "        G = state_values[state]  # V(S_{t+n})\n",
    "        for step in reversed(episode):\n",
    "            G = step.reward + GAMMA * G\n",
    "            s = step.state\n",
    "            state_values[s] += ALPHA * (G - state_values[s])\n",
    "\n",
    "    return state_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1fd8605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24.66800009, 23.44499357, 24.20170967, 24.79193746, 24.14574764])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_step_td(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978fa5b8",
   "metadata": {},
   "source": [
    "### $\\lambda$ Return TD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680cfcf0",
   "metadata": {},
   "source": [
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c2e063",
   "metadata": {},
   "source": [
    "# Solution Methods\n",
    "---\n",
    "\n",
    "## 3. Temporal-Difference (TD) – Bootstrapped Learning\n",
    "\n",
    "- Combines DP and MC:\n",
    "  - Like DP: updates use existing estimates (bootstrapping).  \n",
    "  - Like MC: learns from sampled experience, not full model.\n",
    "- **TD(0) update rule (state-value)**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eac3224",
   "metadata": {},
   "source": [
    "V(s_t) ← V(s_t) + α [ r_{t+1} + γ V(s_{t+1}) − V(s_t) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6082351",
   "metadata": {},
   "source": [
    "\n",
    "- **Pros**: can learn online, before episode ends; lower variance than MC.  \n",
    "- **Cons**: introduces bias due to bootstrapping; sensitive to step-size α.\n",
    "\n",
    "- **TD control methods**: SARSA (on-policy), Q-learning (off-policy).\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Policy Gradient – Direct Optimization\n",
    "\n",
    "- Instead of computing value functions, **directly parameterize the policy** π_θ(a|s) and optimize expected return:\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64292ca1",
   "metadata": {},
   "source": [
    "J(θ) = E_πθ [ G_t ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870cca08",
   "metadata": {},
   "source": [
    "\n",
    "- **Update rule (gradient ascent)**:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b72dae",
   "metadata": {},
   "source": [
    "θ ← θ + α ∇_θ J(θ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e58e51",
   "metadata": {},
   "source": [
    "\n",
    "- **Pros**: handles continuous action spaces naturally; can represent stochastic policies.  \n",
    "- **Cons**: high variance in gradients; slower convergence.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Actor–Critic Overview\n",
    "\n",
    "- Combines **value-based** and **policy-based** methods:\n",
    "  - **Actor**: updates the policy (like policy gradient).  \n",
    "  - **Critic**: estimates value function (like TD) to reduce variance of updates.\n",
    "- Intuition: the critic **guides** the actor by telling it whether actions are good or bad.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Key Takeaways\n",
    "\n",
    "| Method | Model Needed | Online Learning | Bias vs Variance | Action Space |\n",
    "|--------|--------------|----------------|----------------|--------------|\n",
    "| Dynamic Programming | Yes | No | None | Discrete |\n",
    "| Monte Carlo | No | Episodic | Low variance, high bias? | Discrete/Continuous |\n",
    "| Temporal-Difference | No | Yes | Some bias, lower variance | Discrete/Continuous |\n",
    "| Policy Gradient | No | Yes | High variance | Continuous-friendly |\n",
    "| Actor–Critic | No | Yes | Balanced bias/variance | Continuous-friendly |\n",
    "\n",
    "- Choice depends on **model availability**, **task type**, and **action space**.\n",
    "- These methods form the backbone for **practical RL algorithms** like DQN, PPO, A3C, etc.\n",
    "\n",
    "---\n",
    "\n",
    "*Next: Practical Implementation — coding examples, environments, and debugging tips.*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
