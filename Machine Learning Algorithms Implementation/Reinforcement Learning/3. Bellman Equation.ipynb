{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08034dd7",
   "metadata": {},
   "source": [
    "# Value Functions and Bellman Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2de3b9",
   "metadata": {},
   "source": [
    "- In the previous set of notes, we have proven that the discounted future rewards under some specific policy will always converge, so long as the discounting factor $0 \\le \\gamma \\lt 1$\n",
    "\n",
    "\\begin{aligned}\n",
    "    G_t &= r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + ... & \\gamma \\lt 1\n",
    "\\end{aligned}\n",
    "\n",
    "- Because convergence is guaranteed, we can now define the long term values of both actions and states!\n",
    "\n",
    "- **State-Value Function**: This is the expected return of starting from state $s$ under policy $\\pi$\n",
    "    \n",
    "    \\begin{aligned}\n",
    "        v_{\\pi}(s) &= E_{\\pi}[G_t | s_t = s] \\\\\n",
    "        &\\text{where} \\\\\n",
    "        &\\quad G_t = \\text{Total discounted reward from time } t \\\\\n",
    "        &\\quad E_{\\pi} = \\text{Expectation of trajectories generated by policy } \\pi\n",
    "    \\end{aligned}\n",
    "\n",
    "- **Action-Value Function**: This is the expected return of taking action $a$ from state $s$ then following policy $\\pi$\n",
    "    \n",
    "    \\begin{aligned}\n",
    "        q_{\\pi}(s, a) &= E_{\\pi}[G_t | s_t = s, a_t = a] \\\\\n",
    "        &\\text{where} \\\\\n",
    "        &\\quad G_t = \\text{Total discounted reward from time } t \\\\\n",
    "        &\\quad E_{\\pi} = \\text{Expectation of trajectories generated by policy } \\pi\n",
    "    \\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6678076c",
   "metadata": {},
   "source": [
    "## Bellman Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb92976",
   "metadata": {},
   "source": [
    "- The Bellman Equations are simply a recursive representation of this idea!\n",
    "\n",
    "\\begin{aligned}\n",
    "    \n",
    "    &\\text{State-Value Function:} \\\\\n",
    "    &\\quad v_{\\pi}(s) = \\sum_a \\pi(a|s) \\sum_{s'} P(s' | s, a) [R(s, a, s') + \\gamma v_{\\pi}(s')] \\\\ \\\\\n",
    "\n",
    "    &\\text{Action-Value Function:} \\\\\n",
    "    &\\quad q_{\\pi}(s,a) = \\sum_{s'} P(s' | s, a) [R(s, a, s') + \\gamma \\sum_{a'} \\pi(a'|s') q_{\\pi}(s', a')]\n",
    "\n",
    "\\end{aligned}\n",
    "\n",
    "- Intuition\n",
    "    - **State-Value Function**: The value of state $s$ is the sum of of the next step's reward plus the discounted state value of the next step, weighted over all possible next steps, weighted over the probability of taking action $a$ given the current state $s$\n",
    "    - **Action-Value Function**: The value of action $a$ given state $s$ is the sum of the reward of transitioning from $s$ to $s'$ given $a$, plus the discounted action values of the next state $s'$ weighted by the probability of the next action set $a'$, weighted by the probability of transitioning to $s'$ when the current state is $s$ and the action taken is $a$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a306f120",
   "metadata": {},
   "source": [
    "- What happens under optimal policy $\\pi^*$?\n",
    "\n",
    "- State-Value Function\n",
    "    - For the state value function, if we know $\\pi^*$, there is no longer a need to average over all possible actions, because we can always take the best action $a^*$. Therefore, the outer summation disappears, leaving only the inner summation, because an action $a$ transitions us into a new state $s'$ probabilistically\n",
    "\n",
    "    \\begin{aligned}\n",
    "        &\\quad v^*(s) = \\max_a \\sum_{s'} P(s' | s, a) [R(s, a, s') + \\gamma \\cdot v^{*}(s')] \\\\ \\\\\n",
    "    \\end{aligned} \n",
    "\n",
    "- Action-Value Function\n",
    "    - For the action value function, if we know $\\pi^*$, then we are no longer uncertain about what actions we will take in subsequent states $s'$. Therefore, the recursive sum over next actions $a'$ disappears; we will simply take actions that maximise our reward\n",
    "\n",
    "    \\begin{aligned}    \n",
    "        &\\quad q^{*}(s,a) = \\sum_{s'} P(s' | s, a) [R(s, a, s') + \\gamma \\max_{a'} q^{*}(s', a')]\n",
    "    \\end{aligned} \n",
    "\n",
    "- Optimal Policy\n",
    "    - The optimal policy, therfore, is simply to greedily pick the action $a$ with the highest action-value\n",
    "\n",
    "    \\begin{aligned}    \n",
    "        &\\quad \\pi^*(s) = \\argmax_a q^*(s,a)\n",
    "    \\end{aligned} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab9722f",
   "metadata": {},
   "source": [
    "- Taken together:\n",
    "\n",
    "\\begin{aligned}\n",
    "q^*(s,a) &= \\mathbb{E}[r + \\gamma \\max_{a'} q^*(s',a')] \\\\\n",
    "v^*(s) &= \\max_a q^*(s,a) \\\\\n",
    "\\pi^*(s) &= \\arg\\max_a q^*(s,a)\n",
    "\\end{aligned}\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
