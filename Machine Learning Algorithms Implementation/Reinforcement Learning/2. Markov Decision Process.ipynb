{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a464aa85",
   "metadata": {},
   "source": [
    "# Markov Decision Processes (MDPs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa46a954",
   "metadata": {},
   "source": [
    "- This notebook will focus on the mathematical framework for modelling RL problems, using the Markov Decision Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766518fd",
   "metadata": {},
   "source": [
    "## MDP Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cd4459",
   "metadata": {},
   "source": [
    "- Formally, an MDP has 5 components\n",
    "    - **States**: What are the possible situations the agent can be in. States will contain all contextual information for the agent\n",
    "    - **Actions**: All possible moves an agent can make\n",
    "    - **Transitions**: How the environment responds to actions\n",
    "    - **Rewards**: Feedback signal from the environment  \n",
    "    - **Policy**: The strategy used by the agent to choose actions\n",
    "\n",
    "    \\begin{aligned}\n",
    "        &(S, A, P, R, \\gamma) \\\\\n",
    "        & \\text{where} \\\\\n",
    "        &\\quad S = \\text{set of states} \\\\\n",
    "        &\\quad A = \\text{set of actions} \\\\\n",
    "        &\\quad P(S' | S, A) = \\text{probability that taking action A at state S bring you to S'} \\\\\n",
    "        &\\quad R(S, A, S') = \\text{Reward received from transitioning from S to S' by taking action A} \\\\\n",
    "        &\\quad \\gamma = \\text{discount factor to weight future rewards} \\\\\n",
    "    \\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af704f7",
   "metadata": {},
   "source": [
    "- All that's missing from this setup is how an agent will decide on the action to take given $S$, or the **policy**\n",
    "\n",
    "\\begin{aligned}\n",
    "    \\pi(a|s) &= \\text{Probability of action } A \\text{ in state } S\n",
    "\\end{aligned} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ac581e",
   "metadata": {},
   "source": [
    "- Thus, the goal of RL is to find the optimal policy $\\pi(a|s)$ so that we maximise some cumulative future reward $G_t$ such that:\n",
    "\n",
    "\\begin{aligned}\n",
    "    G_t &= r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + ... & \\gamma \\lt 1\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7bb41d",
   "metadata": {},
   "source": [
    "### Prove that if $\\gamma < 1$, then $G_t$ converges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b02389",
   "metadata": {},
   "source": [
    "- Let's assume the rewards are bounded by some reward $R_{\\max}$. This is logical; there should not be a process that gives infinite reward\n",
    "\n",
    "\\begin{aligned}\n",
    "    |r_t| \\le R_{\\max} \\quad \\forall t\n",
    "\\end{aligned}\n",
    "\n",
    "- Then, it must be true that any discounted reward $r$ must also follow this bound\n",
    "\n",
    "\\begin{aligned}\n",
    "    |\\gamma^k r_{t+k}| \\le \\gamma^k R_{\\max} \\quad \\forall t, k\n",
    "\\end{aligned}\n",
    "\n",
    "- Let's consider the discounted sum up to an arbitrary timestep $N$\n",
    "\n",
    "\\begin{aligned}\n",
    "    S_n &= \\sum_{k=0}^{n} \\gamma^k r_{t+k+1}\n",
    "\\end{aligned}\n",
    "\n",
    "- This must be bounded by\n",
    "\n",
    "\\begin{aligned}\n",
    "    |S_n| &\\le \\sum_{k=0}^{n} |\\gamma^k r_{t+k+1}| \\\\\n",
    "    &\\le \\sum_{k=0}^{n} \\gamma^k R_{\\max} \\\\\n",
    "    &= R_{\\max} \\sum_{k=0}^{n} \\gamma^k \\\\\n",
    "\\end{aligned}\n",
    "\n",
    "- But the last term is just a geometric series of $\\gamma$. Since $0 \\le \\gamma \\lt 1$, by geometric series we know that\n",
    "\n",
    "\\begin{aligned}\n",
    "    \\sum_{k=0}^{n} \\gamma^k &= \\frac{1 - \\gamma^{n+1}}{1 - \\gamma}\n",
    "\\end{aligned}\n",
    "\n",
    "- Therefore\n",
    "\n",
    "\\begin{aligned}\n",
    "    |S_n| &\\le R_{\\max} \\sum_{k=0}^{n} \\gamma^k \\\\\n",
    "    &= R_{\\max} \\frac{1 - \\gamma^{n+1}}{1 - \\gamma} \\\\\n",
    "    &\\le \\frac{R_{\\max}}{1 - \\gamma} \n",
    "\\end{aligned}\n",
    "\n",
    "- The last condition holds, because $1 - \\gamma^{n+1} \\lt 1$ if $0 \\le \\gamma \\lt 1$\n",
    "\n",
    "- Therefore, since $R_{\\max}$ is defined and $\\gamma$ is defined, it must be true that $|S_n|$ is defined, and therefore the discounted sum of future rewards must converge for any arbitrary $N$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcd50f1",
   "metadata": {},
   "source": [
    "## What is the meaning of Markov?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1d9b69",
   "metadata": {},
   "source": [
    "- Why do we consider this a Markov process?\n",
    "\n",
    "- The core assumption here is that the transition between states $P(S_{t+1} | S_t, A_t)$ is ONLY dependent on $S_t$; or formally\n",
    "\n",
    "\\begin{aligned}\n",
    "    P(S_{t+1}| S_t, a_t, S_{t-1}, a_{t-1} ...) &= P(S_{t+1}| S_t, a_t)\n",
    "\\end{aligned}\n",
    "\n",
    "- The current state is a sufficient statistic of the past for decision making. Under this assumption, you can make a Dynamic Programming / RL solving algorithms work"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
