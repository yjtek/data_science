{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kullback-Liebler Divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's suppose we have a distribution $P(X)$ and $Q(X)$ across some random variable X\n",
    "\n",
    "- We wish to evaluate how far $Q(X)$ differs (diverges) from $P(X)$. Let's denote this quantity as $D_{KL}(P || Q)$\n",
    "\n",
    "- Then KL divergence is defined as:\n",
    "$$\\begin{aligned}\n",
    "    D_{KL}(P || Q) &= \\sum_{x \\in X} P(X) \\log(\\frac{P(X)}{Q(X)})\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can pretty much take this formula and run with it, because it's quite idiot proof tbh\n",
    "\n",
    "- But building up this formula from its information theory foundations is extremely instructive. So we'll have a `Theoretical Foundations` section below to deal with exactly this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this section, we will implement the KL divergence of a biased vs an unbiased die\n",
    "\n",
    "- The point is to see how, as the dice gets increasingly biased, the divergence gets larger!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "unbiased_die = {x+1: 1/6 for x in range(6)}\n",
    "biased_die = {x+1: ((x+1)/6)**2 / np.sum([((x+1)/6)**2 for x in range(6)]) for x in range(6)}\n",
    "biased_die2 = {x+1: ((x+1)/6)**3 / np.sum([((x+1)/6)**3 for x in range(6)]) for x in range(6)}\n",
    "biased_die3 = {x+1: ((x+1)/6)**4 / np.sum([((x+1)/6)**4 for x in range(6)]) for x in range(6)}\n",
    "\n",
    "# unbiased_die, biased_die, biased_die2, biased_die3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(p, q):\n",
    "    assert p.keys() == q.keys()\n",
    "\n",
    "    res = 0\n",
    "    for k in unbiased_die.keys():\n",
    "        res += p[k] * np.log(p[k]/q[k])\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.5260162999520948)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_divergence(unbiased_die, biased_die)\n",
    "# kl_divergence(unbiased_die, biased_die2) ##increasing divergence with increasingly distance from original\n",
    "# kl_divergence(unbiased_die, biased_die3) ##increasing divergence with increasingly distance from original\n",
    "\n",
    "# ## However, divergence measure is NOT symmetric!\n",
    "# kl_divergence(biased_die, unbiased_die) == kl_divergence(unbiased_die, biased_die)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.5260162999520948)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.special import kl_div\n",
    "np.sum(kl_div([1/6]*6, list(biased_die.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- KL Divergence has a straightforward implementation, and the theory builds up from **Shannon Entropy**. If you're not 100% familiar, go read the notebook on the topic. The stuff below assumes full knowledge of Shannon Entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KL Divergence from Shannon Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We've previously established how Shannon entropy is derived, KL divergence is nothing more than an extension of the entropy definition!\n",
    "\n",
    "- Shannon Entropy Recap: \n",
    "    - For a given count of outcomes $N$, entropy measures the minimum bits needed to encode the outcomes\n",
    "    - If all outcomes are equally likely, each outcome requires $\\log_2(N)$ bits (i.e. for 8 outcomes, you need 3 bits, because 2^3=8)\n",
    "        - So the weighted average bits needed (or the Shannon Entropy) across all outcomes is simply $N \\cdot \\frac{1}{N} \\log_2(N)$\n",
    "            - $\\frac{1}{N}$ is the probablility of observing each of $N$ equally likely outcomes\n",
    "            - $\\frac{1}{N} \\log_2(N)$ is the probability weighted number of bits needed for 1 outcome\n",
    "            - $N \\cdot \\frac{1}{N} \\log_2(N)$ is the weighted average bits needed\n",
    "        - Note that $\\log_2(N)$ can be written as $- \\log_2(\\frac{1}{N})$, and $P(X) = \\frac{1}{N}$, which gives us $- \\log_2(P(X))$\n",
    "    - If outcomes are unequal, then we can simply weight the outcome probabilities to get the weighted average bits required\n",
    "        - $-\\sum_X P(X) \\log_2(P(X))$\n",
    "\n",
    "- KL Divergence from Entropy \n",
    "    - Suppose we have some set of discrete events that follows a probability distribution $P(X)$\n",
    "    - Suppose we mistakenly attribute a different probability distribution $Q(X)$\n",
    "    - Then, our mistake will have created additional \"uncertainty\", which can be measured by\n",
    "    $$\\begin{aligned}\n",
    "        -\\sum_X P(X) \\log_2(Q(X))\n",
    "    \\end{aligned}$$\n",
    "\n",
    "    - This is also known as **cross entropy**\n",
    "\n",
    "    - Since $\\log_2(Q(X))$ is negative and monotonically increasing between 0 and 1 (which is the support of $Q(X)$), it must be true that $-\\sum_X P(X) \\log_2(Q(X))$ is minimised when $P(X) = Q(X)$. This is because the largest weight $P(X)$ will then be multiplied by the smallest possible $\\log_2(Q(X))$\n",
    "\n",
    "    - Therefore, any mistaken assumption of $Q(X) \\neq P(X)$ will **create** uncertainty, which increases entropy. \n",
    "    $$\\begin{aligned}\n",
    "        -\\sum_X P(X) \\log_2(Q(X)) \\ge -\\sum_X P(X) \\log_2(P(X))\n",
    "    \\end{aligned}$$\n",
    "\n",
    "    - This uncertainty created can be treated as a meausre of how different $P(X)$ and $Q(X)$ are from each other, which is given by \n",
    "    $$\\begin{aligned}\n",
    "        -\\sum_X P(X) \\log_2(Q(X)) + \\sum_X P(X) \\log_2(P(X)) &= \\sum_X P(X) \\log_2(\\frac{P(X)}{Q(X)}) \\\\\n",
    "        &= \\text{KL Divergence}\n",
    "    \\end{aligned}$$\n",
    "\n",
    "- This gives us the KL divergence exactly!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5849625007211556 1.8484436218941096 1.7956083181006413 1.8329487933642483\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "unbiased_die = {x+1: 1/6 for x in range(6)}\n",
    "biased_die = {x+1: ((x+1)/6)**2 / np.sum([((x+1)/6)**2 for x in range(6)]) for x in range(6)}\n",
    "biased_die2 = {x+1: ((x+1)/6)**3 / np.sum([((x+1)/6)**3 for x in range(6)]) for x in range(6)}\n",
    "biased_die3 = {x+1: ((x+1)/6)**4 / np.sum([((x+1)/6)**4 for x in range(6)]) for x in range(6)}\n",
    "\n",
    "biased2_unbiased_cross_entropy = 0\n",
    "for key in unbiased_die:\n",
    "    biased2_unbiased_cross_entropy += -biased_die2[key] * np.log2(unbiased_die[key])\n",
    "\n",
    "biased2_biased_cross_entropy = 0\n",
    "for key in unbiased_die:\n",
    "    biased2_biased_cross_entropy += -biased_die2[key] * np.log2(biased_die[key])\n",
    "\n",
    "biased2_biased2_cross_entropy = 0\n",
    "for key in unbiased_die:\n",
    "    biased2_biased2_cross_entropy += -biased_die2[key] * np.log2(biased_die2[key])\n",
    "\n",
    "biased2_biased3_cross_entropy = 0\n",
    "for key in unbiased_die:\n",
    "    biased2_biased3_cross_entropy += -biased_die2[key] * np.log2(biased_die3[key])\n",
    "\n",
    "### Cross entropy is minimised when P(X) = Q(X)\n",
    "print(biased2_unbiased_cross_entropy, biased2_biased_cross_entropy, biased2_biased2_cross_entropy, biased2_biased3_cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.cs.cmu.edu/~dst/Tutorials/Info-Theory/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science-SgH_SThs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
