{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jensen Shannon Divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NOTE: To understand this derivation, you first need to understand Kullback-Liebler Divergence (KL Divergence). So go read that notebook if you are not already clear about the derivation of KL Divergence. This notebook will assume full knowledge of everything there*\n",
    "\n",
    "- We have established that the KL-divergence between distributions $P(X)$ and $Q(X)$, denoted as $D_{KL}(P || Q)$ is simply the difference between the cross entropy of approximating $P(X)$ with $Q(X)$, and the entropy of $Q(X)$\n",
    "$$\\begin{aligned}\n",
    "    D_{KL}(P || Q) &= \\sum_X P(X) \\log_2(\\frac{P(X)}{Q(X)})\n",
    "\\end{aligned}$$\n",
    "\n",
    "- However, this runs into an immediate problem; KL divergence is not symmetric. \n",
    "$$\\begin{aligned}\n",
    "    D_{KL}(P || Q) \\neq D_{KL}(Q || P)\n",
    "\\end{aligned}$$\n",
    "\n",
    "- Jensen-Shannon is simply adapting the KL-divergence to make it a symmetric measure of distance between $P(X)$ and $Q(X)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To ensure symmetric measure of distance, for a given $P(X)$ and $Q(X)$, find the midpoint distribution $M(X)$ by taking $M(X) = \\frac{P(X) + Q(X)}{2}$\n",
    "\n",
    "- Then, we compute the KL-divergences $D_{KL}(P || M)$ and $D_{KL}(Q || M)$\n",
    "\n",
    "- Finally, just take the weighted average of the 2 KL-divergences as the Jensen Shannon Divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def getdist():\n",
    "    x = np.random.rand(10) \n",
    "    return x/np.sum(x)\n",
    "\n",
    "def yj_kl_div(p, q):\n",
    "    res=0\n",
    "    for i in range(len(p)):\n",
    "        res += p[i] * np.log(p[i]/q[i])\n",
    "    return res\n",
    "\n",
    "def yj_js_div(p, q):\n",
    "    m = (p+q)/2\n",
    "    js_div = 0.5 * (yj_kl_div(p,m) + yj_kl_div(q,m))\n",
    "    return js_div\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.10552098302073679)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = getdist()\n",
    "q = getdist()\n",
    "yj_js_div(p, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.10552098302073681)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.spatial.distance import jensenshannon\n",
    "jensenshannon(p,q) ** 2 ##the scipy module returns the sqrt value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science-SgH_SThs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
