{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutual Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NOTE: To understand this derivation, you first need to understand Shannon Entropy. This notebook will assume full knowledge of everything there*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mutual information measures the extent to which 2 variables are independent from one another, and this is very related to KL divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We know that entropy is a measure of uncertainty given by some probability distribution $P$ over some discrete random variable $X$\n",
    "    - $H(X) = \\sum_X P(X) \\log_2(P(X))$ is a measure of the uncertainty for a given distribution $P$ (i.e. entropy)\n",
    "\n",
    "- In the same way, we can define an entropy for a joint distribution of 2 random variables $X$ and $Y$\n",
    "    - $H(X,Y) = \\sum_X P(X,Y) \\log_2(P(X,Y))$ \n",
    "\n",
    "- In KL Divergence, we know that $D_{KL}(P || Q)$ gives us the measure of how much uncertainty increases when we use distribution $Q$ to approximate distribution $P$\n",
    "\n",
    "- The idea here is that we can check the extent to which $X$ and $Y$ are independent, by checking how much uncertainty increases when we use distribution $P(X) \\cdot P(Y)$ to estimate the joint distribution $P(X,Y)$\n",
    "    - The idea here is that if X and Y are independent, then $P(X,Y) = P(X) \\cdot P(Y)$\n",
    "\n",
    "- Therefore, Mutual Information (MI) is measured by the Kullback-Liebler Divergence between $P(X,Y)$ and $P(X) \\cdot P(Y)$\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    D_{KL}(P(X,Y) || P(X) \\cdot P(Y)) &= \\sum_X \\sum_Y P(X,Y) \\cdot \\log(\\frac{P(X,Y)}{P(X) \\cdot P(Y)})\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from collections import Counter\n",
    "\n",
    "N=1000\n",
    "NRANGE=5\n",
    "\n",
    "X1 = np.random.randint(0,NRANGE,N) # Base\n",
    "X2 = X1 + np.random.randint(-2,3,N) # X1 + Symmetric Random Noise\n",
    "X3 = np.random.randint(0,NRANGE,N) # Independent Draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_distribution_map(X1, X2=None):\n",
    "    if X2 is None:\n",
    "        dist_map = dict(Counter(X1))\n",
    "    else:\n",
    "        dist_map = dict(Counter([(x,y) for x,y in zip(X1, X2)]))\n",
    "    \n",
    "    return {k: v/N for k,v in dist_map.items()}\n",
    "\n",
    "def yj_mutual_info_score(X1, X2):\n",
    "    joint_dist_x1x2 = make_distribution_map(X1=X1, X2=X2)\n",
    "    dist_x1 = make_distribution_map(X1)\n",
    "    dist_x2 = make_distribution_map(X2)\n",
    "\n",
    "    res = 0\n",
    "    for joint_key in joint_dist_x1x2.keys():\n",
    "        xkey, ykey = joint_key\n",
    "        entropy = joint_dist_x1x2[joint_key] * np.log(joint_dist_x1x2[joint_key] / (dist_x1[xkey] * dist_x2[ykey]))\n",
    "        res += entropy\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.609237515573737 0.4750464928364453 0.0076388736754139435\n"
     ]
    }
   ],
   "source": [
    "print(yj_mutual_info_score(X1,X1), yj_mutual_info_score(X1,X2), yj_mutual_info_score(X1,X3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6092375155737368 0.4750464928364452 0.007638873675413535\n"
     ]
    }
   ],
   "source": [
    "print(mutual_info_score(X1, X1), mutual_info_score(X1, X2), mutual_info_score(X1, X3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science-SgH_SThs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
