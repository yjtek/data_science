{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shannon Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Shannon entropy is the main building block for many distance measures, and so deserves its own notebook. Many other measures (KL Divergence, Jensen Shannon, etc) will be derived based on the definition of Shannon Entropy here, so this is a super important segment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deriving Shannon Entropy from Information Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the digital domain, we often use `bits` as our building blocks of information\n",
    "\n",
    "- A `bit` is just a single on-off switch. It has the value of 1 when on, and 0 when off. By putting bits together, we can express more complex pieces of information.\n",
    "    - The most canonical example of using bits to represent information is using them to represent numbers (i.e. binary)\n",
    "    - \n",
    "\n",
    "- Bit strings of longer lengths can encode more information\n",
    "    - length 1 --> 2 states, 0 or 1\n",
    "    - length 2 --> 4 states, 00 or 10 or 01 or 11\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Representing information with bits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's consider a coin flip\n",
    "    - If we wish to communicate the result of a coin flip, the simplest idea is to use 1 bit of information. That is, let 1 represent heads, and 0 represent tails.\n",
    "    - Then for $N$ flips, we require $N$ bits of information\n",
    "\n",
    "- Let's now suppose the same scenario, but with a biased coin that produces heads 99% of the time, and tails 1% of the time\n",
    "    - We can use the same encoding scheme of 1 for heads and 0 for tails to encode the result of $N$ tosses, which will again give us $N$ bits of information\n",
    "    - **We can do better!**\n",
    "        - Let's suppose $N = 1,000,000$ \n",
    "        - In a million tosses, we expect 10,000 tails, and 990,000 heads\n",
    "        - Since there are only 2 outcomes, and most of them are heads, we can choose to transmit only the information about the 10,000 tails!\n",
    "        - Since we need to encode **where** the tails occur, we need approximately 20bits of information for each tail, which will encode the index (between 0 and 1 million) where the tail occurs\n",
    "        - This lets us express 10,000 tails in 20 * 10,000 = 200,000 bytes!\n",
    "    - **We can do even better!**\n",
    "        - Instead of transmitting the raw indices, we can choose to transmit the distance between tail!\n",
    "        - Since there are 10,000 tails, on average there must be around 100 flips between each tail\n",
    "        - So if we only encode the distance between each tail, we require around 7 bits of information per tail on average. Let's call it 10 bits, to account for long stretches of heads\n",
    "        - Therefore, information about the 10,000 tails can be expressed using 10,000 * 10 = 100,000 bits!\n",
    "        - Therefore, 100,000 / 1,000,000 = 0.1 bits needed per flip on average\n",
    "    - Since we used a fixed number of bits to represent information of each flip, we call this **fixed length encoding**\n",
    "\n",
    "- TLDR;\n",
    "    1. We can represent \"information\" (e.g. the results from a flip of a coin) using bits\n",
    "    2. We don't always need a full 1 bit to represent 1 outcome; if we do things smartly!\n",
    "        - For instance, in this case, we represent the same information (the count and position of \"tails\") in a different way (representing the distance between tails instead of their positions directly)\n",
    "        - This lets us encode what was originally $N$ bits, into something much less\n",
    "        - Since each representation of distance takes up the same number of bits, we call it **fixed length encoding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable Length Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's expand our scenario to something with more than 2 states. Let's imagine with have an 8 sided die, instead of a coin. \n",
    "    - Now, there are 8 outcomes instead of 2.\n",
    "\n",
    "- Since there are 8 outcomes, we can represent each outcome with 3 bits of information $\\log_2(8) = 3$. \n",
    "    - Imagine we toss this die 1000 times. Since each outcome is represented with a binary string of length 3, this requires 3*1000 = 3000 bits\n",
    "    - We cannot apply our earlier trick of representing only the distances between some selected outcome, because there are now 8 different scenarios to encode!!\n",
    "\n",
    "- Nonetheless, **We can do better!** assuming the dice is not fair!!\n",
    "    - Let's suppose the probabilities of the 8 outcomes are as follows\n",
    "        - $\\frac{1}{2}, \\frac{1}{4}, \\frac{1}{8}, \\frac{1}{16}, \\frac{1}{32}, \\frac{1}{64}, \\frac{1}{128}, \\frac{1}{128}$\n",
    "    - Up to this point, we have used 3 bits to encode each outcome (000 for outcome 1, 001 for outcome 2, etc.,), giving us 3000 bits needed for 1000 rolls of the dice\n",
    "        - Outcome 1 to Outcome 8: 000, 001, 010, 100, 011, 101, 110, 111\n",
    "    - What if we exploit the difference in the outcome probabilities? \n",
    "        - Idea: Encode outcomes with larger probabilities with shorter strings, so we use fewer bits overall\n",
    "\n",
    "    - Let's try this instead: \n",
    "        - Outcome 1 to Outcome 8: 0, 10, 110, 1110, 11110, 111110, 1111110, 11111110\n",
    "    \n",
    "    - How many bits does it take to encode 3000 rolls now?\n",
    "        - $\\frac{1}{2} + \\frac{2}{4} + \\frac{3}{8} ... + \\frac{7}{128} \\approx 1.98$ bits per roll, or 1980 bits for 1000 rolls! \n",
    "        - This cuts the number of bits needed by more than one third!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generalising Variable Length Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We've seen how representing more frequent common outcomes with fewer bits can let us encode information more efficiently\n",
    "\n",
    "- Intuitively, for a given number of outcomes $N$, we will always need $\\log_2(N)$ bits to encode each outcome, assuming each outcome is equally likely. \n",
    "    - This has been formalised as jargon; so now $\\log_2(N)$ is known as the **entropy** of a uniform distribution with $N$ outcomes\n",
    "\n",
    "- When the outcomes are NOT equally likely, we can adjust the expected number of bits needed by the weighted sum of the information content, as we did in the example above!\n",
    "    - Recall that we looked at $\\log_2(N)$ as the number of bits needed\n",
    "    - This can be rewritten as $-\\log_2(1/N) = -\\log_2(P(X))$, where $P(X)$ is the probability of observing some outcome $X$\n",
    "    \n",
    "- Then, the number of bits needed is simply the weighted average of the number of bits needed for each outcome \n",
    "$$\\begin{aligned}\n",
    "    \\text{Bits needed to encode each roll} &= - \\sum_X P(X) \\log_2(P(X)) \\\\\n",
    "    &= \\text{Uncertainty} \\\\\n",
    "    &= \\text{Shannon Entropy}\n",
    "\\end{aligned}$$\n",
    "\n",
    "- This derivation gives us the exact definition of **Shannon Entropy**\n",
    "    - The more bits are needed to encode the distribution, the more uncertain the outcome is "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shannon Entropy maximisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To build intuition, we study when Shannon Entropy is maximised and when it is minimised using the same die roll example from the implementation\n",
    "\n",
    "- We see that we get progressively less uncertain (lower entropy) as the outcomes become more skewed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.584962500721156 2.082047059877024 1.7956083181006413 1.5556975591204392\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "unbiased_die = {x+1: 1/6 for x in range(6)}\n",
    "biased_die = {x+1: ((x+1)/6)**2 / np.sum([((x+1)/6)**2 for x in range(6)]) for x in range(6)}\n",
    "biased_die2 = {x+1: ((x+1)/6)**3 / np.sum([((x+1)/6)**3 for x in range(6)]) for x in range(6)}\n",
    "biased_die3 = {x+1: ((x+1)/6)**4 / np.sum([((x+1)/6)**4 for x in range(6)]) for x in range(6)}\n",
    "\n",
    "unbiased_die_shannon_entropy = 0\n",
    "for key in unbiased_die:\n",
    "    unbiased_die_shannon_entropy += -unbiased_die[key] * np.log2(unbiased_die[key])\n",
    "\n",
    "biased_die_shannon_entropy = 0\n",
    "for key in biased_die:\n",
    "    biased_die_shannon_entropy += -biased_die[key] * np.log2(biased_die[key])\n",
    "\n",
    "biased_die2_shannon_entropy = 0\n",
    "for key in biased_die2:\n",
    "    biased_die2_shannon_entropy += -biased_die2[key] * np.log2(biased_die2[key])\n",
    "\n",
    "biased_die3_shannon_entropy = 0\n",
    "for key in biased_die3:\n",
    "    biased_die3_shannon_entropy += -biased_die3[key] * np.log2(biased_die3[key])\n",
    "\n",
    "\n",
    "print(unbiased_die_shannon_entropy, biased_die_shannon_entropy, biased_die2_shannon_entropy, biased_die3_shannon_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proof that Shannon Entropy is the lower bound on the expected number of bits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We've covered a few ways that we can reduce the number of bits used when representing; e.g. fixed/variable length encoding. However, we have concluded above that Shannon Entropy represents the best we can do; that is, the lower bound of bits we need to encode a set of information\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\text{Shannon Entropy} &= - \\sum_X P(X) \\log_2(P(X))\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is a pretty foundational idea in modern AI/machine learning development, so it pays to dig a little deeper to understand this. How do we know that this is the best we can do? Restating with jargon, how do we know that Shannon Entropy represents the **information-theoretic lower bound** of the expected number of bits per symbol for any uniquely decodable length of code?\n",
    "\n",
    "- A non-formal proof is provided in `kl_divergence.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
