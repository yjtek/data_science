{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "- Regularised regression a similar idea to the usual OLS regression, except that we constrain the estimated parameters in some way\n",
    "\n",
    "- While this notebook will look specifically at regularised *regression*, do note that regularisation as a concept can and has been applied to multiple other domains (e.g. tree-based boosting, neural net building etc.)\n",
    "\n",
    "- In this notebook, we look specifically at 2 forms of regularisation; L1 regularisation (also known as LASSO), and L2 regularisation (also known as ridge regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why must KKT Conditions be met for optimal $\\hat{\\beta}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "- This section will do 2 things: \n",
    "    - We will look at the generalised theory of a constrained OLS regression\n",
    "    - In doing so, we will also show logically why any optimal solution must meet the 4 Kurush-Kuhn-Tucker (KKT) conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In normal OLS, we assume the following functional form\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    y &= X\\beta\n",
    "\\end{aligned}$$\n",
    "\n",
    "- We wish to find estimates of $\\beta$ that minimises the following loss function\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\cal L &= (y - X\\hat{\\beta})^2\n",
    "\\end{aligned}$$\n",
    "\n",
    "- In the case of regularisation, we wish to minimise the same loss, **BUT** with some constraint. Let's express this constraint as some function of the model parameters $\\beta$ being less than some constant $t$\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\cal L &= (y - X\\hat{\\beta})^2 \\qquad \\text{s.t. } f(\\beta) \\le t\n",
    "\\end{aligned}$$\n",
    "\n",
    "- To incorporate this constraint into the optimisation process, we simply introduce an addition to the loss, there increases when the constraint is violated. As we can see below, whenever $f(\\beta) > t$, we get an additional loss term, which forces our $\\beta$ value downwards in the optimisation\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\cal L &= (y - X\\hat{\\beta})^2 + (f(\\beta) - t)\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "$$\\begin{array}{c}\n",
    "    \\textbf{KKT 1: Primal Feasibility} \\\\ \n",
    "    \\text{For a solution $\\hat{\\beta}$ to be valid, it must respect the constraint $f(\\beta) \\le t$, or $f(\\beta) - t \\le 0$}\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- However, note 1 issue here: when $f(\\beta) < t$ (i.e. an interior point), $(f(\\beta) - t) < 0$. This means that we can end up with a lower $\\cal L$ from reducing $f(\\beta)$, which may shift $\\beta$ away from its true optimum point\n",
    "\n",
    "- So ideally\n",
    "    - $f(\\beta) - t$ should be \"switched on\" when $f(\\beta) > t$ to increase the loss of violating the condition\n",
    "    - But it should be \"switched off\" when $f(\\beta) < t$ to avoid adjusting $\\beta$ away from the optimum $\\hat{\\beta}$ that minimises the MSE term\n",
    "\n",
    "- To allow for such an interaction , we introduce a term $\\lambda$ into $\\cal L$ such that\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\cal L &= (y - X\\hat{\\beta})^2 + \\lambda (f(\\beta) - t)\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{array}{c}\n",
    "    \\textbf{KKT 2: Complementary Slackness} \\\\ \n",
    "    \\text{For a solution $\\hat{\\beta}$ to be valid, it must be true that either (i) $\\lambda = 0$ when $f(\\hat{\\beta}) < t$} \\\\\n",
    "    \\text{Or (ii) $f(\\hat{\\beta}) = t$, because by primal feasibility, we never have $f(\\hat{\\beta}) > t$} \\\\ \n",
    "    \\text{This means that $\\lambda \\cdot (f(\\hat{\\beta}) - t) = 0$}\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- By the formulation of the loss function below, the constraint term must contribute positively to the loss when the constraint is violated. That is, we must always be **adding** to the loss if $(f(\\beta) > t)$\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\cal L &= (y - X\\hat{\\beta})^2 + \\lambda (f(\\beta) - t)\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "$$\\begin{array}{c}\n",
    "    \\textbf{KKT 3: Dual Feasibility} \\\\ \n",
    "    \\text{When $f(\\beta) > t$, the condition is violated} \\\\\n",
    "    \\text{In such a case, $\\cal L$ must increase} \\\\\n",
    "    \\text{For $\\cal L$ to increase, $\\lambda (f(\\beta) - t) \\ge 0$} \\\\\n",
    "    \\text{Since $f(\\beta) > t$, $f(\\beta) - t > 0$} \\\\\n",
    "    \\text{Therefore, $\\lambda > 0$ must also be true, else we'd be lowering the loss when violating the constraint} \\\\\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finally, when we find a solution $\\hat{\\beta}$, how do ew know it is optimal?\n",
    "\n",
    "- Recall that $\\hat{\\beta}$ must minimise the loss $\\cal L$, and to solve for $\\hat{\\beta}$, we set the first order condition of $\\cal L$ to be 0\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\nabla_\\beta \\cal L_{\\lambda}(\\beta) &= \\nabla_\\beta \\left \\| y - X\\beta \\right \\|^2 + \\lambda \\nabla_\\beta f(\\beta) \\\\ \n",
    "    &= 0\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{array}{c}\n",
    "    \\textbf{KKT 4: Stationarity} \\\\ \n",
    "    \\text{At optimal solution $\\hat{\\beta}$, any change in $\\beta$ should not change the loss (i.e. gradient of loss relative to beta must be 0)} \n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derivation of L2 Regularisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we established in the section above, it must be true that "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
