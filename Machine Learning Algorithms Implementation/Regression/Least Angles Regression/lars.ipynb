{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d952ef1",
   "metadata": {},
   "source": [
    "## Least Angles Regression (LARS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d3c8f8",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "71259d7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, -1,  1, -1,  1, -1,  1, -1,  1, -1])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.sign([((-1)**x)*x for x in range(10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cd25de",
   "metadata": {},
   "source": [
    "Input: \n",
    "    - Data matrix X (n x p), centered and normalized (columns have mean 0 and norm 1)\n",
    "    - Response vector y, centered (mean 0)\n",
    "Output: \n",
    "    - Path of coefficients β over steps\n",
    "\n",
    "Initialize:\n",
    "    β = 0\n",
    "    r = y             # residual\n",
    "    A = []            # active set (indices of predictors currently in the model)\n",
    "    β_path = [β]\n",
    "\n",
    "While |A| < p:\n",
    "\n",
    "    # 1. Compute correlations with residual\n",
    "    c = X.T @ r                     # correlations between each predictor and residual\n",
    "    C = max(|c|)                    # largest absolute correlation\n",
    "    j = argmax(|c|)                 # index of most correlated predictor\n",
    "\n",
    "    # 2. Add j to active set\n",
    "    A.append(j)\n",
    "    X_A = X[:, A]                  # submatrix with active predictors\n",
    "\n",
    "    # 3. Compute equiangular direction u_A\n",
    "    G_A = X_A.T @ X_A              # Gram matrix\n",
    "    one_vec = [sign(c[j]) for j in A] # for each active predictor `a` in A, indicate if the correlation `c` between `a` and `r` is +ve or -ve\n",
    "    w = inv(G_A) @ one_vec\n",
    "    u = X_A @ w / sqrt(one_vec.T @ w)   # equiangular direction (unit norm)\n",
    "\n",
    "    # 4. Compute step size γ (how far to go in this direction before another variable catches up)\n",
    "    a = X.T @ u\n",
    "    γ = min over j ∉ A of:\n",
    "        ((C - c[j]) / (C - a[j]), (C + c[j]) / (C + a[j])) if denominator > 0\n",
    "\n",
    "    # 5. Update β and residual\n",
    "    β[A] += γ * w\n",
    "    r = y - X @ β\n",
    "    β_path.append(β.copy())\n",
    "\n",
    "Return β_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "08ac319a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, LassoLars\n",
    "from sklearn.datasets import make_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "45d7caa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([39.69859513, 64.71630739, 61.73356078, 33.10463631, 20.27644721]),\n",
       " array([39.6985006 , 64.71631659, 61.73357321, 33.10462564, 20.27645807]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = make_regression(n_informative=5, n_samples=200, n_features=5)\n",
    "Lasso(fit_intercept=False).fit(X,y).coef_, LassoLars(fit_intercept=False).fit(X,y).coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33230bb6",
   "metadata": {},
   "source": [
    "- LARS is a variable selection technique, and can be treated as an \"upgraded\" forward stepwise regression\n",
    "    - In stepwise regerssion, we iteratively add the best variable relative to the target\n",
    "    - In LARS, we add the variable most correlated to the current residual\n",
    "\n",
    "- Inuitively, the idea of LARS is quite simple\n",
    "    - We start of with 0s for all coefficients, and find the residuals. If all coefficients are 0, the residual is just the target $y$ at the outset\n",
    "    - Next, we identify the predictor that correlates most with the target among those not yet chosen\n",
    "    - While\n",
    "\n",
    "- Start with all coefficients = 0. So the initial residual is just the response vector y.\n",
    "\n",
    "- Find the predictor most correlated with the residual — that is, the one that best explains what hasn’t been modeled yet.\n",
    "\n",
    "- Move the coefficient for that predictor in the direction that minimally increases the residual sum of squares, until:\n",
    "    - Another predictor becomes equally correlated with the current residual.\n",
    "\n",
    "- At that point, move in a direction that is equiangular between the current set of active predictors.\n",
    "\n",
    "- Continue this process, adding predictors to the active set and adjusting their coefficients accordingly.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science-SgH_SThs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
