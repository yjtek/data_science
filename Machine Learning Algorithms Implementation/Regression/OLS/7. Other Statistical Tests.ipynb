{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the OLS Summary Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the previous section, we showed theoretically and practically how we can derive a coefficient matrix $\\beta$, just from the objective function of minimising the mean squared error (MSE)\n",
    "\n",
    "- But you should notice something odd about our results. Our matrix algebra gave us only coefficient values\n",
    "\n",
    "- But the OLS table actually gives us so much more than this! \n",
    "\n",
    "- How can we derive every part of the OLS Summary table? Let's find out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 6)\n",
      "[-0.16521089  0.2381359   0.00976686 60.45175552 26.46640238  4.8924384 ]\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.994</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.994</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>1.775e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 17 Jan 2025</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>17:09:58</td>     <th>  Log-Likelihood:    </th> <td> -1508.0</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   500</td>      <th>  AIC:               </th> <td>   3028.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   494</td>      <th>  BIC:               </th> <td>   3053.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>   -0.1652</td> <td>    0.233</td> <td>   -0.710</td> <td> 0.478</td> <td>   -0.622</td> <td>    0.292</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    0.2381</td> <td>    0.236</td> <td>    1.008</td> <td> 0.314</td> <td>   -0.226</td> <td>    0.702</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>    0.0098</td> <td>    0.222</td> <td>    0.044</td> <td> 0.965</td> <td>   -0.426</td> <td>    0.445</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>   60.4518</td> <td>    0.222</td> <td>  272.722</td> <td> 0.000</td> <td>   60.016</td> <td>   60.887</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>   26.4664</td> <td>    0.227</td> <td>  116.601</td> <td> 0.000</td> <td>   26.020</td> <td>   26.912</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    4.8924</td> <td>    0.223</td> <td>   21.982</td> <td> 0.000</td> <td>    4.455</td> <td>    5.330</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 1.207</td> <th>  Durbin-Watson:     </th> <td>   1.760</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.547</td> <th>  Jarque-Bera (JB):  </th> <td>   1.202</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.028</td> <th>  Prob(JB):          </th> <td>   0.548</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.766</td> <th>  Cond. No.          </th> <td>    1.11</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &        y         & \\textbf{  R-squared:         } &     0.994   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.994   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } & 1.775e+04   \\\\\n",
       "\\textbf{Date:}             & Fri, 17 Jan 2025 & \\textbf{  Prob (F-statistic):} &     0.00    \\\\\n",
       "\\textbf{Time:}             &     17:09:58     & \\textbf{  Log-Likelihood:    } &   -1508.0   \\\\\n",
       "\\textbf{No. Observations:} &         500      & \\textbf{  AIC:               } &     3028.   \\\\\n",
       "\\textbf{Df Residuals:}     &         494      & \\textbf{  BIC:               } &     3053.   \\\\\n",
       "\\textbf{Df Model:}         &           5      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "               & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{x1}    &      -0.1652  &        0.233     &    -0.710  &         0.478        &       -0.622    &        0.292     \\\\\n",
       "\\textbf{x2}    &       0.2381  &        0.236     &     1.008  &         0.314        &       -0.226    &        0.702     \\\\\n",
       "\\textbf{x3}    &       0.0098  &        0.222     &     0.044  &         0.965        &       -0.426    &        0.445     \\\\\n",
       "\\textbf{x4}    &      60.4518  &        0.222     &   272.722  &         0.000        &       60.016    &       60.887     \\\\\n",
       "\\textbf{x5}    &      26.4664  &        0.227     &   116.601  &         0.000        &       26.020    &       26.912     \\\\\n",
       "\\textbf{const} &       4.8924  &        0.223     &    21.982  &         0.000        &        4.455    &        5.330     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       &  1.207 & \\textbf{  Durbin-Watson:     } &    1.760  \\\\\n",
       "\\textbf{Prob(Omnibus):} &  0.547 & \\textbf{  Jarque-Bera (JB):  } &    1.202  \\\\\n",
       "\\textbf{Skew:}          &  0.028 & \\textbf{  Prob(JB):          } &    0.548  \\\\\n",
       "\\textbf{Kurtosis:}      &  2.766 & \\textbf{  Cond. No.          } &     1.11  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.994\n",
       "Model:                            OLS   Adj. R-squared:                  0.994\n",
       "Method:                 Least Squares   F-statistic:                 1.775e+04\n",
       "Date:                Fri, 17 Jan 2025   Prob (F-statistic):               0.00\n",
       "Time:                        17:09:58   Log-Likelihood:                -1508.0\n",
       "No. Observations:                 500   AIC:                             3028.\n",
       "Df Residuals:                     494   BIC:                             3053.\n",
       "Df Model:                           5                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "x1            -0.1652      0.233     -0.710      0.478      -0.622       0.292\n",
       "x2             0.2381      0.236      1.008      0.314      -0.226       0.702\n",
       "x3             0.0098      0.222      0.044      0.965      -0.426       0.445\n",
       "x4            60.4518      0.222    272.722      0.000      60.016      60.887\n",
       "x5            26.4664      0.227    116.601      0.000      26.020      26.912\n",
       "const          4.8924      0.223     21.982      0.000       4.455       5.330\n",
       "==============================================================================\n",
       "Omnibus:                        1.207   Durbin-Watson:                   1.760\n",
       "Prob(Omnibus):                  0.547   Jarque-Bera (JB):                1.202\n",
       "Skew:                           0.028   Prob(JB):                        0.548\n",
       "Kurtosis:                       2.766   Cond. No.                         1.11\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "# import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "\n",
    "x,y = make_regression(\n",
    "    n_samples=500, \n",
    "    n_features=5, \n",
    "    n_informative=2, \n",
    "    n_targets=1, \n",
    "    noise=5, \n",
    "    bias=5,\n",
    "    random_state=123\n",
    ")\n",
    "x = np.append(x, np.ones((500,1)), axis = 1)\n",
    "print(x.shape)\n",
    "\n",
    "betas = np.linalg.inv((x.transpose() @ x)) @ x.transpose() @ y\n",
    "np.set_printoptions(suppress=True)\n",
    "print(betas)\n",
    "\n",
    "print('='*50)\n",
    "res = sm.OLS(exog=x, endog=y, hasconst=True).fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Statistical Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = x.shape[0]\n",
    "epsilon = y - x @ betas\n",
    "s = np.std(epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skew and Kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.02770497453847136), np.float64(-0.2336862603260439))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "_skew = skew(epsilon)\n",
    "_kurtosis = kurtosis(epsilon) ##+3 if you want regular kurtosis because normal distribution has kurtosis of 3, and scipy's implementation computes excess kurtosis\n",
    "_skew, _kurtosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Omnibus Normality Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This tests the hypothesis that there is no \"pattern\" left in the residuals after fitting the model. Usually used to detect issues with model fit\n",
    "- Specifically, we can use this to check the residuals for\n",
    "    - Non-normality: errors are assumed normal after fitting the model\n",
    "    - Homoscedasticity: errors are assumed homoscedastic, see 6. Summary Table: Standard Error of Coefficients and t-test\n",
    "    - Independence: errors are assumed independent\n",
    "\n",
    "- To compute the Omnibus statistic:\n",
    "    - Compute residuals\n",
    "    - Compute skewness and kurtosis of residuals\n",
    "    $$\\begin{aligned}\n",
    "        O = \\frac{n}{6} \\cdot (\\text{skew}^2 + \\frac{1}{4} \\cdot (\\text{kurtosis} - 3)^2)\n",
    "    \\end{aligned}$$\n",
    "    - $n$ is sample size, and $\\text{kurtosis} - 3$ because normal distribution has a kurtosis of 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NormaltestResult(statistic=np.float64(1.2073378518941995), pvalue=np.float64(0.5468017761105753))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import normaltest\n",
    "normaltest(epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jarque-Bera Normality Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Very closely related to the Omnibus test, we are testing for normality in the residuals\n",
    "- Only different in how the skew/kurtosis statistics are adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.2016568900391862)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jb_test_statistic = (_skew**2 + 0.25 * (_kurtosis)**2) * n/6\n",
    "jb_test_statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.5483571641059298)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.stats import chi2\n",
    "jb_p_value = 1 - chi2.cdf(jb_test_statistic, df=2)\n",
    "jb_p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Durbin Watson Statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is used to detect autocorrelation in residuals, and is probably more relevant if you are fitting ARIMA type models\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    DW &= \\frac{\\sum_{t=2}^{n} (e_t - e_{t-1})^2}{\\sum_{t=1}^{n} e_t^2}\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.7599039565117907)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eps_sq = np.sum(epsilon**2)\n",
    "eps_autocorr = np.sum([(epsilon[i] - epsilon[i-1])**2 for i in range(1, x.shape[0])])\n",
    "dw_stat = eps_autocorr/eps_sq\n",
    "dw_stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Condition Number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is a reflection of the multicollinearity seen in the variables\n",
    "\n",
    "- It is computed by taking the maximum singular value divided by the minimum singular value\n",
    "\n",
    "- Large condition number implies higher multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.1103349968531293)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u, s, vt = np.linalg.svd(x)\n",
    "condition_number = s.max()/s.min()\n",
    "condition_number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It is useful to think about the intuition of this:\n",
    "\n",
    "- When taking an SVD of some design matrix $X$, you get 3 matrixes\n",
    "    - $U$ --> \"direction\" in row space\n",
    "    - $V^T$ --> \"direction\" in column space\n",
    "    - $\\sum$ --> diagonal matrix containing the stretching and/or shrinking magnitudes along the principal directions\n",
    "\n",
    "- Ideally, if all your features are uncorrelated, you should have similarly large singular values\n",
    "\n",
    "- But if there is high correlation, intuitively there is one \"direction\" that captures most of the variation, with the rest being super small.\n",
    "\n",
    "- As such, the ratio of largest to smallest singular values will be large!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science-SgH_SThs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
