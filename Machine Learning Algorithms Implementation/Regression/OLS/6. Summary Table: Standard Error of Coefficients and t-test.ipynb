{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the OLS Summary Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the previous section, we showed theoretically and practically how we can derive a coefficient matrix $\\beta$, just from the objective function of minimising the mean squared error (MSE)\n",
    "\n",
    "- But you should notice something odd about our results. Our matrix algebra gave us only coefficient values\n",
    "\n",
    "- But the OLS table actually gives us so much more than this! \n",
    "\n",
    "- How can we derive every part of the OLS Summary table? Let's find out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 6)\n",
      "[-0.16521089  0.2381359   0.00976686 60.45175552 26.46640238  4.8924384 ]\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.994</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.994</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>1.775e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 17 Jan 2025</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>16:40:01</td>     <th>  Log-Likelihood:    </th> <td> -1508.0</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   500</td>      <th>  AIC:               </th> <td>   3028.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   494</td>      <th>  BIC:               </th> <td>   3053.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>   -0.1652</td> <td>    0.233</td> <td>   -0.710</td> <td> 0.478</td> <td>   -0.622</td> <td>    0.292</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    0.2381</td> <td>    0.236</td> <td>    1.008</td> <td> 0.314</td> <td>   -0.226</td> <td>    0.702</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>    0.0098</td> <td>    0.222</td> <td>    0.044</td> <td> 0.965</td> <td>   -0.426</td> <td>    0.445</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>   60.4518</td> <td>    0.222</td> <td>  272.722</td> <td> 0.000</td> <td>   60.016</td> <td>   60.887</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>   26.4664</td> <td>    0.227</td> <td>  116.601</td> <td> 0.000</td> <td>   26.020</td> <td>   26.912</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    4.8924</td> <td>    0.223</td> <td>   21.982</td> <td> 0.000</td> <td>    4.455</td> <td>    5.330</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 1.207</td> <th>  Durbin-Watson:     </th> <td>   1.760</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.547</td> <th>  Jarque-Bera (JB):  </th> <td>   1.202</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.028</td> <th>  Prob(JB):          </th> <td>   0.548</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.766</td> <th>  Cond. No.          </th> <td>    1.11</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &        y         & \\textbf{  R-squared:         } &     0.994   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.994   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } & 1.775e+04   \\\\\n",
       "\\textbf{Date:}             & Fri, 17 Jan 2025 & \\textbf{  Prob (F-statistic):} &     0.00    \\\\\n",
       "\\textbf{Time:}             &     16:40:01     & \\textbf{  Log-Likelihood:    } &   -1508.0   \\\\\n",
       "\\textbf{No. Observations:} &         500      & \\textbf{  AIC:               } &     3028.   \\\\\n",
       "\\textbf{Df Residuals:}     &         494      & \\textbf{  BIC:               } &     3053.   \\\\\n",
       "\\textbf{Df Model:}         &           5      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "               & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{x1}    &      -0.1652  &        0.233     &    -0.710  &         0.478        &       -0.622    &        0.292     \\\\\n",
       "\\textbf{x2}    &       0.2381  &        0.236     &     1.008  &         0.314        &       -0.226    &        0.702     \\\\\n",
       "\\textbf{x3}    &       0.0098  &        0.222     &     0.044  &         0.965        &       -0.426    &        0.445     \\\\\n",
       "\\textbf{x4}    &      60.4518  &        0.222     &   272.722  &         0.000        &       60.016    &       60.887     \\\\\n",
       "\\textbf{x5}    &      26.4664  &        0.227     &   116.601  &         0.000        &       26.020    &       26.912     \\\\\n",
       "\\textbf{const} &       4.8924  &        0.223     &    21.982  &         0.000        &        4.455    &        5.330     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       &  1.207 & \\textbf{  Durbin-Watson:     } &    1.760  \\\\\n",
       "\\textbf{Prob(Omnibus):} &  0.547 & \\textbf{  Jarque-Bera (JB):  } &    1.202  \\\\\n",
       "\\textbf{Skew:}          &  0.028 & \\textbf{  Prob(JB):          } &    0.548  \\\\\n",
       "\\textbf{Kurtosis:}      &  2.766 & \\textbf{  Cond. No.          } &     1.11  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.994\n",
       "Model:                            OLS   Adj. R-squared:                  0.994\n",
       "Method:                 Least Squares   F-statistic:                 1.775e+04\n",
       "Date:                Fri, 17 Jan 2025   Prob (F-statistic):               0.00\n",
       "Time:                        16:40:01   Log-Likelihood:                -1508.0\n",
       "No. Observations:                 500   AIC:                             3028.\n",
       "Df Residuals:                     494   BIC:                             3053.\n",
       "Df Model:                           5                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "x1            -0.1652      0.233     -0.710      0.478      -0.622       0.292\n",
       "x2             0.2381      0.236      1.008      0.314      -0.226       0.702\n",
       "x3             0.0098      0.222      0.044      0.965      -0.426       0.445\n",
       "x4            60.4518      0.222    272.722      0.000      60.016      60.887\n",
       "x5            26.4664      0.227    116.601      0.000      26.020      26.912\n",
       "const          4.8924      0.223     21.982      0.000       4.455       5.330\n",
       "==============================================================================\n",
       "Omnibus:                        1.207   Durbin-Watson:                   1.760\n",
       "Prob(Omnibus):                  0.547   Jarque-Bera (JB):                1.202\n",
       "Skew:                           0.028   Prob(JB):                        0.548\n",
       "Kurtosis:                       2.766   Cond. No.                         1.11\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "# import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "\n",
    "x,y = make_regression(\n",
    "    n_samples=500, \n",
    "    n_features=5, \n",
    "    n_informative=2, \n",
    "    n_targets=1, \n",
    "    noise=5, \n",
    "    bias=5,\n",
    "    random_state=123\n",
    ")\n",
    "x = np.append(x, np.ones((500,1)), axis = 1)\n",
    "print(x.shape)\n",
    "\n",
    "betas = np.linalg.inv((x.transpose() @ x)) @ x.transpose() @ y\n",
    "np.set_printoptions(suppress=True)\n",
    "print(betas)\n",
    "\n",
    "print('='*50)\n",
    "res = sm.OLS(exog=x, endog=y, hasconst=True).fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coefficient Standard Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We already know the main formula in OLS;\n",
    "    $$\\begin{aligned}\n",
    "        \\hat{\\beta} &= (X^TX)^{-1} X^Ty\n",
    "    \\end{aligned}$$\n",
    "\n",
    "- However, every $\\hat{\\beta}$ also has a corresponding standard error estimate $\\sigma_{\\hat{\\beta}}$, which lets us test how statistically significant the coefficients are. How is this magically derived?\n",
    "    $$\\begin{aligned}\n",
    "        \\sigma_{\\hat{\\beta}}^2 &= \\text{Var}({\\hat{\\beta}}) \\\\\n",
    "        &= \\text{Var}((X^TX)^{-1} X^Ty) \\\\\n",
    "        &= \\text{Var}((X^TX)^{-1} X^T(X\\beta + \\epsilon)) \\\\\n",
    "        &= \\text{Var}((X^TX)^{-1} X^TX\\beta + (X^TX)^{-1} X^T\\epsilon)) \\\\\n",
    "        &= \\text{Var}(\\beta + (X^TX)^{-1} X^T\\epsilon)) \\\\\n",
    "        &= \\text{Var}(\\beta) + \\text{Var}((X^TX)^{-1} X^T\\epsilon)) + 2 \\cdot \\text{Cov}(\\beta, (X^TX)^{-1} X^T \\epsilon) \\\\\n",
    "        &= \\text{Var}((X^TX)^{-1} X^T\\epsilon) & \\because \\text{Var}(\\beta) = 0 \\\\\n",
    "        &= (X^TX)^{-1} X^T \\text{Var}(\\epsilon) X (X^TX)^{-1} & \\because \\text{Var}(A\\epsilon) = A \\text{Var}(\\epsilon) A^T \\\\\n",
    "        &= (X^TX)^{-1} X^T \\sigma^2 I X (X^TX)^{-1} & \\because \\text{Var}(\\epsilon) = \\sigma^2 I \\text{ where } \\sigma^2 \\text{ is a scalar; i.e. } \\epsilon \\text{ is homoscedastic} \\\\\n",
    "        &= \\sigma^2 (X^TX)^{-1} X^T I X (X^TX)^{-1} \\\\  \n",
    "        &= \\sigma^2 (X^TX)^{-1} X^T X (X^TX)^{-1} \\\\\n",
    "        &= \\sigma^2 (X^TX)^{-1}\n",
    "    \\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Amazingly, there is a closed form for the variance of the coefficients, if we simply assume that the errors are homoscedastic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.23265304, 0.2362958 , 0.22176446, 0.22166033, 0.22698345,\n",
       "       0.22256677])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = x @ np.array(betas).reshape(-1, 1)\n",
    "epsilon = y.reshape(-1,1) - yhat\n",
    "variance_hat = np.sum(epsilon**2) / (x.shape[0] - x.shape[1])\n",
    "var_covar_matrix = variance_hat * np.linalg.inv(x.T @ x)\n",
    "sigma = np.sqrt(np.diagonal(var_covar_matrix)) ##Same as OLS!\n",
    "sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis Testing the Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Once we have the standard errors, conducting a hypothesis test is trivial\n",
    "\n",
    "- Compute the t-test statistic $t$ using\n",
    "    $$\\begin{aligned}\n",
    "        t &= \\frac{\\beta_i - b}{\\hat{\\sigma}_{\\beta_i}}\n",
    "    \\end{aligned}$$\n",
    "\n",
    "- Since we're testing for coefficient significance, $b = 0$ typically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -0.71011705,   1.00778727,   0.04404157, 272.7224774 ,\n",
       "       116.60058235,  21.98189102])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_values = betas / sigma ## Same as OLS!\n",
    "t_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Significance Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.47796261, 0.31404405, 0.96488885, 0.        , 0.        ,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import t\n",
    "twotail_probability_of_observing_value_at_or_greater_than_t = 1 - (t.cdf(np.abs(t_values), df=x.shape[0]) - t.cdf(-np.abs(t_values), df=x.shape[0]))\n",
    "twotail_probability_of_observing_value_at_or_greater_than_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence Interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.62230892, -0.22611915, -0.42593818, 60.01625506, 26.02044349,\n",
       "         4.45515706]),\n",
       " array([ 0.29188715,  0.70239095,  0.44547189, 60.88725598, 26.91236126,\n",
       "         5.32971974]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import t\n",
    "lb, ub = t.ppf(0.025, df=x.shape[0]), t.ppf(0.975, df=x.shape[0])\n",
    "betas + (lb * sigma), betas + (ub * sigma) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science-SgH_SThs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
