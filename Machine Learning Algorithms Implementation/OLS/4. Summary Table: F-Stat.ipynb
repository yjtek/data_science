{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the OLS Summary Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the previous section, we showed theoretically and practically how we can derive a coefficient matrix $\\beta$, just from the objective function of minimising the mean squared error (MSE)\n",
    "\n",
    "- But you should notice something odd about our results. Our matrix algebra gave us only coefficient values\n",
    "\n",
    "- But the OLS table actually gives us so much more than this! \n",
    "\n",
    "- How can we derive every part of the OLS Summary table? Let's find out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 6)\n",
      "[-0.16521089  0.2381359   0.00976686 60.45175552 26.46640238  4.8924384 ]\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.994</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.994</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>1.775e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 07 Oct 2024</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>14:49:09</td>     <th>  Log-Likelihood:    </th> <td> -1508.0</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   500</td>      <th>  AIC:               </th> <td>   3028.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   494</td>      <th>  BIC:               </th> <td>   3053.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>   -0.1652</td> <td>    0.233</td> <td>   -0.710</td> <td> 0.478</td> <td>   -0.622</td> <td>    0.292</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    0.2381</td> <td>    0.236</td> <td>    1.008</td> <td> 0.314</td> <td>   -0.226</td> <td>    0.702</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>    0.0098</td> <td>    0.222</td> <td>    0.044</td> <td> 0.965</td> <td>   -0.426</td> <td>    0.445</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>   60.4518</td> <td>    0.222</td> <td>  272.722</td> <td> 0.000</td> <td>   60.016</td> <td>   60.887</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>   26.4664</td> <td>    0.227</td> <td>  116.601</td> <td> 0.000</td> <td>   26.020</td> <td>   26.912</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    4.8924</td> <td>    0.223</td> <td>   21.982</td> <td> 0.000</td> <td>    4.455</td> <td>    5.330</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 1.207</td> <th>  Durbin-Watson:     </th> <td>   1.760</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.547</td> <th>  Jarque-Bera (JB):  </th> <td>   1.202</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.028</td> <th>  Prob(JB):          </th> <td>   0.548</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.766</td> <th>  Cond. No.          </th> <td>    1.11</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &        y         & \\textbf{  R-squared:         } &     0.994   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.994   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } & 1.775e+04   \\\\\n",
       "\\textbf{Date:}             & Mon, 07 Oct 2024 & \\textbf{  Prob (F-statistic):} &     0.00    \\\\\n",
       "\\textbf{Time:}             &     14:49:09     & \\textbf{  Log-Likelihood:    } &   -1508.0   \\\\\n",
       "\\textbf{No. Observations:} &         500      & \\textbf{  AIC:               } &     3028.   \\\\\n",
       "\\textbf{Df Residuals:}     &         494      & \\textbf{  BIC:               } &     3053.   \\\\\n",
       "\\textbf{Df Model:}         &           5      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "               & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{x1}    &      -0.1652  &        0.233     &    -0.710  &         0.478        &       -0.622    &        0.292     \\\\\n",
       "\\textbf{x2}    &       0.2381  &        0.236     &     1.008  &         0.314        &       -0.226    &        0.702     \\\\\n",
       "\\textbf{x3}    &       0.0098  &        0.222     &     0.044  &         0.965        &       -0.426    &        0.445     \\\\\n",
       "\\textbf{x4}    &      60.4518  &        0.222     &   272.722  &         0.000        &       60.016    &       60.887     \\\\\n",
       "\\textbf{x5}    &      26.4664  &        0.227     &   116.601  &         0.000        &       26.020    &       26.912     \\\\\n",
       "\\textbf{const} &       4.8924  &        0.223     &    21.982  &         0.000        &        4.455    &        5.330     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       &  1.207 & \\textbf{  Durbin-Watson:     } &    1.760  \\\\\n",
       "\\textbf{Prob(Omnibus):} &  0.547 & \\textbf{  Jarque-Bera (JB):  } &    1.202  \\\\\n",
       "\\textbf{Skew:}          &  0.028 & \\textbf{  Prob(JB):          } &    0.548  \\\\\n",
       "\\textbf{Kurtosis:}      &  2.766 & \\textbf{  Cond. No.          } &     1.11  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.994\n",
       "Model:                            OLS   Adj. R-squared:                  0.994\n",
       "Method:                 Least Squares   F-statistic:                 1.775e+04\n",
       "Date:                Mon, 07 Oct 2024   Prob (F-statistic):               0.00\n",
       "Time:                        14:49:09   Log-Likelihood:                -1508.0\n",
       "No. Observations:                 500   AIC:                             3028.\n",
       "Df Residuals:                     494   BIC:                             3053.\n",
       "Df Model:                           5                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "x1            -0.1652      0.233     -0.710      0.478      -0.622       0.292\n",
       "x2             0.2381      0.236      1.008      0.314      -0.226       0.702\n",
       "x3             0.0098      0.222      0.044      0.965      -0.426       0.445\n",
       "x4            60.4518      0.222    272.722      0.000      60.016      60.887\n",
       "x5            26.4664      0.227    116.601      0.000      26.020      26.912\n",
       "const          4.8924      0.223     21.982      0.000       4.455       5.330\n",
       "==============================================================================\n",
       "Omnibus:                        1.207   Durbin-Watson:                   1.760\n",
       "Prob(Omnibus):                  0.547   Jarque-Bera (JB):                1.202\n",
       "Skew:                           0.028   Prob(JB):                        0.548\n",
       "Kurtosis:                       2.766   Cond. No.                         1.11\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "# import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "\n",
    "x,y = make_regression(\n",
    "    n_samples=500, \n",
    "    n_features=5, \n",
    "    n_informative=2, \n",
    "    n_targets=1, \n",
    "    noise=5, \n",
    "    bias=5,\n",
    "    random_state=123\n",
    ")\n",
    "x = np.append(x, np.ones((500,1)), axis = 1)\n",
    "print(x.shape)\n",
    "\n",
    "betas = np.linalg.inv((x.transpose() @ x)) @ x.transpose() @ y\n",
    "np.set_printoptions(suppress=True)\n",
    "print(betas)\n",
    "\n",
    "print('='*50)\n",
    "res = sm.OLS(exog=x, endog=y, hasconst=True).fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F-Statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The F-statistic is very very closely related to the $R^2$, and is defined using similar components\n",
    "\n",
    "- If you are unfamiliar with the terms below, see the `Concept` section of `3. Summary Table: R Square.ipynb` for a quick revision\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\text{F-Stat} &= \\frac{\\frac{ESS}{k-1}}{\\frac{RSS}{n - k}} \\\\ \\\\\n",
    "    \\text{where } \\\\\n",
    "    ESS &= \\sum_i (\\hat{y}_i - \\bar{y}_i)^2 \\\\\n",
    "    RSS &= \\sum_i (y_i - \\hat{y}_i)^2 \\\\\n",
    "    k &= \\text{Number of features in the regression} \\\\\n",
    "    n &= \\text{Number of observations in the regression}\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "- The intuition here is straightforward; if my explained sum of squares is much higher than the residual sum of squares, my F-statistic will be higher\n",
    "\n",
    "- At very high levels of F-statistic, the model you fit must be \"good\" in the sense that most of the variance is explained by the model! Hence the conclusion that the variables are \"jointly significant\", even if the individual variables are not significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(17745.272222202613)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred = x @ betas\n",
    "ess = np.sum((ypred - np.mean(y))**2)\n",
    "rss = np.sum((y - ypred)**2)\n",
    "k = x.shape[1]\n",
    "n = x.shape[0]\n",
    "\n",
    "fstat = (ess/(k-1)) / (rss/(n-k))\n",
    "fstat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking p-value of F-statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value: 1.1102230246251565e-16\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import f\n",
    "# Calculate the p-value (1 - CDF of F-distribution at the observed F-statistic)\n",
    "p_value = 1 - f.cdf(fstat, k-1, n-k)\n",
    "print(\"p-value:\", p_value) ## Statistically significant if < 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep dive: Why is the F-Distribution Valid?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We've gone through how to compute the F statistic using the adjust ESS and RSS from the regression.\n",
    "\n",
    "- However, it is not clear why we assumed at the outset that the formula above should follow an F distribution. \n",
    "\n",
    "- An F distribution is defined by \n",
    "$$\\begin{aligned}\n",
    "    F &= \\frac{U_1 / d_1}{U_2 / d_2} \\\\ \\\\\n",
    "    \\text{where} \\\\\n",
    "    &U_1, U_2 \\sim \\chi^2_{d_1}, \\chi^2_{d_2} \\\\\n",
    "    &U_1 \\perp U_2 \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "- We claim that this expression follows an F distribution:\n",
    "$$\n",
    "    \\frac{\\frac{ESS}{k - 1}}{\\frac{RSS}{n - k}}\n",
    "$$\n",
    "\n",
    "- Therefore, for this to be true, it must prove that these 2 statements are true\n",
    "$$\\begin{aligned}\n",
    "    ESS \\sim \\chi^2_{k-1} \\\\ \n",
    "    RSS \\sim \\chi^2_{n-k} \\\\\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proof that $ESS \\sim \\chi^2_{k-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's start from the definition of ESS. If this is unclear, see the `Concepts` section of the R Square notebook\n",
    "$$\\begin{aligned}\n",
    "    ESS &= (\\hat{y}_i - \\bar{y})^T (\\hat{y}_i - \\bar{y})\n",
    "\\end{aligned}$$\n",
    "\n",
    "- We'll rewrite the value of $\\hat{y}_i$ in another way:\n",
    "$$\\begin{aligned}\n",
    "    \\hat{y}_i &= X \\hat{\\beta} \\\\\n",
    "    &= X (X^TX)^{-1} X^T y \\\\\n",
    "    &= Hy & H = X (X^TX)^{-1} X^T\n",
    "\\end{aligned}$$\n",
    "\n",
    "- Let's assume that the variable $y$ is de-meaned, so $\\bar{y} = 0$. Then: \n",
    "$$\\begin{aligned}\n",
    "    ESS &= (\\hat{y}_i - \\bar{y})^T (\\hat{y}_i - \\bar{y}) \\\\\n",
    "    &= \\hat{y}_i^T \\hat{y}_i \\\\\n",
    "    &= (Hy)^T Hy \\\\\n",
    "    &= y^T H^T H y\n",
    "\\end{aligned}$$\n",
    "\n",
    "- $H$ is symmetric and and idempotent (proven in the later section, scroll down). Therefore:\n",
    "    - Symmetric: $H^T = H$\n",
    "    - Idempotent: $H \\cdot H = H$\n",
    "$$\\begin{aligned}\n",
    "    ESS &= (\\hat{y}_i - \\bar{y})^T (\\hat{y}_i - \\bar{y}) \\\\\n",
    "    &= \\hat{y}_i^T \\hat{y}_i \\\\\n",
    "    &= (Hy)^T Hy \\\\\n",
    "    &= y^T H^T H y \\\\\n",
    "    &= y^T H H y \\\\\n",
    "    &= y^T H y \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "- We know from the regular OLS expression that \n",
    "$$\\begin{aligned}\n",
    "    y &= X\\beta + \\epsilon \\quad \\text{where} \\quad \\epsilon \\sim N(0, \\sigma^2 I)\n",
    "\\end{aligned}$$\n",
    "\n",
    "- Since $X\\beta$ is a fixed set of values, and $\\epsilon$ is normally distributed, it implies that $y$ must also be normally distributed!   \n",
    "    - Why?\n",
    "    - By assumption in the OLS, error $e$ must follow a normal distribution\n",
    "    - $\\beta X$ is deterministic\n",
    "    - Therefore, $y \\sim N(\\beta X, \\sigma^2 I)$\n",
    "\n",
    "- So if $y$ is normally distributed, and $ESS = y^T H y$ is a quadratic form of a normally distributed variable\n",
    "    - The quadaratic form of any normally distributed variable must follow a chi-square distribution, with degrees of freedom given by the rank of $H$\n",
    "\n",
    "- The rank of $H$ is $k$, where $k$ is the number of predictors in $X$\n",
    "    - $X$ is $n \\times m$, so the rank is $\\min(n,m)$\n",
    "    - $(X^TX)^{-1}$ is $m \\times m$, so the rank is $m$\n",
    "    - $X^T$ is $m \\times n$, so the rank is $\\min(n,m)$\n",
    "    - $H = X (X^TX)^{-1} X^T$, and rank of $H$ must be \n",
    "$$\\begin{aligned}\n",
    "    \\text{rank}(H) &\\le \\min(\\text{rank}(X), \\text{rank}((X^TX)^{-1}), \\text{rank}(X^T)) \\\\\n",
    "    \\text{rank}(H) &\\le\\min(k, k, k) \\\\\n",
    "    \\therefore \\text{rank}(H) &= k\n",
    "\\end{aligned}$$\n",
    "\n",
    "- We know that among the 6 columns in $X$, one of them is an intercept parameter that does not contribute to the degrees of freedom for ESS. Thus, we exclude this, giving us $k-1$ degrees of freedom\n",
    "\n",
    "- Therefore, we have shown that $ESS \\sim \\chi^2_{k-1}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proof that $RSS \\sim \\chi^2_{n-k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Similar to ESS, let's start from definitions;\n",
    "$$\\begin{aligned}\n",
    "    RSS &= \\sum_i (y_i - \\hat{y}_i)^2 \\\\\n",
    "    &= (y - \\hat{y})^T (y - \\hat{y})\n",
    "\\end{aligned}$$\n",
    "\n",
    "- By definition, $y_i - \\hat{y_i}$ is simply the residual after OLS, $e_i$. Therefore\n",
    "$$\\begin{aligned}\n",
    "    RSS &= \\sum_i (y_i - \\hat{y}_i)^2 \\\\\n",
    "    &= (y - \\hat{y})^T (y - \\hat{y}) \\\\\n",
    "    &= \\hat{e}^T \\hat{e}\n",
    "\\end{aligned}$$\n",
    "\n",
    "- In OLS, we always assume that the **true error** $\\hat{\\epsilon} \\sim N(0, \\sigma^2$. Here, we find that $RSS = \\hat{e}^T \\hat{e}$, and note that $\\hat{e} \\neq e$\n",
    "\n",
    "- However, we can still derive some insights about the distribution of $\\hat{e}$ by rewriting it in the following way, where $H = X (X^TX)^{-1} X^T$ represents the hat matrix of y (derivation below)\n",
    "$$\\begin{aligned}\n",
    "    \\hat{e} &= y - \\hat{y} \\\\\n",
    "    &= y - Hy \\\\\n",
    "    &= (I - H) y \\\\ \\\\\n",
    "\n",
    "    \\hat{y} &= Hy \\\\\n",
    "    &= X \\hat{\\beta} \\\\\n",
    "    &= X (X^TX)^{-1} X^Ty\n",
    "\\end{aligned}$$\n",
    "\n",
    "- Since $\\hat{e}$ can be written as a function of $y$, and we know that $y = X \\beta + \\epsilon$, therefore:\n",
    "$$y \\sim N(X \\beta, \\sigma^2 I)$$\n",
    "\n",
    "- As such, it must thus be true that, because when applying a vector to normal random variable, you scale the variance by the squared value\n",
    "$$\\begin{aligned}\n",
    "    \\hat{\\epsilon} &= (I - H)y \\sim N(0, \\sigma^2 (I - H) (I - H)^T)\n",
    "\\end{aligned}$$\n",
    "\n",
    "- However, we know that $H$ is symmetric (so $H = H^T$) and idempotent (so $H \\cdot H = H$). \n",
    "    - See section below for symmetry and idempotence test for $H$\n",
    "\n",
    "- Therefore, it must be true that $I - H$ is also symmetric and idempotent. Consequently:\n",
    "$$\\begin{aligned}\n",
    "    \\hat{\\epsilon} &= (I - H)y \\sim N(0, \\sigma^2 (I - H) (I - H)^T) \\\\\n",
    "    \\therefore \\hat{\\epsilon} &\\sim N(0, \\sigma^2 (I - H))\n",
    "\\end{aligned}$$\n",
    "\n",
    "- Since $\\hat{\\epsilon} \\sim N(0, \\sigma^2 (I - H))$, the term $\\hat{\\epsilon}^T \\hat{\\epsilon}$ is the quadratic form of a normal random variable, which has a $\\chi^2$ distribution!\n",
    "\n",
    "- Since the covariance matrix is multiplied by $(I - H)$, which has rank $n-k$, $RSS \\sim \\chi^2_{n-k}$\n",
    "    - See section below for why $(I - H)$ has rank $n-k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proof that $H$ is symmetric and idempotent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Proof of Symmetry\n",
    "$$\\begin{aligned}\n",
    "    H^T &= (X (X^TX)^{-1} X^T)^T \\\\\n",
    "    &= (X^T)^T ((X^TX)^{-1})^T X^T \\\\\n",
    "    &= X ((X^TX)^{-1})^T X^T \\\\\n",
    "    &= X ((X^TX)^T)^{-1} X^T & (A^{-1})^T = (A^T)^{-1} \\\\\n",
    "    &= X (X^TX)^{-1} X \\\\\n",
    "    &= H\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Proof of Idempotence\n",
    "$$\\begin{aligned}\n",
    "    H \\cdot H &= (X (X^TX)^{-1} X^T) \\cdot (X (X^TX)^{-1} X^T) \\\\\n",
    "    &= X (X^TX)^{-1} X^TX (X^TX)^{-1} X^T \\\\\n",
    "    &= X (X^TX)^{-1} X^T & (X^TX)^{-1} X^TX = I \\\\\n",
    "    &= H\n",
    "\\end{aligned}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why does $I - H$ have rank $n - k$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remember the definition of $H$ is such that $\\hat{y} = Hy$\n",
    "    - That is, $H$ \"puts\" a hat on $y$, thus it is called the hat matrix\n",
    "\n",
    "- Geometrically, $H$ projects observations $y$ onto the space spanned by the design matrix $X$\n",
    "    - This entire possible space spanned by the dimensions of X is known as the **range** of $H$\n",
    "    - Relatedly, the number of linearly independent (i.e. orthogonal) vectors in $X$ and $H$ is known as the **rank** of $H$\n",
    "\n",
    "- Since $H$ is symmetric and idempotent, it is a **projection matrix**\n",
    "\n",
    "- $I - H$ is also a **projection matrix**\n",
    "    - But it projects onto the orthogonal complement to the column space of $X$ / $H$\n",
    "    - This is known as the **residual space**\n",
    "\n",
    "- How do we know that $I - H$ and $H$ are complementary projection matrices? That is, show that any vector projected by $H$ lies in a subspace orthogonal to any vector projected by $I - H$\n",
    "    - Take some arbitrary vector $V$\n",
    "        - When projected onto the column space of $H$, we have $HV$\n",
    "        - When projected onto the orthogonal column space, we have $(I-H)V$\n",
    "\n",
    "    - To prove that $HV$ and $(I-H)V$ are orthogonal, we need to show that $(HV)^T \\cdot (I - H)V = 0$\n",
    "    $$\\begin{aligned}\n",
    "        (HV)^T \\cdot (I - H)V &= (HV)^T \\cdot (V - HV) \\\\\n",
    "        &= (HV)^T V - (HV)^T HV \\\\\n",
    "        &= V^T H^T V - V^T H^T H V \\\\\n",
    "        &= V^T H V - V^T H V & \\because H \\text{ is idempotent + symmetric}\\\\\n",
    "        &= 0\n",
    "    \\end{aligned}$$\n",
    "\n",
    "    - Therefore, $H$ and $I - H$ project onto orthogonal subspaces, because their dotproduct with any arbitrary vector $V$ is 0\n",
    "\n",
    "- So since $H$ and $I - H$ project onto complementary subspaces, and with an $n$ dimensional $y$, if $H$ has rank $k$, then $I - H$ must account for the rest of the dimensions, which are $n-k$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science-SgH_SThs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
