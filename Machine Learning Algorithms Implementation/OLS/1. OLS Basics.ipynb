{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary Least Squares (OLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- OLS is the starting point of all machine learning, mainly because it has been around longest\n",
    "- It is extraordinarily useful, and many ML approaches can be implemented in OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In linear regression we want to minimise the **Mean Squared Error (MSE)** given some data. Assume that \n",
    "$$\\begin{aligned} \n",
    "    \\text{MSE} &= (Y_i - X_{i} \\cdot \\beta)^2 \\\\\n",
    "    &= (Y_i - X_{i} \\cdot \\beta)^T \\cdot (Y_i - X_{i} \\cdot \\beta) \\\\\n",
    "    &= Y_i^T Y_i - Y_i^T X_i \\beta - (X_i \\beta)^T Y_i + (X_i \\beta)^T (X_i \\beta) \\\\\n",
    "    &= Y_i^T Y_i - ((X_i \\beta)^T Y_i)^T - (X_i \\beta)^T Y_i + (X_i \\beta)^T (X_i \\beta) & \\because B^T A^T = (AB)^T \\\\\n",
    "    &= Y_i^T Y_i - (X_i \\beta)^T Y_i - (X_i \\beta)^T Y_i + (X_i \\beta)^T (X_i \\beta) & \\because (X_i \\beta)^T Y_i \\text{ is scalar}\\\\\n",
    "    &= Y_i^T Y_i - 2 (X_i \\beta)^T Y_i + (X_i \\beta)^T (X_i \\beta) \\\\\n",
    "    &= Y_i^T Y_i - 2 (X_i \\beta)^T Y_i + (X_i \\beta)^T (X_i \\beta) \\\\\n",
    "    &= Y_i^T Y_i - 2 (X_i \\beta)^T Y_i + \\beta^T X_i^T X_i \\beta \n",
    "\\end{aligned}$$\n",
    "\n",
    "- Explaining each term\n",
    "    - $Y_i$ is an $n \\times 1$ vector that represents the dependent variable\n",
    "    - $X_i$ is an $n \\times m$ matrix that represents the independent variables\n",
    "    - $\\beta$ is an $m \\times 1$ matrix that represents the coefficients to the regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving for Coefficients $\\beta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In OLS, we try to choose $\\beta$ such that minimise the MSE\n",
    "\n",
    "- That is, we want to find $\\beta^{*}$ such that\n",
    "$$\\begin{aligned}\n",
    "    \\frac{\\partial (\\text{E[MSE]})}{\\partial (\\beta)} &= 0\n",
    "\\end{aligned}$$\n",
    "\n",
    "- Combining these 2 expressions:\n",
    "$$\\begin{aligned}\n",
    "    \\frac{\\partial (\\text{MSE})}{\\partial (\\beta)} &= \\frac{\\partial (Y_i^T Y_i - 2 (X_i \\beta)^T Y_i + \\beta^T X_i^T X_i \\beta)}{\\partial (\\beta)} \\\\\n",
    "    &= \\frac{\\partial}{\\partial \\beta} (Y_i^T Y) - \\frac{\\partial}{\\partial \\beta} (2 (X_i \\beta)^T Y_i) + \\frac{\\partial}{\\partial \\beta} (\\beta^T X_i^T X_i \\beta) \\\\\n",
    "    &= -2 X_i^T Y_i + 2 X_i^T X_i \\beta \\\\\n",
    "    &= 0 \\\\ \\\\\n",
    "\n",
    "    \\therefore -2 X_i^T Y_i + 2 X_i^T X_i \\beta &= 0 \\\\\n",
    "    X_i^T Y_i &= X_{i}^T X_{i} \\beta \\\\\n",
    "    \\beta &= (X_{i}^T \\cdot X_{i})^{-1} \\cdot X_i^T Y_i \n",
    "\\end{aligned}$$\n",
    "\n",
    "- Solving this gives us exactly the OLS coefficients $\\beta$! Let's test this out in code. You will see that all the coefficients of the regression are exactly the same!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.16118975  0.23764784  0.01202546 60.45176247 26.46231774]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "# import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "\n",
    "x,y = make_regression(n_samples=500, n_features=5, n_informative=2, n_targets=1, noise=5, random_state=123)\n",
    "\n",
    "betas = np.linalg.inv((x.transpose() @ x)) @ x.transpose() @ y\n",
    "np.set_printoptions(suppress=True)\n",
    "print(betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.994</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.994</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>2.222e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 05 Oct 2024</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>14:16:46</td>     <th>  Log-Likelihood:    </th> <td> -1508.2</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   500</td>      <th>  AIC:               </th> <td>   3026.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   495</td>      <th>  BIC:               </th> <td>   3047.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     4</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "   <td></td>     <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th> <td>   -0.1612</td> <td>    0.232</td> <td>   -0.694</td> <td> 0.488</td> <td>   -0.618</td> <td>    0.295</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th> <td>    0.2376</td> <td>    0.236</td> <td>    1.007</td> <td> 0.315</td> <td>   -0.226</td> <td>    0.702</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th> <td>    0.0120</td> <td>    0.222</td> <td>    0.054</td> <td> 0.957</td> <td>   -0.423</td> <td>    0.447</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th> <td>   60.4518</td> <td>    0.221</td> <td>  272.934</td> <td> 0.000</td> <td>   60.017</td> <td>   60.887</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th> <td>   26.4623</td> <td>    0.227</td> <td>  116.754</td> <td> 0.000</td> <td>   26.017</td> <td>   26.908</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 1.208</td> <th>  Durbin-Watson:     </th> <td>   1.759</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.547</td> <th>  Jarque-Bera (JB):  </th> <td>   1.202</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.028</td> <th>  Prob(JB):          </th> <td>   0.548</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.766</td> <th>  Cond. No.          </th> <td>    1.11</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &        y         & \\textbf{  R-squared:         } &     0.994   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.994   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } & 2.222e+04   \\\\\n",
       "\\textbf{Date:}             & Sat, 05 Oct 2024 & \\textbf{  Prob (F-statistic):} &     0.00    \\\\\n",
       "\\textbf{Time:}             &     14:16:46     & \\textbf{  Log-Likelihood:    } &   -1508.2   \\\\\n",
       "\\textbf{No. Observations:} &         500      & \\textbf{  AIC:               } &     3026.   \\\\\n",
       "\\textbf{Df Residuals:}     &         495      & \\textbf{  BIC:               } &     3047.   \\\\\n",
       "\\textbf{Df Model:}         &           4      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "            & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{x1} &      -0.1612  &        0.232     &    -0.694  &         0.488        &       -0.618    &        0.295     \\\\\n",
       "\\textbf{x2} &       0.2376  &        0.236     &     1.007  &         0.315        &       -0.226    &        0.702     \\\\\n",
       "\\textbf{x3} &       0.0120  &        0.222     &     0.054  &         0.957        &       -0.423    &        0.447     \\\\\n",
       "\\textbf{x4} &      60.4518  &        0.221     &   272.934  &         0.000        &       60.017    &       60.887     \\\\\n",
       "\\textbf{x5} &      26.4623  &        0.227     &   116.754  &         0.000        &       26.017    &       26.908     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       &  1.208 & \\textbf{  Durbin-Watson:     } &    1.759  \\\\\n",
       "\\textbf{Prob(Omnibus):} &  0.547 & \\textbf{  Jarque-Bera (JB):  } &    1.202  \\\\\n",
       "\\textbf{Skew:}          &  0.028 & \\textbf{  Prob(JB):          } &    0.548  \\\\\n",
       "\\textbf{Kurtosis:}      &  2.766 & \\textbf{  Cond. No.          } &     1.11  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.994\n",
       "Model:                            OLS   Adj. R-squared:                  0.994\n",
       "Method:                 Least Squares   F-statistic:                 2.222e+04\n",
       "Date:                Sat, 05 Oct 2024   Prob (F-statistic):               0.00\n",
       "Time:                        14:16:46   Log-Likelihood:                -1508.2\n",
       "No. Observations:                 500   AIC:                             3026.\n",
       "Df Residuals:                     495   BIC:                             3047.\n",
       "Df Model:                           4                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "x1            -0.1612      0.232     -0.694      0.488      -0.618       0.295\n",
       "x2             0.2376      0.236      1.007      0.315      -0.226       0.702\n",
       "x3             0.0120      0.222      0.054      0.957      -0.423       0.447\n",
       "x4            60.4518      0.221    272.934      0.000      60.017      60.887\n",
       "x5            26.4623      0.227    116.754      0.000      26.017      26.908\n",
       "==============================================================================\n",
       "Omnibus:                        1.208   Durbin-Watson:                   1.759\n",
       "Prob(Omnibus):                  0.547   Jarque-Bera (JB):                1.202\n",
       "Skew:                           0.028   Prob(JB):                        0.548\n",
       "Kurtosis:                       2.766   Cond. No.                         1.11\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = sm.OLS(exog=x, endog=y, hasconst=True).fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding in a constant term $\\beta_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is a trivial extension of the theory above\n",
    "\n",
    "- We simply add in another feature column that are all `1`s. \n",
    "    - The coefficient to this  will give us the constant marginal value of an observation when all other features are 0\n",
    "\n",
    "- Solving this is trivially taking the same matrix multiplication, but with 1 extra column\n",
    "\n",
    "- You can play around with the dataset generation `bias` term to see how well the new coefficient captures the bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.16521089  0.2381359   0.00976686 60.45175552 26.46640238  4.8924384 ]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "# import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "\n",
    "x,y = make_regression(n_samples=500, n_features=5, n_informative=2, n_targets=1, bias=5, noise=5, random_state=123)\n",
    "x = np.append(x, np.ones((500,1)), axis = 1)\n",
    "\n",
    "betas = np.linalg.inv((x.transpose() @ x)) @ x.transpose() @ y\n",
    "np.set_printoptions(suppress=True)\n",
    "print(betas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science-SgH_SThs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
