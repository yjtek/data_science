{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the OLS Summary Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the previous section, we showed theoretically and practically how we can derive a coefficient matrix $\\beta$, just from the objective function of minimising the mean squared error (MSE)\n",
    "\n",
    "- But you should notice something odd about our results. Our matrix algebra gave us only coefficient values\n",
    "\n",
    "- But the OLS table actually gives us so much more than this! \n",
    "\n",
    "- How can we derive every part of the OLS Summary table? Let's find out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 6)\n",
      "[-0.16521089  0.2381359   0.00976686 60.45175552 26.46640238  4.8924384 ]\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.994</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.994</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>1.775e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 09 Oct 2024</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>09:41:36</td>     <th>  Log-Likelihood:    </th> <td> -1508.0</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   500</td>      <th>  AIC:               </th> <td>   3028.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   494</td>      <th>  BIC:               </th> <td>   3053.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>   -0.1652</td> <td>    0.233</td> <td>   -0.710</td> <td> 0.478</td> <td>   -0.622</td> <td>    0.292</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    0.2381</td> <td>    0.236</td> <td>    1.008</td> <td> 0.314</td> <td>   -0.226</td> <td>    0.702</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>    0.0098</td> <td>    0.222</td> <td>    0.044</td> <td> 0.965</td> <td>   -0.426</td> <td>    0.445</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>   60.4518</td> <td>    0.222</td> <td>  272.722</td> <td> 0.000</td> <td>   60.016</td> <td>   60.887</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>   26.4664</td> <td>    0.227</td> <td>  116.601</td> <td> 0.000</td> <td>   26.020</td> <td>   26.912</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    4.8924</td> <td>    0.223</td> <td>   21.982</td> <td> 0.000</td> <td>    4.455</td> <td>    5.330</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 1.207</td> <th>  Durbin-Watson:     </th> <td>   1.760</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.547</td> <th>  Jarque-Bera (JB):  </th> <td>   1.202</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.028</td> <th>  Prob(JB):          </th> <td>   0.548</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.766</td> <th>  Cond. No.          </th> <td>    1.11</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &        y         & \\textbf{  R-squared:         } &     0.994   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.994   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } & 1.775e+04   \\\\\n",
       "\\textbf{Date:}             & Wed, 09 Oct 2024 & \\textbf{  Prob (F-statistic):} &     0.00    \\\\\n",
       "\\textbf{Time:}             &     09:41:36     & \\textbf{  Log-Likelihood:    } &   -1508.0   \\\\\n",
       "\\textbf{No. Observations:} &         500      & \\textbf{  AIC:               } &     3028.   \\\\\n",
       "\\textbf{Df Residuals:}     &         494      & \\textbf{  BIC:               } &     3053.   \\\\\n",
       "\\textbf{Df Model:}         &           5      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "               & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{x1}    &      -0.1652  &        0.233     &    -0.710  &         0.478        &       -0.622    &        0.292     \\\\\n",
       "\\textbf{x2}    &       0.2381  &        0.236     &     1.008  &         0.314        &       -0.226    &        0.702     \\\\\n",
       "\\textbf{x3}    &       0.0098  &        0.222     &     0.044  &         0.965        &       -0.426    &        0.445     \\\\\n",
       "\\textbf{x4}    &      60.4518  &        0.222     &   272.722  &         0.000        &       60.016    &       60.887     \\\\\n",
       "\\textbf{x5}    &      26.4664  &        0.227     &   116.601  &         0.000        &       26.020    &       26.912     \\\\\n",
       "\\textbf{const} &       4.8924  &        0.223     &    21.982  &         0.000        &        4.455    &        5.330     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       &  1.207 & \\textbf{  Durbin-Watson:     } &    1.760  \\\\\n",
       "\\textbf{Prob(Omnibus):} &  0.547 & \\textbf{  Jarque-Bera (JB):  } &    1.202  \\\\\n",
       "\\textbf{Skew:}          &  0.028 & \\textbf{  Prob(JB):          } &    0.548  \\\\\n",
       "\\textbf{Kurtosis:}      &  2.766 & \\textbf{  Cond. No.          } &     1.11  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.994\n",
       "Model:                            OLS   Adj. R-squared:                  0.994\n",
       "Method:                 Least Squares   F-statistic:                 1.775e+04\n",
       "Date:                Wed, 09 Oct 2024   Prob (F-statistic):               0.00\n",
       "Time:                        09:41:36   Log-Likelihood:                -1508.0\n",
       "No. Observations:                 500   AIC:                             3028.\n",
       "Df Residuals:                     494   BIC:                             3053.\n",
       "Df Model:                           5                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "x1            -0.1652      0.233     -0.710      0.478      -0.622       0.292\n",
       "x2             0.2381      0.236      1.008      0.314      -0.226       0.702\n",
       "x3             0.0098      0.222      0.044      0.965      -0.426       0.445\n",
       "x4            60.4518      0.222    272.722      0.000      60.016      60.887\n",
       "x5            26.4664      0.227    116.601      0.000      26.020      26.912\n",
       "const          4.8924      0.223     21.982      0.000       4.455       5.330\n",
       "==============================================================================\n",
       "Omnibus:                        1.207   Durbin-Watson:                   1.760\n",
       "Prob(Omnibus):                  0.547   Jarque-Bera (JB):                1.202\n",
       "Skew:                           0.028   Prob(JB):                        0.548\n",
       "Kurtosis:                       2.766   Cond. No.                         1.11\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "# import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "\n",
    "x,y = make_regression(\n",
    "    n_samples=500, \n",
    "    n_features=5, \n",
    "    n_informative=2, \n",
    "    n_targets=1, \n",
    "    noise=5, \n",
    "    bias=5,\n",
    "    random_state=123\n",
    ")\n",
    "x = np.append(x, np.ones((500,1)), axis = 1)\n",
    "print(x.shape)\n",
    "\n",
    "betas = np.linalg.inv((x.transpose() @ x)) @ x.transpose() @ y\n",
    "np.set_printoptions(suppress=True)\n",
    "print(betas)\n",
    "\n",
    "print('='*50)\n",
    "res = sm.OLS(exog=x, endog=y, hasconst=True).fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Log likelihood is computed using the following formula. We will show the derivation of this formula in a later section\n",
    "$$\\begin{aligned}\n",
    "    LL &= -\\frac{n}{2} (\\log(2\\pi) + \\log(\\sigma^2) + \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (y_i - \\bar{y}_i)^2) \\\\\n",
    "    \\text{where} \\\\\n",
    "    \\sigma^2 &= \\frac{1}{n-k} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-1508.0629969883523)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "ypred = x @ betas\n",
    "n = len(y)\n",
    "k = x.shape[1]\n",
    "rss = (y - ypred)**2\n",
    "sigma_square = (1 / (n-k)) * np.sum((y - ypred)**2)\n",
    "\n",
    "ll = (\n",
    "    -n/2 * (\n",
    "        np.log(2 * np.pi) +\n",
    "        np.log(sigma_square) +\n",
    "        ((1/(n*sigma_square)) * np.sum(rss))\n",
    "    )\n",
    ")\n",
    "ll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How do I derive the formula for Log Likelihood?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Definition of Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The likelihood in OLS is simply the probability of observing the random vector $\\hat{\\epsilon}_i$, or equivalently, $y_i$\n",
    "\n",
    "- In OLS, we assume that residuals $\\hat{\\epsilon}_i \\sim N(0, \\sigma^2)$ \n",
    "\n",
    "- We also know that $\\hat{\\epsilon}_i = y_i - \\hat{y}_i$\n",
    "\n",
    "- Therefore, the PDF of $\\hat{\\epsilon}_i$ is \n",
    "$$\\begin{aligned}\n",
    "    f(\\hat{\\epsilon}_i) &= \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp(- \\frac{1}{2 \\sigma^2} \\epsilon_i^2) \\\\\n",
    "    &= \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp(- \\frac{1}{2 \\sigma^2} (y - \\hat{y_i})^2) \\\\\n",
    "    &= f(y | X, \\beta) & \\because \\hat{y_i} \\text{ is deterministic given X and } \\beta\n",
    "\\end{aligned}$$\n",
    "\n",
    "- Suppose we have $N$ independent observations\n",
    "\n",
    "- Then, the likelihood of observing these $N$ observations must be, by definition:\n",
    "$$\\begin{aligned}\n",
    "    \\prod_i f(\\hat{\\epsilon}_i) &= \\prod_i f(y_i | X, \\beta) \\\\\n",
    "    &= \\prod_i \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp(- \\frac{1}{2 \\sigma^2} (y_i - \\hat{y}_i)^2) \\\\\n",
    "    &= L(\\beta, \\sigma^2, X)\n",
    "\\end{aligned}$$\n",
    "\n",
    "- The product of independent probabilities for each observation $\\epsilon_i$ (or equivalently, the outcome $y_i$) is simply product of their probabilities, drawn from a normal distribution! \n",
    "\n",
    "- Thus, we have shown how to compute **Likelihood**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Derivation of Log Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As you might have deduced from the product above, computing likelihood is annoying \n",
    "\n",
    "- Because we have to loop over every residual $\\epsilon_i$ to compute its probability, and take a product\n",
    "    - Taking products is very annoying, because it can lead to exceedingly small values, which can be numerically unstable\n",
    "\n",
    "- Thus, we usually modify the **Likelihood** slightly by taking $\\log(L(\\beta, \\sigma^2, X))$, which gives us **Log Likelihood**\n",
    "    - Why?\n",
    "    - Because the $\\log$ of a product turns it into a summation! And working with summations is MUCH easier\n",
    "\n",
    "- Therefore:\n",
    "$$\\begin{aligned}\n",
    "    \\log(L(\\beta, \\sigma^2, X)) &= LL(\\beta, \\sigma^2, X) \\\\\n",
    "    &= \\log(\\prod_i \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp(- \\frac{1}{2 \\sigma^2} (y_i - \\hat{y}_i)^2)) \\\\\n",
    "    &= \\sum_i \\log(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}) + \\sum_i \\log(\\exp(- \\frac{1}{2 \\sigma^2} (y_i - \\hat{y}_i)^2)) \\\\\n",
    "    &= \\sum_i \\log((2\\pi\\sigma^2)^{-\\frac{1}{2}}) + \\sum_i -\\frac{1}{2 \\sigma^2} (y_i - \\hat{y}_i)^2 \\\\\n",
    "    &= \\sum_i -\\frac{1}{2} \\log(2\\pi\\sigma^2) + \\sum_i -\\frac{1}{2 \\sigma^2} (y_i - \\hat{y}_i)^2 \\\\\n",
    "    &= -\\frac{n}{2} \\log(2\\pi\\sigma^2) + -\\frac{1}{2 \\sigma^2} \\sum_i (y_i - \\hat{y}_i)^2 \\\\\n",
    "    &= -\\frac{n}{2} \\log(2\\pi) - \\frac{n}{2} \\log(\\sigma^2) - \\frac{RSS}{2 \\sigma^2}\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log-Likelihood Extension: Akaike Information Criteria (AIC) and Bayesian Information Criteria (BIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The AIC is an extension of Log-Likelihood, and is computed using \n",
    "$$\\begin{aligned}\n",
    "    AIC &= 2k - 2 \\log(L)\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(3028.1259939767046)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aic = (2*k) - (2*ll)\n",
    "aic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The BIC is an extension of log-likelihood and is computed using\n",
    "$$\\begin{aligned}\n",
    "    BIC = - 2 \\log (L) + K \\log (N)\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(3053.4136425672377)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "bic = (-2 * ll) + (k * np.log(n))\n",
    "bic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derivation of AIC and BIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science-SgH_SThs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
