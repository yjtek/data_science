{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent in OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gradient descent is quite a basic idea\n",
    "    - Define a loss function $L$ that is a function of parameters $\\beta$; i.e. $L(\\beta)$\n",
    "    - Find the Jacobian $L'(\\beta)$\n",
    "        - If $L'(\\beta_i) < 0$, then the gradient of the loss function is negative. Thaat is, loss decreases as $\\beta$ increases. Thus, we want to increase $\\beta$ \n",
    "        - If $L'(\\beta_i) > 0$, then the gradient of the loss function is positive. Thaat is, loss increases as $\\beta$ increases. Thus, we want to decrease $\\beta$\n",
    "    - Therefore, we can update $\\beta_i$ by taking\n",
    "        $$\\begin{aligned}\n",
    "            \\beta_{t+1} &= \\beta_{t} - \\eta \\cdot \\frac{\\partial L}{\\partial \\beta_i} & \\qquad \\eta \\text{ is a constant representing learning rate}\n",
    "        \\end{aligned}$$\n",
    "\n",
    "    - This is all gradient descent is, at its most basic. All other more complicated optimisers (ADAM, ADAGRAD, etc) are just modifications of this basic idea\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's suppose we have\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    y &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 3) (300, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.6580363 ,  4.8536405 , 19.88349535]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "BETA = np.array([0.5,5,20]).reshape(-1, 1)\n",
    "X = np.concatenate(\n",
    "    (\n",
    "        np.ones((300,1)), \n",
    "        np.random.uniform(0,1,600).reshape(300,-1)\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "y = X @ BETA + np.random.normal(0, 1, 300).reshape(-1, 1)\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression(fit_intercept=False)\n",
    "lr.fit(X, y)\n",
    "lr.coef_ ## Able to find \"true\" coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now that we have set up the problem, let's try to use gradient descent to solve it instead\n",
    "\n",
    "- From the equation's assumed functional form, let's define our loss function $L$ as\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    L &= (y - \\hat{y})^2 \\\\\n",
    "    &= (y - X \\hat{\\beta})^2 \\\\ \\\\\n",
    "\n",
    "    \\frac{\\partial L}{\\partial \\hat{\\beta}} &= \\frac{\\partial}{\\partial \\hat{\\beta}} (y - X \\hat{\\beta})^2 \\\\\n",
    "    &= \\frac{\\partial}{\\partial \\hat{\\beta}} (y - X \\hat{\\beta}^T)^T (y - X \\hat{\\beta}^T)\\\\\n",
    "    &= \\frac{\\partial}{\\partial \\hat{\\beta}} (y^Ty - 2 y^T X \\hat{\\beta} + \\hat{\\beta}^T X^T X \\hat{\\beta}) \\\\\n",
    "    &= - 2 X^T y + X^T X \\hat{\\beta} \\\\\n",
    "    &= 2 X^T (X \\hat{\\beta} - y) \n",
    "\\end{aligned}$$\n",
    "\n",
    "- Then, we iteratively update our $\\beta$ by \n",
    "$$\\begin{aligned}\n",
    "    \\beta_{t+1} &= \\beta_{t} - \\eta (2 X^T (X \\hat{\\beta} - y) )\n",
    "\\end{aligned}$$\n",
    "\n",
    "- Assuming we want to find the gradient descent fo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.061],\n",
       "       [ 4.94 ],\n",
       "       [19.943]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class OLSWithGradientDescent:\n",
    "    def __init__(self):\n",
    "        self.iter = 2000\n",
    "        self.eta = 5e-4\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        assert X.shape[0] == y.shape[0]\n",
    "        \n",
    "        y = y.reshape(-1,1)\n",
    "        assert y.shape[1] == 1\n",
    "\n",
    "        self.coefs_ = np.round(np.ones((X.shape[1], 1)), 3)\n",
    "\n",
    "        for _ in range(self.iter):\n",
    "            jacobian = X.T @ ((X @ self.coefs_) - y)\n",
    "            self.coefs_ = np.round(self.coefs_ - (self.eta * jacobian), 3)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return X @ self.coefs_\n",
    "\n",
    "\n",
    "ols_gd = OLSWithGradientDescent()\n",
    "ols_gd.fit(X, y)\n",
    "ols_gd.coefs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.91534986e-06"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.91534986e-06"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science-SgH_SThs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
