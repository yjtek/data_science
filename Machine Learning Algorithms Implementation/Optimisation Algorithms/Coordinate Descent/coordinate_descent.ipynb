{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coordinate Descent in OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In gradient descent, we update every $\\beta$ value in OLS at the same time using the update function (see Gradient Descent section for more details)\n",
    "$$\\begin{aligned}\n",
    "    \\hat{\\beta_{t+1}} &= \\hat{\\beta_{t}} - \\eta \\frac{\\partial L}{\\partial \\hat{\\beta}}\n",
    "\\end{aligned}$$\n",
    "\n",
    "- This doesn't always work. Some cases where it may fail:\n",
    "    - In many cases, your descent could become stuck at local minima or saddle points\n",
    "    - perhaps due to vanishing or exploding gradients, the updating of the coefficients may not converge\n",
    "    - In high dimensional spaces, the gradient magnitude may be vastly different across dimensions, which can lead to non-convergence or slow convergence\n",
    "\n",
    "- In such cases, instead of trying to update all coefficients jointly, it is possible to simply update the coefficients one at a time. This is **coordinate descent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theoretical Foundation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The idea of coordinate descent is extremely similar to gradient descent, except for an additional loop that goes over the coordinates one by one\n",
    "\n",
    "- First, let's define the loss function\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    L(\\beta) &= \\sum_i (y_i - \\hat{y_i})^2 \\\\\n",
    "    &= \\sum_i (y_i - X_i \\hat{\\beta})^2 \n",
    "\\end{aligned}$$\n",
    "\n",
    "- The \"descent\" idea is the same. At each point, we want to find the rate of change of the loss $L$ with respect to a change in $\\beta$. Except in this case, we want to do this one $\\beta_i$ at a time. To facilitate this, let's rewrite $L(\\beta)$ to make things clearer. This formulation is exactly the same as the one above; we simply separate out the parameter $\\beta_{k}$ to make it clear that this is the value we are updating\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    L(\\beta) &= \\sum_i (y_i - X_i \\hat{\\beta})^2 \\\\\n",
    "    &= \\sum_i (y_i - \\sum_{j \\neq k} (X_{i,j} \\hat{\\beta_{j}}) - X_{i,k} \\hat{\\beta_{k}})^2\n",
    "\\end{aligned}$$\n",
    "\n",
    "- Since we are treating all coefficients besides $k$ as fixed, the first 2 terms in the summation becomes constant! Thus, we define the residual as contributed by the fixed terms as the **partial residual of k**. That is, it tells us what the residual of observation $i$ should be, if we exclude the contribution of $k$\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    r_i^{(k)} &= y_i - \\sum_{j \\neq k} (X_{i,j} \\hat{\\beta_{j}})\n",
    "\\end{aligned}$$\n",
    "\n",
    "- Written this way, the true residual becomes\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    r_i &= (y_i - \\hat{y_i})^2 \\\\\n",
    "    &= y_i - \\sum_{j \\neq k} (X_{i,j} \\hat{\\beta_{j}}) - X_{i,k} \\hat{\\beta_{k}} \\\\\n",
    "    &= r_i^{(k)} - X_{i,k} \\hat{\\beta_{k}}\n",
    "\\end{aligned}$$\n",
    "\n",
    "- Then the original loss function (which we want to minimize) can be rewritten in terms of the partial residual term. \n",
    "$$\\begin{aligned}\n",
    "    \\min_{\\beta} L(\\beta) &= \\min_{\\beta} \\sum_i (y_i - \\hat{y_i})^2 \\\\\n",
    "    &= \\min_{\\beta} \\sum_i (r_i^{(k)} - X_{i,k} \\hat{\\beta_{k}})^2\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Then, computing $\\frac{\\partial L}{\\partial \\hat{\\beta_{k}}}$:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\frac{\\partial L}{\\partial \\hat{\\beta_{k}}} &= \\frac{\\partial}{\\partial \\hat{\\beta_{k}}} \\sum_i (r_i^{(k)} - X_{i,k} \\hat{\\beta_{k}})^2 \\\\\n",
    "    &= \\sum_i 2 X_{i,k} \\cdot (r_i^{(k)} - X_{i,k} \\hat{\\beta_{k}}) \\\\\n",
    "    &= 0 \\\\ \\\\\n",
    "\n",
    "    \\therefore \\sum_i 2 X_{i,k} \\cdot (r_i^{(k)} - X_{i,k} \\hat{\\beta_{k}}) &= 0 \\\\\n",
    "    \\sum_i X_{i,k} \\cdot (r_i^{(k)} - X_{i,k} \\hat{\\beta_{k}}) &= 0 \\\\\n",
    "    \\sum_i (X_{i,k} r_i^{(k)} - X_{i,k}^2 \\hat{\\beta_{k}}) &= 0 \\\\\n",
    "    \\sum_i X_{i,k} r_i^{(k)} &= \\sum_i X_{i,k}^2 \\hat{\\beta_{k}} \\\\\n",
    "    \\hat{\\beta_{k}} &= \\frac{\\sum_i X_{i,k} r_i^{(k)}}{\\sum_i X_{i,k}^2} \n",
    "\n",
    "\\end{aligned}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 3) (300, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.55683854,  5.0426451 , 19.99019941]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "BETA = np.array([0.5,5,20]).reshape(-1, 1)\n",
    "X = np.concatenate(\n",
    "    (\n",
    "        np.ones((300,1)), \n",
    "        np.random.uniform(0,1,600).reshape(300,-1)\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "y = X @ BETA + np.random.normal(0, 1, 300).reshape(-1, 1)\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression(fit_intercept=False)\n",
    "lr.fit(X, y)\n",
    "lr.coef_ ## Able to find \"true\" coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.55683854],\n",
       "       [ 5.0426451 ],\n",
       "       [19.99019941]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class OLSWithCoordinateDescent:\n",
    "    def __init__(self):\n",
    "        self.iter = 100\n",
    "        self.eta = 1e-2\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.coefs_ = np.ones((n_features, 1))\n",
    "\n",
    "        for _ in range(self.iter):\n",
    "            for _coef_index, _coef in enumerate(self.coefs_):\n",
    "                partial_residual_k = (\n",
    "                    y - #n x 1\n",
    "                    X @ self.coefs_ + #n x m @ m x 1\n",
    "                    (self.coefs_[_coef_index] * X[:, _coef_index]).reshape(-1,1) #scalar * n x 1\n",
    "                )\n",
    "\n",
    "                self.coefs_[_coef_index] = (X[:, _coef_index].T @ partial_residual_k) / (np.sum(X[:, _coef_index] ** 2))\n",
    "                \n",
    "                # print(self.coefs_)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return X @ self.coefs_\n",
    "    \n",
    "ols = OLSWithCoordinateDescent()\n",
    "ols.fit(X,y)\n",
    "ols.coefs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science-SgH_SThs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
