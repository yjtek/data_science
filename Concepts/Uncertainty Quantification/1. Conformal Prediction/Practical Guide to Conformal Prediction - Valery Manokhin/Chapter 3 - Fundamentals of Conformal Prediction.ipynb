{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3: Fundamentals of Conformal Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will cover 2 types of conformal prediction: (i) Inductive conformal prediction (ICP) and (ii) Transductive conformal prediction (TCP)\n",
    "    - In TCP, you build your nonconformity score relative to your entire dataset\n",
    "    - In ICP, you build your nonconformity score relative to your calibration dataset \n",
    "\n",
    "- Components of a Conformal Predictor\n",
    "    - **Nonconformity measure**: How to measure how different the new data point is from the entire dataset (TCP) or from your validation set (ICP)\n",
    "    - **Calibration set**: Some amount of data (representative of the whole population) set aside to compute non-conformity scores for the rest of your known data. Using this, we will establish the prediction interval/region for new data points\n",
    "    - **Test set**: The new points you want to test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-conformity Measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How should we measure an observation's non-conformity?\n",
    "\n",
    "- 2 ways: (i) model-dependent and (ii) model-independent non-conformity\n",
    "\n",
    "- Model-dependent nonconformity measures: \n",
    "    - distance to support vectors in support vector machines\n",
    "    - the residual error in linear regression models\n",
    "    - discrepancy between predicted and actual class probabilities in probabilistic classifiers\n",
    "\n",
    "- Model-Agnostic nonconformity measures. This series will focus on this\n",
    "    - hinge loss\n",
    "    - margin \n",
    "    - Brier score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hinge Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In a classification problem, suppose the true label of an observation is \"Class 1\"\n",
    "- And a model predicts a probability of \"Class 1\" as $P(X)$\n",
    "- Then\n",
    "$$\\begin{aligned}\n",
    "    \\text{Hinge Loss} &= 1 - P(X)\n",
    "\\end{aligned}$$\n",
    "\n",
    "- Example\n",
    "    - Model class probability output are Class 1: 0.5, Class 2: 0.3, Class 3: 0.2\n",
    "    - True label is Class 2\n",
    "    - Then hinge loss is $1-0.3=0.7$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Margin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The difference between the predicted probability of the most likely class that isn't the true class and the true class\n",
    "- That is;\n",
    "$$\\begin{aligned}\n",
    "    \\text{Margin} = \\max_{y_i \\neq y_{\\text{true}}} P(y_i | X) - P(y_{\\text{true}})\n",
    "\\end{aligned}$$\n",
    "\n",
    "- The more positive the margin, the more \"nonconformal\" the observation must be\n",
    "\n",
    "- Example\n",
    "    - Model class probability output are Class 1: 0.5, Class 2: 0.3, Class 3: 0.2\n",
    "    - True label is Class 2\n",
    "    - Then hinge loss is $0.5-0.3=0.2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Brier Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Considered \"proper\" in the sense that it gives a true reflection of how good or bad a predicted probability it (no sharp cutoffs, consistent marginal change w.r.t distance from true probability)\n",
    "\n",
    "- Brier score is computed as the sum of squared differences between labels and their predicted probabilities\n",
    "\n",
    "- Example\n",
    "    - Model class probability output are Class 1: 0.5, Class 2: 0.3, Class 3: 0.2\n",
    "    - True label is Class 2\n",
    "    - Then Brier score is is $\\frac{(0 - 0.5)^2 + (1 - 0.3)^2 + (0 - 0.2)^2}{3} \\approx 0.26$\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\text{Brier Score} = \\frac{\\sum_i (\\text{Label} - P(y_i | X_i))^2}{n}\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluating Non-Conformity Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each non-conformity measure is typically evaluated along 2 criteria;\n",
    "    - One-Class Classification (OneC)\n",
    "        - Count of prediction sets that include only one label \n",
    "    - AvgC\n",
    "        - The average size of the class labels in the prediction set\n",
    "\n",
    "- TLDR;\n",
    "    - To maximise singleton sets (i.e. maximise `OneC`), use `Margin` as non-conformity score\n",
    "    - To minimise `AvgC`, use `Hinge Loss` as non-conformity score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Again, we give 3 possible non-conformal scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Absolute error\n",
    "$$\\begin{aligned}\n",
    "    \\text{Non-conformity} &= | y_{\\text{pred}} - y_{\\text{true}} |\n",
    "\\end{aligned}$$\n",
    "\n",
    "- Normalised error\n",
    "$$\\begin{aligned}\n",
    "    \\text{Non-conformity} &= \\frac{| y_{\\text{pred}} - y_{\\text{true}} |}{\\text{s.d.}_{\\text{residuals}}}\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration Set: TCP vs ICP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ICP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Procedure\n",
    "    - Divide your data into training $T$ and calibration $C$ set\n",
    "    - Train point prediction model $H$ using $T$\n",
    "    - For each observation in $C$\n",
    "        - Use $H$ to predict every class probability \n",
    "        - Use an appropriate `Non-conformity Measure` to compute nonconformity scores $\\alpha$ \n",
    "    - For a new observation $x$, for each possible label, compute a non-conformity score by assuming $x$'s true label is that label \n",
    "        - i.e. if you have 3 classes, assume $x$ is class 1, and compute nonconformity score. Then assume $x$ is class 2, etc. \n",
    "    - Compute the p-value of the test object by:\n",
    "        - Counting the proportion of observations in the calibration set that have nonconformity score greater than the test object for that class\n",
    "        - If this number if high, then the test object must be \"conforming\" to the rest of the dataset. Else it is not!\n",
    "        $$\\begin{aligned}\n",
    "            \\text{p-value} &= \\frac{|z_{\\alpha_i \\ge \\alpha_T}| + 1}{n + 1}\n",
    "        \\end{aligned}$$\n",
    "        - The $+1$ comes from the fact that you are adding the test object to the bag of objects you are comparing\n",
    "    \n",
    "    - If the computed p-value (i.e. credibility) is higher than the significance level (i.e. 1 - 0.95 = 0.05), that means that the proportion of the calibration set with higher non-conformity score is very low.\n",
    "        - Add the label to the return set. Else don't add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TCP vs ICP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The main differences between TCP and ICP are:\n",
    "    - In ICP, you portion out a dataset to be used as calibration set, to compute nonconformal scores\n",
    "    - In TCP, you train the model on the entire dataset PLUS the new observation you want to score \n",
    "        - Since you don't know what label the unknown observation should have, you do training $k$ times, where $k$ is the number of possible labels\n",
    "        - For each trained model, use it to compute nonconformal score for the entire dataset \n",
    "        - Then compute p-value for the unknown observation\n",
    "\n",
    "- You can see that, because you need to train $k * m$ models ($k$ being the number of labels, $m$ being the number of unknown observations), this is very computationally intensive.\n",
    "\n",
    "- Hence, ICP is generally preferred unless your dataset is VERY small\n",
    "\n",
    "- Procedure for TCP\n",
    "    - Train the underlying classifier on the entire training set.\n",
    "    - Append each test point to the training set with each possible class label one class label at a time\n",
    "    - For each appended test point with a postulated label, retrain the classifier and compute the nonconformity score for the test point given the postulated label.\n",
    "    - Calculate the p-values for each postulated label, comparing the test pointâ€™s nonconformity score to the scores of the points in the training set\n",
    "    - For each test point and each postulated label, include the postulated label in the prediction set if its p-value is greater than or equal to the chosen significance level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confidence vs Credibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remember, in CP, the overview is:\n",
    "    - For any new observation, we compare its nonconformal score $\\alpha_{\\text{new}}$ with all the other non-conformal scores in the calibration set $\\boldsymbol{\\alpha_C}$\n",
    "    - We count the proportion of values in $\\boldsymbol{\\alpha_C}$ that exceeds $\\alpha_{\\text{new}}$\n",
    "    - Let's say this is 0.96; that is, 96% of the conformal scores in the calibration set is higher than $\\alpha_{\\text{new}}$\n",
    "\n",
    "- Confidence:\n",
    "    - The confidence level $X\\%$ (i.e. 95%) is the proportion of your calibration set/dataset whose nonconformal score exceeds the test observation, before you include it in the return set\n",
    "    - That is, for label to be included in the return set, the test observation must have a lower nonconformal score than  $X\\%$ of the other data points\n",
    "    - Confidence is $1 - \\epsilon$ where $\\epsilon$ is the significance level\n",
    "\n",
    "- Credibility:\n",
    "    - Credibility is just p-value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Assume we have some classification model, and these are the nonconformity scores for the calibration dataset and the test point. Here are the values in the calibration set\n",
    "    - Point 1 (Label A): 0.4\n",
    "    - Point 2 (Label A): 0.3\n",
    "    - Point 3 (Label B): 0.2\n",
    "    - Point 4 (Label B): 0.5\n",
    "\n",
    "- For the test observation, the model produces the following probs:\n",
    "    - $P(Class A) = 0.75$\n",
    "    - $P(Class B) = 0.65$\n",
    "\n",
    "- Then nonconformity scores are:\n",
    "    - For A: $1 - 0.75 = 0.25$\n",
    "    - For B: $1 - 0.65 = 0.35$\n",
    "\n",
    "- Thus, p-values for each labels are:\n",
    "    - For A: 0.25 is smaller than 0.4, 0.3, and 0.5. So p-value is $3+1 / 4+1 = 0.8$\n",
    "    - For B: 0.35 is smaller than 0.4 and 0.5. So p-value is $2+1 / 4+1 = 0.6$\n",
    "    - At our preset confidence of 0.95, it implies that our significance value is 0.05\n",
    "    - Since both 0.8 and 0.6 exceeds 0.05, we include both labels in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online vs Offline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Offline conformal interval uses a fixed model and calibration set to compute nonconformal scores, does not update the model and calibration set over time\n",
    "\n",
    "- Online is the opposite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conformal Prediction Coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When talking about coverage, we are basically asking \"how well does our prediction interval cover the true value\"?\n",
    "\n",
    "- There are 2 types of coverage; **conditional** and **unconditional**\n",
    "\n",
    "- **Unconditional Coverage**:\n",
    "    - This looks at the proportion of times the predicted intervals contains the true value across the entire dataset\n",
    "    - As the name suggests, does not account for dependencies between observations (maybe your prediction interval sucks for a particular subset of the data), or for changes in data distribution\n",
    "\n",
    "- **Conditional Coverage**:\n",
    "    - How well do your intervals contain the true value for specific subsets of the data\n",
    "        - i.e. for some specific category, for some specific time period, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applications of Conformal Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Conformal predictions can be applied to:\n",
    "    - Quantify uncertainty in any ML task\n",
    "    - Combined with model validation techniques like cross-validation/bootstrapping to better quantify uncertainty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
