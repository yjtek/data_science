{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1: Introduction to Conformal Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The key benefit of conformal prediction (CP) is that it provides valid confidence measures for individual predictions\n",
    "\n",
    "- Jargon\n",
    "    - **Coverage**: Probability that the returned region/set actually contains the true values. Such regions are called valid regions\n",
    "\n",
    "- Why CP\n",
    "    - Other approaches to the uncertainty quantification problem do not provide the same theoretical guarantees, and/or make assumptions about the data\n",
    "        - Bayesian methods\n",
    "        - Monte Carlo methods \n",
    "        - Bootstrapping\n",
    "\n",
    "    - CP makes no distributional assumptions AND provides validity guarantees\n",
    "\n",
    "- CP Features\n",
    "    - Guaranteed coverage: This means basically that if CP tells you your returned interval/set contains the true value 95% of the time, it truly will contain it 95% of the time\n",
    "    - Distribution free: No assumptions about distribution needed\n",
    "    - Model-agnostic: Works on all models, basically due to the lack of assumptions needed\n",
    "    - Non-intrusive: Don't need to change the way your model works\n",
    "    - Dataset Size: Doesn't need large data and asymptotic assumptions to work\n",
    "\n",
    "- How does it work? \n",
    "    - CP usually provides a prediction in the form of a **prediction interval** or **prediction set** with a specified confidence level (i.e. 95%)\n",
    "    - General workflow is:\n",
    "        - CP model learns from past training examples to quantify uncertainty around predictions for the new observations\n",
    "        - The model does this by calculating **nonconformity scores**, that is; measuring how different or “nonconforming” the new observation is compared to the entire/validation set \n",
    "        - These nonconformity scores will determine whether the new observation falls in the range of values expected based on the entire/validation data.\n",
    "        - Using this, the model calculates unit-level confidence measures and prediction sets/intervals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding conformity measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The conformity score indicates how well a new observation fits the observed data. \n",
    "\n",
    "- You want the intervals to adapt based on the model’s uncertainty; expanding for difficult predictions and contracting for clear ones\n",
    "\n",
    "- Example: \n",
    "    - We have a dataset of patients diagnosed with a disease, with features such as age, gender, and test results\n",
    "    - We want to predict whether new patients are at risk\n",
    "    - A simple conformity measure could calculate how similar the feature values are between new patients and those in the training data. \n",
    "        - New patients very different from the data would get low conformity scores and wide prediction intervals, indicating high uncertainty. \n",
    "    - This would work like anomaly detection, but instead of a binary \"anomaly/not anomaly\", we have an actual continuous score\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### p-values in conformal prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- p-values have a similar idea to the usual p-value computation, in that we are trying to measure if the \"p-value\" of our non-conformity score exceeds some threshold\n",
    "    - e.g. can I say with 95% confidence that this new picture can be classified as a dog\n",
    "\n",
    "- So roughly, the procedure is:\n",
    "    1. Compute nonconformity score of new observation with your train/validation dataset\n",
    "    2. Assign each possible label to the observation and recompute nonconformity score with that assigned label (i.e. assuming new observation is class 1, how non-conformal is it with the other \"class 1\" obs in my train/validation set)\n",
    "    3. Compute p-value by comparing new observation's non-conformity score with non-conformity score of other observations\n",
    "    4. If the p-value exceeds some significance value, add the label to the valid prediction set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification by Anastasios N. Angelopoulos and Stephen Bates (https://arxiv.org/abs/2107.07511)\n",
    "- Emmanuel Candes, NeurIPS 2022: https://slideslive.com/38996063/conformal-prediction-in-2022?ref=speaker-43789\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
