{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 6: Conformal Prediction for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why calibration is important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You need to calibrate well, or your probability/regression outputs may not be meaningful\n",
    "    - For example, it won't make sense if 20% of the people who you predict to have 1% chance of cancer actually have cancer\n",
    "    - Without proper calibration, deciding on cutoffs may be difficult also. If your 50% isn't truly 50%, then what is the point?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to evaluate calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are a few ways to evaluate the calibration of your model?\n",
    "    - Calibration Plot: Plot frequency of positives against mean predicted probability\n",
    "    - Calibration Error: Reduce calibration evaluation to a single metric\n",
    "        - Mean absolute distance between estimated probabilities and observed probabilities\n",
    "    - Calibration metrics: Some of these include Expected Calibration Error, Log Loss, Brier Score (discussed in Chapter 3/4)\n",
    "    - Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approaches to Classifier Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogram Binning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Not really an \"algorithm\" per se\n",
    "\n",
    "- Procedure\n",
    "    - Divide the predicted probabilities into bins/intervals \n",
    "    - For these same intervals, compute the observed proportions\n",
    "    - Compute the ratio between the predicted and observed proportions\n",
    "    - Adjust all probabilities within the intervals according to the adjustment ratio\n",
    "\n",
    "- Some drawbacks \n",
    "    - Bins are kind of inflexible, so if miscalibration is non-linear, then you have problems\n",
    "    - Binning causes information loss, because you are hard coding the boundaries between bins and calibrating only using bin averages\n",
    "    - Sensitive to thresholds chosen\n",
    "    - Discontinuous adjustments, even in adjacent bins\n",
    "    - If your data distribution shifts, the bins may not generalise well to new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Platt Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Procedure\n",
    "    - Collect a labeled validation set or a holdout set that is distinct from the training data used to train the classifier\n",
    "    - Use the classifier to generate the raw output scores or logits for the instances in the validation set\n",
    "    - Fit a second logistic regression model on the validation set, treating the raw scores as the independent variable and the true class labels as the dependent variable\n",
    "    - Once the logistic regression model has been trained, it can be used as a calibration function.\n",
    "        - i.e. Given a new instance, the raw score produced by the classifier is input into the logistic regression model, which transforms it into a calibrated probability estimate.\n",
    "\n",
    "- Idea is: you fit the predictions of a holdout set (that you know the true labels for) using another model\n",
    "\n",
    "- Some drawbacks\n",
    "    - You need a holdout validation set that the first model has not seen\n",
    "    - Logreg assumption\n",
    "    - If your holdout set contains some extreme values by chance, you may create a terrible calibration function unknowingly\n",
    "    - Hard to map this to multiclass setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Isotonic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Procedure\n",
    "    - Collect a labeled validation set or a holdout set that is separate from the training data\n",
    "    - Use the classifier to generate the raw output scores or probabilities for the instances in the validation set\n",
    "    - Sort the instances in the validation set based on the raw scores\n",
    "    - Initialize the isotonic regression function as the identity function, where the initial predicted probabilities are equal to the raw scores\n",
    "    - Iteratively update the isotonic regression function by adjusting the predicted probabilities to minimize the squared differences between the predicted probabilities and the target probabilities. This adjustment is subject to the constraint of non-decreasing probabilities.\n",
    "    - Repeat the updating process until convergence or a stopping criterion is reached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Downsides\n",
    "    - Overfitting possible\n",
    "    - Sensitive to outliers\n",
    "    - Limited flexibility\n",
    "    - No multiclass support\n",
    "    - Limited probabilistic interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
